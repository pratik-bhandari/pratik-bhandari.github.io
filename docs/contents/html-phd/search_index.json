[["index.html", "Interaction of top-down and bottom-up processes in spoken language comprehension Welcome Abstract Acknowledgments", " Interaction of top-down and bottom-up processes in spoken language comprehension Pratik Bhandari Abstract It seems pretty easy to listen to and understand someone speaking. However, our day-to-day conversations occur under adverse listening conditions. For example, background noise comes from different sound sources, multiple people talk simultaneously (e.g., in a café), a poor signal connection distorts the voice of a person talking on the other end of a telephone call, and the list goes on. Despite these adversities, most of the time, we communicate successfully. One of the significant contributors to our ability to understand language in adverse listening conditions is predictive language processing. Humans are not passive consumers of language: we use the information available to us from a context and predict the not-yet-encountered, upcoming linguistic events. We do not wait for a speech signal to unfold completely to decode its meaning. This feature of human language processing is critical in understanding speech in adverse listening conditions. The studies in this thesis are timely in the field when the discussion about the role of prediction in language processing is vibrant and to some extent—heated. Some argue that prediction is a universal phenomenon, not only of language, but of human cognition, in general. The present thesis examined the boundary conditions of predictive language processing. We investigated if linguistic predictions are automatic, or if they are constrained by other factors like top-down attention regulation and bottom-up processing of different speech rates in degraded speech comprehension. In this thesis, we examined how listeners can use context information and form predictions while listening to speech at different levels of degradation. The central theme of the thesis is the investigation of the interactions between top-down semantic predictions and bottom-up auditory processing in adverse listening conditions under the theoretical framework of predictive processing and the noisy channel model of communication. We first introduce these concepts of top-down–bottom-up interactions in adverse listening conditions, then report the experiments that empirically investigated different aspects of degraded speech comprehension and the top-down – bottom-up interactions. Our findings showed that to understand a speaker’s utterance in a noisy channel (e.g., due to the degradation of speech signal), a listener takes into account the noise in the signal as well as the context information to form lexical-semantic predictions. Studies have shown that lexical-semantic predictions facilitate language comprehension. We investigated if such a facilitatory effect of linguistic predictions is observed at all levels of speech degradation. We also addressed the debate on the nature of predictability effect (graded vs all-or-nothing). The studies in this thesis concluded that comprehension of degraded speech is predictive in nature: language processing in a noisy channel is probabilistic and rational. Listeners weigh top-down predictive (lexical-semantic cues) and bottom-up auditory (acoustic-phonetic cues) processes. When the speech degradation is not severe, they can rely on the bottom-up input of an upcoming word (i.e., what they actually heard), regardless of the context information available to them. When the speech is moderately degraded but intelligible enough, they generate predictions about the upcoming word from the context information. In addition, the weighing of lexical-semantic and acoustic-phonetic cues is also modulated by attention regulation and speech rate. Taken together, this thesis contributes to a better understanding of the dynamic interaction between top-down and bottom-up processes in speech comprehension. Welcome You are welcome to read my dissertation in this website. You can also download the pdf file (click here, or the cover page). The contents on the pdf file are considered standard, and they supersede those on this site, in case of any discrepancy/errors/typo. Pratik Bhandari 27 December 2022 Abstract It seems pretty easy to listen to and understand someone speaking. However, our day-to-day conversations occur under adverse listening conditions. For example, background noise comes from different sound sources, multiple people talk simultaneously (e.g., in a café), a poor signal connection distorts the voice of a person talking on the other end of a telephone call, and the list goes on. Despite these adversities, most of the time, we communicate successfully. One of the significant contributors to our ability to understand language in adverse listening conditions is predictive language processing. Humans are not passive consumers of language: we use the information available to us from a context and predict the not-yet-encountered, upcoming linguistic events. We do not wait for a speech signal to unfold completely to decode its meaning. This feature of human language processing is critical in understanding speech in adverse listening conditions. The studies in this thesis are timely in the field when the discussion about the role of prediction in language processing is vibrant and to some extent—heated. Some argue that prediction is a universal phenomenon, not only of language, but of human cognition, in general. The present thesis examined the boundary conditions of predictive language processing. We investigated if linguistic predictions are automatic, or if they are constrained by other factors like top-down attention regulation and bottom-up processing of different speech rates in degraded speech comprehension. In this thesis, we examined how listeners can use context information and form predictions while listening to speech at different levels of degradation. The central theme of the thesis is the investigation of the interactions between top-down semantic predictions and bottom-up auditory processing in adverse listening conditions under the theoretical framework of predictive processing and the noisy channel model of communication. We first introduce these concepts of top-down–bottom-up interactions in adverse listening conditions, then report the experiments that empirically investigated different aspects of degraded speech comprehension and the top-down – bottom-up interactions. Our findings showed that to understand a speaker’s utterance in a noisy channel (e.g., due to the degradation of speech signal), a listener takes into account the noise in the signal as well as the context information to form lexical-semantic predictions. Studies have shown that lexical-semantic predictions facilitate language comprehension. We investigated if such a facilitatory effect of linguistic predictions is observed at all levels of speech degradation. We also addressed the debate on the nature of predictability effect (graded vs all-or-nothing). The studies in this thesis concluded that comprehension of degraded speech is predictive in nature: language processing in a noisy channel is probabilistic and rational. Listeners weigh top-down predictive (lexical-semantic cues) and bottom-up auditory (acoustic-phonetic cues) processes. When the speech degradation is not severe, they can rely on the bottom-up input of an upcoming word (i.e., what they actually heard), regardless of the context information available to them. When the speech is moderately degraded but intelligible enough, they generate predictions about the upcoming word from the context information. In addition, the weighing of lexical-semantic and acoustic-phonetic cues is also modulated by attention regulation and speech rate. Taken together, this thesis contributes to a better understanding of the dynamic interaction between top-down and bottom-up processes in speech comprehension. Acknowledgments This dissertation is a culmination of almost four years of work I conducted as a PhD student. Several people, events and circumstances made it possible for me to start and finish the journey through my PhD. Family, friends, colleagues, and even strangers have helped me arrive at this juncture of life. My family — baa, aamaa, bhai, buhari — has been there with me every step of the way. Words fall short of expressing gratitude towards them! I am very grateful to my supervisors, Jutta Kray and Vera Demberg, for taking me in. They gave me the freedom to pursue my interests but knew well when it was time to set me back on course. All these years, they were considerate of my health and working schedule; I couldn’t ask for more. I was lucky to have supervisors from two different departments: I learnt from their unique perspectives in all the scientific works. I got the best of both worlds from Psychology and Computational Linguistics. I express my gratitude to Jutta for trusting and encouraging me to grow and develop as a researcher. She did not let me get distracted by all the exciting topics in the field. Perhaps “doing science” requires focusing on a specific set of questions, pursuing them, and not getting distracted. I am grateful to Jutta for sharing her experiences and letting me learn from them. I express my gratitude to Vera for guiding, inspiring, and pushing me beyond my comfort zone. I appreciate the challenging discussions on various aspects of our studies. Vera has always been accessible. She has created a lab, a group that is like a family and not just a workplace. I will miss everyone and everything, from the rigorous interdisciplinary scientific discussion to hiking, board game night, and whatnot! It must be my good luck that I had some brilliant and inspiring researchers mentoring me. I cannot thank Yoana Vergilova enough, who has been my mentor since my first day at Saarland University. Her feedback always propelled the pace of the development of my project. Our conversations about human society, as well as different aspects of psycholinguistic and neuroscience research, have had a significant impact on my work. Its imprints can be seen in the pages that follow. Thanks also to Katja Häuser for the discussion on prediction, ageing, and statistics. Some of our discussions left me with questions, the answer to which I have attempted to find and write in this dissertation. My gratitude goes to Bernd Möbius for his guidance in the development of one of my projects. I joined SFB around the same time as Marjolein van Os and Anupama Chingacham on the same project. I cannot thank Marjolein enough for her intellectual companionship, for listening to my experiences and sharing hers: the pain, difficulties, and joy we encountered along the PhD journey. Our discussion on language comprehension, predictive processing, statistics, and everything-PhD are all imprinted in this dissertation. I extend my gratitude to Anu for always encouraging me, checking in, and helping me learn about language models. The student research assistants have assisted me to a great extent in different stages of the experiments, from creating stimuli to annotating participants’ responses. Thanks to Katharina Stein, Valentin Kany, Melanie Hilz, Sarah Memeche and Amélie Leibrock. My colleagues Marian Marchal, Kaveri Anuranjana, and Emilia Ellisiepan have been incredibly generous with their time and resources. I thank them. I also thank Lena Müller, Corinna Lorenz, Margarita Ryzhova, Xudong Hong, Jia Ern Loy, Frances Pikyu Yung, Sasha Mayn, Jingqi Yan, Dongqi Pu, and Merel Scholman. I am especially grateful to Marian for patiently and critically reading through the chapters of this dissertation. (Of course, all the errors are my own.) She is not just a great academic but also a kind and wonderful friend. The Collaborative Research Center (SFB) was genuinely collaborative, not just academically but also in building a network of people. I want to thank Ivan Yuen, Omnia Ibrahim, Iona Gessinger, Koel Dutta Chowdhury, Badr Abdullah, Torsten Jachmann, Christoph Aurnhammer, and Julia Meßmer. Many thanks to the past and present members of the SFB coordination office: Patricia Borrull Enguix, Diana Steffen, Sabine Loskyll, and Marie-Ann Kühne. Their helping hands made the cumbersome paperwork easier. Thanks goes also to the former secretary at Vera’s office, Gabriele Reibold, who dealt with a wide range of issues, bureaucratic and otherwise, to make my life easier when I was just starting the PhD. I wish some ease for her now. Many people outside the immediate vicinity of my research project have also contributed to this dissertation. I am forever grateful to Saadullah Amin, without whom I couldn’t possibly have completed this dissertation. Saad is an empathetic friend; his moral and emotional support kept me going through the hard times. Thanks to my friends who listened to my rants, complaints and cries for years and yet encouraged me to persevere and enjoy the journey: I hope the spontaneous meetups and trips will continue with Kirk Goodard, Trisha Thomas, and Liu Mengxing. Despite being miles away, Keshav Poudel has always inspired me to be a better human being. I thank him for being a harsh critic and a loving friend. Thanks to Tobias Schneider and Pratiti Bhadra for regularly checking in on my health and research. I’m grateful to my lovely neighbours Maria and Felix Hubert, who saved my life and have cared for me. My gratitude goes to Magda Altmann and James Hartzell who gave me a home away from home. They have always provided a shelter of perspectives when I felt I ran out of hope and vision. They inspire me to live life to its fullest. I wouldn’t even have known about the scientific research in cognitive science, psychology and psycholinguistics if Lekhnath Pathak (Tribhuvan University, Nepal) had not introduced me to the field and if Ramesh Kumar Mishra (University of Hyderabad, India) had not trained me in his lab. I am forever indebted to them both. Thanks also to my Master’s thesis supervisor David Soto (BCBL, Spain), who introduced me to the world of predictive processing. Finally, thanks to the strangers who never failed to remind me of ‘my ground’. "],["chapter-introduction.html", "1 Introduction 1.1 Research goals 1.2 Research contributions 1.3 Overview of the thesis 1.4 Dissemination of research findings", " 1 Introduction One of the features that distinguishes us, humans, from other species is our ability to communicate using verbal language (Hauser et al., 2002; Lieberman, 2013; Pinker &amp; Jackendoff, 2005). We speak. We listen. We understand. This seemingly straightforward path of communication goes through plenty of hindrances. One of them is an adverse listening condition caused by background noise and speech distortion (e.g., Chen &amp; Loizou, 2011; Fontan et al., 2015). Human comprehenders rely on top-down predictive and bottom-up auditory processes to understand spoken language. Language comprehension in adverse listening conditions is aptly described by the noisy channel model of communication (Gibson et al., 2013, 2019; Levy, 2008; C. E. Shannon, 1948) schematically represented in Figure 1.1 below. Figure 1.1: Schematic representation of the noisy channel model of communication The speaker produces an utterance \\(u_i\\) with a meaning \\(m_i\\) that she intends to send. The utterance is encoded into a signal and sent through a channel of transmission. During transmission, some external noise disrupts the signal. The receiver (e.g., a listener) perceives the signal as \\(u_p\\) and decodes it to recover the meaning as \\(m_p\\). The human language comprehension system is assumed to be engaged in optimal Bayesian decoding that uses all the sources of information (e.g., prior semantic knowledge, context information, world knowledge, etc.) and infers the intended meaning from the perceived utterance that it receives from a noisy channel of communication (Gibson et al., 2013; Levy, 2008; cf. Markman &amp; Otto, 2011). For successful communication to occur, the message recovered by the listener must be approximately equal to the message intended to be sent by the speaker. Let’s take an example. X sees a spherical object flying towards Y. So, X intends to warn Y about the spherical object which is about to hit him. To convey this message, X utters “Ball!”. Due to external noise, X’s (i.e., the speaker’s) utterance is distorted, so Y (i.e., the listener) perceives the utterance as “Hall!”. The listener then interprets that the speaker’s message is intended to point him to a “building where lectures take place”. (In this case of unsuccessful communication, or due to the listener wrongly identifying the speaker’s intended message, Y gets hit by a ball.) We assume that the goal of a listener is to identify the message \\(m_i\\) that is most likely from the perceived utterance \\(u_p\\), taking into account the external noise (\\(N\\)) and the prior likelihood of the speaker uttering \\(u_i\\). This can be expressed formally as1: \\[\\begin{align} \\hat{m_p} &amp;= \\mathop{\\mathrm{argmax}}\\limits_{m_p} P(m_p,u_p,N,u_i,m_i) \\tag{1.1} \\end{align}\\] This sequence of events from the intended message \\(m_i\\) to the perceived message \\(m_p\\) can be graphically represented in a Bayesian network (Bruineberg et al., 2021; Darwiche, 2010; Pearl, 1985) in Figure 1.2 (cf. Figure 1.1). Figure 1.2: Bayesian network representation of the noisy channel model of communication Figure 1.2 models the dependencies among the events, which shows that the external noise and the speaker’s utterance are independent. However, the listener’s perception of the uttered message is dependent also on the noise. The communication in the noisy channel, represented as a Bayesian network, can now be expressed as: \\[\\begin{align} \\hat{m_p} &amp;= \\mathop{\\mathrm{argmax}}\\limits_{m_p} P(m_p | u_p) * P(u_p|u_i, N) * P(u_i | m_i) * P(m_i) \\tag{1.2} \\end{align}\\] Equation (1.2) can be interpreted easily from its corresponding representation in Figure 1.2. It shows: \\(P(m_p | u_p)\\): the probability of inferring a meaning \\(m_p\\) (e.g., a building where lectures take place) from a perceived utterance \\(u_p\\) (e.g., hall) \\(P(u_p|u_i, N)\\): bottom-up auditory information, i.e., the probability of the listener hearing a particular utterance \\(u_p\\) (e.g., hall) given that the speaker has uttered an utterance \\(u_i\\) (e.g., ball) in the noisy channel \\(N\\) (e.g., background noise, signal distortion, etc.) \\(P(u_i|m_i)P(m_i)\\): prior information (e.g., top-down semantic knowledge, information about the speaker, etc.), i.e., the probability of a speaker uttering \\(u_i\\) with an intended message \\(m_i\\) with the probability that the intended message is \\(m_i\\) \\(P(m_i)\\): speaker model, i.e., the probability that the speaker intends to send a particular message In this thesis, we investigate in the interaction of bottom-up \\(P(u_p|u_i, N)\\) and top-down \\(P(u_i|m_i)P(m_i)\\) processes. Henceforth, we focus our discussion on these two components. The channel of transmission can become noisy due to factors like background noise present in a conversation, a poor signal transmission of a telephone call that distorts the speaker’s speech, hearing loss of a listener, hearing aid or cochlear implant worn by a listener, and so on. To understand speech in such a noisy channel of communication, a listener puts different weights on the distorted bottom-up auditory input \\(P(u_p|u_i, N)\\) vs the prior information \\(P(u_i|m_i)P(m_i)\\) (e.g., context information). This weighing of top-down and bottom-up processes is considered as a rational process in the models of probabilistic language processing in reading comprehension (Levy, 2008; Ryskin et al., 2018; see also van Os et al., 2021b for an implementation of the rational approach in spoken language comprehension in background noise). In this thesis, we also investigate to what extent listeners use their priors from the context information when the signal is distorted at different levels. Clean speech and reading comprehension studies have demonstrated that listeners and readers use prior knowledge and context information to form semantic predictions about the linguistic events yet to be encountered. Let’s take the following sentence, for example: The day was breezy so the boy went outside to fly a ___ Most readers would expect the final word to be kite in this sentence (DeLong et al., 2005; cf. Nieuwland et al., 2020). Here, the words up to the final word of the sentence provide a context: A reader can utilise their knowledge about what a boy would ideally do outside on a breezy day. It leads the reader to predict that the sentence continuation is most likely kite and not an improbable word like rocket. Similar results are observed in the auditory domain as well. Listeners use context information from what they have heard and form predictions about an upcoming word (e.g., Altmann &amp; Kamide, 2007; Ankener, 2019). That is, human language comprehension is predictive in nature, such that listeners engage in predictive language processing (Section 2.2.1, Kuperberg &amp; Jaeger, 2016; Pickering &amp; Gambi, 2018). In a noisy channel, listeners’ engagement in predictive processing is influenced by the noise in the signal (Obleser et al., 2007; Sheldon et al., 2008a). Based on the theoretical accounts of the noisy channel model of communication and the predictive language processing (Christiansen &amp; Chater, 2015; Ferreira &amp; Lowder, 2016; Friston, Parr, et al., 2020; Hale, 2001; Levy, 2008; McClelland &amp; Elman, 1986; Norris et al., 2016; Pickering &amp; Gambi, 2018), this thesis investigates the interaction between top-down predictive and bottom-up auditory processes. We examine how top-down predictive processes facilitate language comprehension in a noisy channel created by acoustic degradation of speech, and what the nature of such a facilitation is (e.g., probabilistic, deterministic). We investigate the levels of noise in the signal for the effect of top-down predictive processes to be most efficient or facilitatory for language comprehension. By manipulating different factors of top-down as well as bottom-up processes (e.g., speech rates, attention allocation to different parts of the speech stream), we examine their role in aiding (or interfering) the comprehension of degraded speech. While doing so, we address the following research goals. 1.1 Research goals To replicate the predictability effect in a noisy channel Almost all the disciplines of cognitive science — anthropology, computer science, linguistics, neuroscience, and psychology — are suffering the so-called replication crisis (Aarts et al., 2015; Cockburn et al., 2020; Ebersole et al., 2016; Minocher et al., 2021; Sanderson &amp; Roberts, 2008). The results of an experiment do not hold up consistently when another group of researchers conduct it again: For example, DeLong et al. (2005) found that it was easier to process the article ‘an’ when readers anticipated a phonologically congruent word ‘airplane’ than when they anticipanted a phonologically incongruent word ‘kite’. But this effect was not replicated in a recent multi-lab collaborative study by Nieuwland et al. (2020). The first goal of this thesis is to test if we can replicate the facilitatory effect of semantic predictability in language comprehension in a noisy channel (e.g., Obleser et al., 2007; Sheldon et al., 2008a). Replication of the predictability effect in comprehension of degraded speech will help gather evidence in favour of (or against) this effect of interest. It will also provide a reliable foundation to test if (and how) other factors (e.g., speed of information processing) influence and interact with the facilitatory effect of predictability. To examine the nature of prediction There are at least two schools of thought which argue that prediction is either all-or-nothing (e.g., Ferreira &amp; Clifton Jr, 1986) or probabilistic and graded (e.g., Luke &amp; Christianson, 2016). These debates generally centre around reading comprehension and clean speech comprehension. The discussion about the nature of prediction in a noisy channel like degraded speech is sparse. Specifically, in degraded speech comprehension, only one study has empirically investigated the theoretical postulation that prediction is restricted only to highly predictable sentence endings (Strauß et al., 2013). Therefore, the second goal of this thesis is to examine the nature of the predictability effect. With carefully designed experiments and materials, this thesis aims to test the distinction between all-or-nothing and probabilistic predictions in degraded speech comprehension. To assess the boundary conditions of predictive language processing Several authors claim that predictive processing is the fundamental nature of human cognition and, thus, by definition, also of language processing (A. Clark, 2013; Friston, Parr, et al., 2020; Friston, Sajid, et al., 2020; Kuperberg, 2021; Lupyan &amp; Clark, 2015). At the same time, an increasing number of studies are showing boundary conditions and prerequisite conditions for predictive language processing (Federmeier et al., 2010; Huettig &amp; Guerra, 2019; Huettig &amp; Mani, 2016; Mishra et al., 2012). For example, prediction can have different effects on unattended stimuli and attended stimuli (cf. Kok et al., 2012). In a noisy channel (i.e., degraded speech), attention to a part of a speech stream can modulate or limit the predictability effect as different parts of the speech stream contain different linguistic units; each linguistic unit (e.g., each word in a sentence) carries its own meaning that serves the entire message (e.g., words serve in building the meaning of the entire sentence). Therefore, the third goal of this thesis is to examine the role of auditory attention that can act as a prerequisite for semantic predictions or limit the automaticity of predictive processing in degraded speech comprehension. This thesis aims to test whether attention to different parts of degraded speech stream aids or hampers facilitatory effects of top-down predictions. To test for the adaptation to degraded speech Despite the difficulty in understanding speech in a noisy channel, listeners rapidly adapt to degraded speech (Rosen et al., 1999): Their performance improves over the course of the experiment. When the properties of speech vary in the dimension of both acoustic-phonetic cues as well as lexical-semantic cues, adaptation can be difficult. The fourth goal of this thesis is to examine if listeners adapt to degraded speech when both degradation level and predictability of speech are varied. We test if an adaptation to the bottom-up perceptual property of speech is influenced by its top-down semantic property. To examine the effect of speech rate Unlike the visual scene that opens in the spatial dimension, speech signal flows in the temporal dimension. This challenges the listeners to process information at different speeds and timescales; more time is available to process the information in slow speech, while less time is available for fast speech (Lerner et al., 2014). Listeners build up the meaning representation as they process the speech to predict upcoming linguistic units. The fifth goal of this thesis is to examine whether a change in information flow, i.e., speech rate, affects the facilitatory effect of predictability. We test if an increase or decrease in speech rate impedes the intelligibility of speech over a noisy channel and whether it impedes or further aids the predictability effect in the noisy channel. To assess language comprehension considering the context Different researchers have used different measurement metrics in the study of speech perception and language comprehension (Amichetti et al., 2018; Obleser et al., 2007; Peelle, 2013; Sheldon et al., 2008a). The measurement is inconsistent across studies which becomes a problem, especially when the effect of context in comprehension is under discussion: cross-study comparison does not give a clear picture of the predictability effect in this case. Therefore, the sixth goal of this thesis is to establish and consistently use a sensitive metric for the measurement of language comprehension that takes into account whether participants (in)correctly use the context-evoking word in a sentence. Studies addressing the research goals outlined above will primarily contribute to elaborating and developing the existing theories of predictive language processing and furthering the understanding of spoken language comprehension in a noisy channel, especially degraded speech comprehension. Below we present the contributions of the research presented in this thesis. 1.2 Research contributions The research reported in this thesis examines theoretical questions of predictive language processing and its boundary conditions when spoken language comprehension takes place through a noisy channel. It contributes to the studies of speech perception, language comprehension, predictive coding, language science, audiology, psycholinguistics, psychology, and, broadly, cognitive science. In an applied setting, this informs translational/clinical researchers about language comprehension in cochlear implantees. Graded effect of predictability We replicate the previous finding of the predictability effect showing that predictability facilitates comprehension of degraded speech at moderate levels of degradation (e.g., Obleser et al., 2007). Additionally, in the current debate of all-or-nothing vs graded prediction, our findings indicate that prediction across the noisy channel of degraded speech is graded in nature rather than being restricted to a narrow space of highly predictable sentence endings. Goals 1 and 2 correspond to this research contribution brought about by the experiments described in Chapters 5 and 6. Attention in predictive language processing We show that predictive processing is not always automatic, and it cannot all by itself explain how listeners understand speech in a noisy channel. Although top-down predictions facilitate comprehension, we show that attention to the context is a prerequisite for such contextual facilitation. Only when listeners attend to the context information and form its meaning representation can the top-down predictions facilitate comprehension of degraded speech. Without proper attention to the context, predictability effects cannot be observed. Goal 3 corresponds to this research contribution brought about by the experiment described in Chapter 5. Absence of perceptual adaptation We show that listeners do not adapt to degraded speech when lexical-semantic cues are taken into consideration. This is in contrast with the previous findings of speech perception experiments, some of which disregard the trial-by-trial variation in sentence context (e.g., Davis et al., 2005; Erb et al., 2013). When listeners are engaged in a linguistic task in which the lexical cues vary on every trial, their cognitive resources are strained by lexical-semantic cues rather than acoustic-phonetic cues. Thus, they do not show any adaptation effect; every trial is effectively a novel trial for them. Goal 4 corresponds to this research contribution brought about by the experiments described mainly in Chapters 6 and 7. Change in information flow and its effect on top-down prediction We show that different rates of information flow — increase or decrease in the rate of speech — have different effects on language comprehension. Intelligibility of speech decreases with both increase and decrease of speech rate. However, the increase in speech rate is particularly detrimental to comprehension of degraded speech as it increases the difficulty in processing sentences with less predictable endings. This is one of the few studies highlighting the role of speed of flow of information in the contextual facilitation of degraded speech. Goal 5 corresponds to this research contribution brought about by the experiment described in Chapter 7. A metric of language comprehension We propose and successfully use a metric of language comprehension that reflects listeners’ use of context information. This metric does not merely measure how many words are correctly identified. Instead, it considers the fact that in the study of the effect of predictability, how well a context is recognised should also be taken into account. Thus, it measures word recognition accuracy in the sentences in which context is correctly recognised. Using such a metric improves the interpretation of contextual facilitation across studies, which is lacking in the extant literature. Goal 6 corresponds to this research contribution brought about by consistent use of this metric in Chapters 6 and 7. 1.3 Overview of the thesis The central theme of this thesis is the study of predictive processing in language comprehension across a noisy channel. On the grounds of predictive language processing and the noisy channel model of communication, we investigate how and to what extent listeners use context information while listening to degraded speech. We replicate and extend prior findings, which claim that predictability facilitates language comprehension at moderate levels of speech degradation. Furthermore, the boundary conditions of predictive processing are tested, examining the effect of different rates of information flow in the predictability effect. We test for the presence of perceptual adaptation and find evidence against the learning effect and adaptation to degraded speech. Chapter 2 provides a background on the rest of the chapters. It provides an overview of degraded speech comprehension and predictive language processing. The current status of the debate on these topics is also presented. Chapter 3 describes the stimuli used in all the experiments in this thesis. It describes the process of stimuli creation and speech processing, and provides an overview of online data collection. Chapter 4 describes the statistical tests employed for data analyses. Binomial logistic mixed effects modelling is performed on the data from all the experiments. This chapter provides a background on this statistical procedure and how it is operated on the statistical software R. Chapter 5 presents two experiments that address the first and the third research goal. These experiments are conducted to examine the predictability effect in degraded speech comprehension and the role of auditory attention. Participants in both experiments are presented with the speech degraded at different levels of degradation and sentences of different levels of predictability. Participants in Experiment 1 are asked to type in only the final word of a sentence; this did not bind their attention to the sentence context. In contrast, the participants in Experiment 2 are asked to type in the entire sentence that they heard, which required them to attend to the sentence context as well. We replicate the previously reported predictability effects in the noisy channel only when participants attended to the entire sentence, including the context. We show that top-down predictions cannot be generated at moderate levels of degradation when insufficient attention is given to context. We discuss the limitation in the existing theories of predictive language processing, which commit to the automaticity of prediction. We show the importance of attention in language comprehension. We end this chapter with the note that the measurement of language comprehension can be further refined and the nature of the predictability effect tested. Chapter 6 addresses the first, the second, the fourth, and the sixth research goals. The predictability effect partially replicated in Chapter 5 is further examined in this chapter. We use a refined metric of measurement of language comprehension that takes into consideration whether listeners correctly identified the context. We observe predictability effects at a moderate level of speech degradation, thereby consistently replicating the facilitatory effect of predictability. We find the predictability effects to be graded in nature and discuss it in the light of existing theories of predictive processing. We also show that regardless of the certainty about the next-trial degradation level, listeners do not adapt to degraded speech when its lexical-semantic property varies every trial. At the end of this chapter, we note the intrinsic difficulty of processing degraded speech and open the question that the predictability effects could be further enhanced (or limited) with more (or less) time available to process the degraded speech. Chapter 7 addresses the questions raised in Chapter 6. In two experiments, it addresses the fourth, the fifth, and the sixth research goals. We use the same metric of measurement of language comprehension as the one used in Chapter 6, which takes into account listeners’ correct identification of the context. Listeners are presented with the moderately degraded speech at which the predictability effect is observed in Chapter 6. In Experiment 1, the moderately degraded speech is presented at normal and fast speech rates. In Experiment 2, the speech rates are normal and slow. For fast speech, both intelligibility and the predictability effect are reduced, driven by the difficulty in processing words that are less predictable from the context. Although more time is available to process the context of the degraded speech at a slow speech rate, there is no increase in the facilitatory effect of predictability with a reduced speech rate; instead, intelligibility is reduced in slow speech compared to normal speech. This chapter reflects on the limitations of predictive processing driven by the constraints in cognitive resources. Chapter 8 summarises the findings of all the studies. It concludes with the closing remarks on the limitations of the the studies, theoretical and practical implications, and the direction for future research. 1.4 Dissemination of research findings Some of the findings reported in this thesis are presented and published elsewhere to disseminate scientific findings to a broader audience. The presentations and publications that report on parts of the research described in this thesis are outlined below. Research articles: Bhandari, P., Demberg, V., &amp; Kray, J. (under review) Speaking fast and slow: How speech rate affects contextual facilitation in degraded speech comprehension. Bhandari, P., Demberg, V., &amp; Kray, J. (2022). Predictability effects in degraded speech comprehension are reduced as a function of attention. Language and Cognition, 1-18. doi:10.1017/langcog.2022.16 Bhandari, P., Demberg, V., &amp; Kray, J. (2021). Semantic predictability facilitates comprehension of degraded speech in a graded manner. Frontiers in Psychology, 12:714485. doi:10.3389/fpsyg.2021.714485 Conference presentations: Bhandari, P., Demberg, V., &amp; Kray, J. (2022). The effect of speech rate on contextual facilitation of degraded speech comprehension. Architectures and Mechanisms for Language Processing, 2022-09-07–2022-09-09. Bhandari, P., Demberg, V., &amp; Kray, J. (2022). Predictability effects in degraded speech comprehension are reduced as function of attention. Architectures and Mechanisms for Language Processing, 2022-09-07–2022-09-09. Bhandari, P., Demberg, V., &amp; Kray, J. (2022). The effect of speech rate in comprehension of degraded speech. International Max Planck Research School (IMPRS) Conference, 2022-06-01–2022-06-03. Bhandari, P., Demberg, V., &amp; Kray, J. (2022). Predictability effects are reduced as a function of attention. Annual Convention of Association for Psychological Science, 2022-05-25–2022-05-28. Bhandari, P., Demberg, V., &amp; Kray, J. (2021). Predictability facilitates comprehension but not adaptation to degraded speech in a graded manner. Conference of the Society for the Neurobiology of Language, 2021-10-05–2021-10-08. Bhandari, P., Demberg, V., &amp; Kray, J. (2021). Predictability facilitates comprehension of degraded speech in a graded manner. Annual Meeting of Cognitive Neuroscience Society, 2021-03-13–2021-03-16. Bibliography Aarts, A. A., Anderson, J. E., Anderson, C. J., Attridge, P. R., Attwood, A., Axt, J., Babel, M., Bahník, Š., Baranski, E., Barnett-Cowan, M., Bartmess, E., Beer, J., Bell, R., Bentley, H., Beyan, L., Binion, G., Borsboom, D., Bosch, A., Bosco, F. A., … Zuni, K. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Altmann, G. T. M., &amp; Kamide, Y. (2007). The real-time mediation of visual attention by language and world knowledge: Linking anticipatory (and other) eye movements to linguistic processing. Journal of Memory and Language, 57(4), 502–518. https://doi.org/10.1016/j.jml.2006.12.004 Amichetti, N. M., Atagi, E., Kong, Y.-Y., &amp; Wingfield, A. (2018). Linguistic context versus semantic competition in word recognition by younger and older adults with cochlear implants. Ear &amp; Hearing, 39(1), 101–109. https://doi.org/10.1097/aud.0000000000000469 Ankener, C. S. (2019). The influence of visual information on word predictability and processing effort [Doctoral dissertation]. Saarland University; Saarländische Universitäts-und Landesbibliothek. Bruineberg, J., Dolega, K., Dewhurst, J., &amp; Baltieri, M. (2021). The emperor’s new markov blankets. Behavioral and Brain Sciences, 1–63. https://doi.org/10.1017/S0140525X21002351 Chen, F., &amp; Loizou, P. C. (2011). Predicting the intelligibility of vocoded speech. Ear and Hearing, 32(3), 331. https://doi.org/10.1097/AUD.0b013e3181ff3515 Chingacham, A., Demberg, V., &amp; Klakow, D. (2021). Exploring the potential of lexical paraphrases for mitigating noise-induced comprehension errors. Proc. Interspeech 2021, 1713–1717. https://doi.org/10.21437/Interspeech.2021-306 Christiansen, M. H., &amp; Chater, N. (2015). The Now-or-Never bottleneck: A fundamental constraint on language. Behavioral and Brain Sciences, 39, 1–72. https://doi.org/10.1017/S0140525X1500031X Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences, 36(3), 181–204. https://doi.org/10.1017/s0140525x12000477 Cockburn, A., Dragicevic, P., Besançon, L., &amp; Gutwin, C. (2020). Threats of a replication crisis in empirical computer science. Communications of the ACM, 63(8), 70–79. https://doi.org/10.1145/3360311 Darwiche, A. (2010). Bayesian networks. Communications of the ACM, 53(12), 80–90. https://doi.org/10.1145/1859204.1859227 Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 DeLong, K. A., Urbach, T. P., &amp; Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8), 1117–1121. https://doi.org/10.1038/nn1504 Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, J. A., Conway, J. G., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012 Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Federmeier, K. D., Kutas, M., &amp; Schul, R. (2010). Age-related and individual differences in the use of prediction during language comprehension. Brain and Language, 115(3), 149–161. https://doi.org/10.1016/j.bandl.2010.07.006 Ferreira, F., &amp; Clifton Jr, C. (1986). The independence of syntactic processing. Journal of Memory and Language, 25(3), 348–368. https://doi.org/10.1016/0749-596X(86)90006-9 Ferreira, F., &amp; Lowder, M. W. (2016). Prediction, information structure, and good-enough language processing (Vol. 65, pp. 217–247). Elsevier Ltd. https://doi.org/10.1016/bs.plm.2016.04.002 Fontan, L., Tardieu, J., Gaillard, P., Woisard, V., &amp; Ruiz, R. (2015). Relationship between speech intelligibility and speech comprehension in babble noise. Journal of Speech, Language, and Hearing Research, 58(3), 977–986. https://doi.org/10.1044/2015_jslhr-h-13-0335 Friston, K. J., Parr, T., Yufik, Y., Sajid, N., Price, C. J., Holmes, E., &amp; Square, Q. (2020). Generative models, linguistic communication and active inference. Neuroscience &amp; Biobehavioral Reviews, 118, 42–64. https://doi.org/10.1016/j.neubiorev.2020.07.005 Friston, K. J., Sajid, N., Quiroga-Martinez, D. R., Parr, T., Price, C. J., &amp; Holmes, E. (2020). Active listening. Hearing Research, xxxx, 107998. https://doi.org/10.1016/j.heares.2020.107998 Gibson, E., Bergen, L., &amp; Piantadosi, S. T. (2013). Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. Proceedings of the National Academy of Sciences, 110(20), 8051–8056. https://doi.org/10.1073/pnas.1216438110 Gibson, E., Futrell, R., Piantadosi, S. P., Dautriche, I., Mahowald, K., Bergen, L., &amp; Levy, R. (2019). How efficiency shapes human language. Trends in Cognitive Sciences, 23(5), 389–407. https://doi.org/10.1016/j.tics.2019.02.003 Guest, O., &amp; Martin, A. E. (2021). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science, 16(4), 789–802. https://doi.org/10.1177/1745691620970585 Hale, J. (2001). A probabilistic earley parser as a psycholinguistic model. Second Meeting of the North American Chapter of the Association for Computational Linguistics, 1–8. https://doi.org/10.3115/1073336.1073357 Hauser, M. D., Chomsky, N., &amp; Fitch, W. T. (2002). The faculty of language: What is It, who has it, and how did it evolve? Science, 298(5598), 1569–1579. https://doi.org/10.1126/science.298.5598.1569 Huettig, F., &amp; Guerra, E. (2019). Effects of speech rate, preview time of visual context, and participant instructions reveal strong limits on prediction in language processing. Brain Research, 1706(June 2017), 196–208. https://doi.org/10.1016/j.brainres.2018.11.013 Huettig, F., &amp; Mani, N. (2016). Is prediction necessary to understand language? Probably not. Language, Cognition and Neuroscience, 31(1), 19–31. https://doi.org/10.1080/23273798.2015.1072223 Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., &amp; De Lange, F. P. (2012). Attention reverses the effect of prediction in silencing sensory signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310 Kuperberg, G. R. (2021). Tea with milk? A hierarchical generative framework of sequential event comprehension. Topics in Cognitive Science, 13(1), 256–298. https://doi.org/10.1111/tops.12518 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Lerner, Y., Honey, C. J., Katkov, M., &amp; Hasson, U. (2014). Temporal scaling of neural responses to compressed and dilated natural speech. Journal of Neurophysiology, 111(12), 2433–2444. https://doi.org/10.1152/jn.00497.2013 Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3), 1126–1177. https://doi.org/10.1016/j.cognition.2007.05.006 Lieberman, P. (2013). The unpredictable species. In The unpredictable species. Princeton University Press. Luke, S. G., &amp; Christianson, K. (2016). Limits on lexical prediction during reading. Cognitive Psychology, 88, 22–60. https://doi.org/10.1016/j.cogpsych.2016.06.002 Lupyan, G., &amp; Clark, A. (2015). Words and the World. Current Directions in Psychological Science, 24(4), 279–284. https://doi.org/10.1177/0963721415570732 Markman, A. B., &amp; Otto, A. R. (2011). Cognitive systems optimize energy rather than information. Behav. Brain Sci, 34(207), 10–1017. https://doi.org/10.1017/S0140525X11000355 McClelland, J. L., &amp; Elman, J. L. (1986). The TRACE model of speech perception. Cognitive Psychology, 18(1), 1–86. https://doi.org/10.1016/0010-0285(86)90015-0 Minocher, R., Atmaca, S., Bavero, C., McElreath, R., &amp; Beheim, B. (2021). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450 Mishra, R. K., Singh, N., Pandey, A., &amp; Huettig, F. (2012). Spoken language-mediated anticipatory eye- movements are modulated by reading ability - Evidence from Indian low and high literates. Journal of Eye Movement Research, 5(1), 1–10. https://doi.org/10.16910/jemr.5.1.3 Nieuwland, M. S., Barr, D. J., Bartolozzi, F., Busch-Moreno, S., Darley, E., Donaldson, D. I., Ferguson, H. J., Fu, X., Heyselaar, E., Huettig, F., Husband, E. M., Ito, A., Kazanina, N., Kogan, V., Kohút, Z., Kulakova, E., Mézière, D., Politzer-Ahles, S., Rousselet, G., … Von Grebmer Zu Wolfsthurn, S. (2020). Dissociable effects of prediction and integration during language comprehension: Evidence from a largescale study using brain potentials. Philosophical Transactions of the Royal Society B: Biological Sciences. https://doi.org/10.1098/rstb.2018.0522 Norris, D., McQueen, J. M., &amp; Cutler, A. (2016). Prediction, Bayesian inference and feedback in speech recognition. Language, Cognition and Neuroscience, 31(1), 4–18. https://doi.org/10.1080/23273798.2015.1081703 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential reasoning. Proceedings of the 7th Annual Conference of the Cognitive Science Society, 329–334. Peelle, J. E. (2013). Cortical responses to degraded speech are modulated by linguistic predictions. Proceedings of Meetings on Acoustics ICA2013, 19, 060108. Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Pinker, S., &amp; Jackendoff, R. (2005). The faculty of language: what’s special about it? Cognition, 95(2), 201–236. https://doi.org/10.1016/j.cognition.2004.08.004 Rosen, S., Faulkner, A., &amp; Wilkinson, L. (1999). Adaptation by normal listeners to upward spectral shifts of speech: Implications for cochlear implants. The Journal of the Acoustical Society of America, 106(6), 3629–3636. https://doi.org/10.1121/1.428215 Ryskin, R., Futrell, R., Kiran, S., &amp; Gibson, E. (2018). Comprehenders model the nature of noise in the environment. Cognition, 181(July 2017), 141–150. https://doi.org/10.1016/j.cognition.2018.08.018 Sanderson, S. K., &amp; Roberts, W. W. (2008). The evolutionary forms of the religious life: A cross-cultural, quantitative analysis. American Anthropologist, 110(4), 454–466. https://doi.org/10.1111/j.1548-1433.2008.00078.x Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(4), 623–656. https://doi.org/10.1002/j.1538-7305.1948.tb00917.x Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008a). Priming and sentence context support listening to noise-vocoded speech by younger and older adults. The Journal of the Acoustical Society of America, 123(1), 489–499. https://doi.org/10.1121/1.2783762 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 van Os, M., Kray, J., &amp; Demberg, V. (2021b). Recognition of minipairs in (un)predictive sentence contexts in two types of noise. Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 43(43), 2943–2949. The mathematical formalisation presented in this thesis is intended to introduce and conceptually clarify the noisy channel model of communication and not to simulate or model the communication that occurs in a noisy channel. A verbal expression could also serve the purpose, but a mathematical formalisation and potential computational modelling (e.g., Chingacham et al., 2021) pave the way towards quantifying and predicting unobserved future events and a better theory building (Guest &amp; Martin, 2021). ↩︎ "],["chapter-background.html", "2 Background 2.1 Speech distortion and degradation 2.2 Prediction and comprehension of degraded speech 2.3 Adaptation to degraded speech 2.4 Summary", " 2 Background In the previous chapter, we outlined the theoretical background and the research goals of the studies in this dissertation. We stated that the central theme of this thesis is to investigate the interaction between top-down predictive and bottom-up auditory processes in language comprehension. Building on the noisy channel model of communication and predictive language processing, the studies in this thesis manipulate the auditory processes and prior information (in the form of semantic context available in a sentence). In this chapter, we provide background on the noisy channel that was created and used to introduce variations in the bottom-up processing in the studies presented in this thesis. We also elaborate on the predictive language processing in the noisy channel and the evidence of its limits and nature. Understanding these fundamental concepts of top-down and bottom-up processes is essential for the chapters that follow; these concepts are briefly reiterated in the following chapters wherever relevant. Additionally, this chapter outlines the gaps in previous research that this thesis fills in. 2.1 Speech distortion and degradation Most of the existing frameworks of spoken language comprehension are inspired by the experiments conducted with clean speech, the condition of “artificial normalcy” (Mattys et al., 2012). However, spoken language communication generally occurs outside the artificial normalcy, alongside different sources of noise and disruption. Probabilistic models of language comprehension, like the noisy channel model of communication (Gibson et al., 2013, 2019; Levy, 2008; C. E. Shannon, 1948) in Figures 1.1 and 1.2 show that the speech signal uttered by a speaker gets disrupted and distorted due to noise (\\(u_i\\rightarrow u_p\\leftarrow N\\)). Distortion can occur at these three points or sources: encoding, transmission, and decoding (Mattys et al., 2012). Speech can be distorted while encoding an utterance/signal due to the variability in speakers’ production, like accented or slow and fast speech. Distortion can arise while decoding the perceived signal due to listener-related factors, like hearing loss or auditory processing disorder. It can also result from an external noise that appears during the transmission, like ambient noise or poor transmission medium (e.g., distortion in the telephone line). These different sources of distortion make a listening condition adverse by affecting the time and frequency-related properties/cues of the speech signal, i.e., temporal envelope cues and spectral details of speech, respectively. The temporal envelope cues are the slow variations in the amplitude of the speech signal over time (Moon et al., 2014; Moon &amp; Hong, 2014), while the spectral details are the frequency-specific information about the speech. The temporal envelope cues reflect the prosodic information of the speech and are used in lexical-semantic and syntactic processing (Greenberg, 1996; Schneider &amp; Pichora-Fuller, 2001; Sheldon et al., 2008b). The spectral details provide information about the sound production reflecting the vocal tract’s resonant properties, speech signal frequency range, energy distribution across frequency bands, etc. (Roberts et al., 2011; R. V. Shannon et al., 1995; R. V. Shannon et al., 2004). In an experimental setup, a noisy channel can be created artificially by digital signal processing (see Section 3.1.2) to investigate the response of the speech perception system to distorted speech and to study language comprehension in an adverse listening condition. For example, signal compression or expansion acts upon the temporal property of the speech and makes it fast or slow (i.e., change its speed), and an optimal level of speech expansion/compression does not distort the spectral property of speech (see Section 3.1.2.2). In addition to speech compression and expansion in Chapter 7, throughout the studies in this thesis, we implement noise-vocoding to manipulate the spectral property of speech and create a noisy channel of communication. Noise-vocoding removes the spectral details of the speech signal in a controlled manner, only leaving intact its temporal and periodicity cues (see Section 3.1.2.1). This method of speech degradation was initially developed as a means to reduce the information in speech signals to be transmitted through the telephone line (Clendeninn, 1940; Dudley, 1939). Shannon and colleagues later modified and used this technique as an analogue to cochlear implants such that the number of channels used in a cochlear implant is similar to the number of noise-vocoding channels in terms of their speech output and intelligibility (Loizou et al., 1999; R. V. Shannon et al., 1995; R. V. Shannon et al., 2004; cf. Orena &amp; Colby, 2021). Therefore, in addition to being a method of speech distortion to parametrically vary and control the quality of speech signals in a graded manner, noise-vocoding is also a method of distortion that is used to understand the speech perception and language comprehension in cochlear implantees (e.g., Patro &amp; Mendel, 2020; R. V. Shannon et al., 2004; Winn, 2016) One of the main factors that determine the intelligibility of degraded speech is the number of noise-vocoding channels.2 The higher the number of noise-vocoding channels, the more is the frequency-specific information available in the degraded speech, and the higher is the intelligibility compared to the speech that is degraded with a lesser number of noise-vocoding channels. For example, listeners have been shown shown to rate 8-channel noise-vocoded speech to be more intelligible and less effortful than 2-channel noise-vocoded speech (e.g., Obleser &amp; Kotz, 2011; Sohoglu et al., 2012). In our studies, we create a noisy channel with different degradation levels and intelligibility by noise-vocoding the speech signal through 1, 4, 6 and 8 channels. The details of the artificial distortions are described in Chapter 3. 2.2 Prediction and comprehension of degraded speech In addition to the quality of speech signals, listeners rely on context information and form top-down predictions to understand speech in adverse listening conditions. Below, we first review the role of predictions in language comprehension in general, and then we discuss the role of top-down predictive processes in comprehension of degraded speech in particular. 2.2.1 Predictive language processing Research from various domains of cognitive (neuro)science, like emotion, vision, odour, and proprioception (the sensation of one’s body position and movement, Tuthill &amp; Azim, 2018), has shown that perception and cognition can be described under the framework of predictive processing; they primarily operate by predicting upcoming events (A. Clark, 2013; Marques et al., 2018; Seth, 2013; Stadler et al., 2012; cf. Bowers &amp; Davis, 2012; Jones &amp; Love, 2011; Pierce &amp; Ollason, 1987). Despite a long-standing scepticism and doubt about the usefulness of prediction in language processing (Forster, 1981; Jackendoff, 2002; Van Petten &amp; Luka, 2012), human language comprehension too has been claimed to be predictive in nature from as early as the mid-twentieth century (e.g., McCullough, 1958; Miller et al., 1951; Morton, 1964) which in recent days has received overwhelming support from computational linguistics, psycholinguistics and cognitive neuroscience of language (e.g., DeLong et al., 2005; Demberg et al., 2013; Heyselaar et al., 2021; Lupyan &amp; Clark, 2015; Pickering &amp; Gambi, 2018). Empirical evidence from several studies suggests that readers and listeners predict upcoming words in a sentence when the words are predictable from the preceding context (Kuperberg &amp; Jaeger, 2016; Nieuwland, 2019; for reviews, Staub, 2015). For instance, predictable words are skipped and read faster than the words that are less predictable from the context (Ehrlich &amp; Rayner, 1981; Frisson et al., 2005; Staub, 2011). In the visual world paradigm, studies have demonstrated that individuals show anticipatory eye movements towards a picture of an object (e.g., cake) that is predictable from the preceding sentence context (e.g., The boy will eat a…) even before hearing the final target word (Altmann &amp; Kamide, 1999; Ankener et al., 2018; Kamide et al., 2003). Similar results have been observed in a virtual world setup with naturalistic scenes (e.g., Heyselaar et al., 2021). The sentence-final word in a highly constraining sentence (e.g., “She dribbles a ball.”) elicits a smaller N400 amplitude3 than a less constraining sentence (e.g., “She buys a ball.”, Federmeier et al., 2007a; Kutas &amp; Hillyard, 1984). Similarly, event-related words (e.g., “luggage”) elicited reduced N400 compared to event-unrelated words (e.g., “vegetables”), which were not predictable from the context (e.g., in the event of “travel”, Metusalem et al., 2012). In sum, as the sentence context builds up, listeners make predictions about upcoming words in the sentence, and these, in turn, facilitate language comprehension. That is, individuals use the context available to them to generate predictions that aids understanding of written and spoken language. 2.2.1.1 But, what is prediction? The history of prediction in language science is rocky (Husband &amp; Bovolenta, 2020). People have been sceptical that language processing is predictive in nature. Different people mean different things when they use the word prediction. As Kuperberg &amp; Jaeger (2016) put it, prediction has become a loaded term; it is used alongside other similar terms like integration (Federmeier, 2007), anticipation, expectation (Van Petten &amp; Luka, 2012), preparedness (Ferreira &amp; Chantavarin, 2018), etc. This thesis uses the word prediction in the following minimal sense. As a sentence unfolds, listeners encounter the context information in the sentence and form its meaning representation, i.e., an internal representation of the context. Before they hear the next word, i.e., before they encounter new bottom-up information, they generate an expectation4 about the new word based on the meaning representation of the context. They could form a prediction about only the semantic feature of the next word, or they could predict the exact word (meaning prediction vs word-form prediction). In reading studies and clean speech comprehension, there are opposing views. One view is that the comprehenders predictively preactivate the upcoming linguistic unit solely based on the top-down information (i.e., predictive preactivation). In contrast, the opposing view is that the comprehenders wait for the bottom-up information to activate the representation (e.g., phonological and semantic representation) of the new information and its neighbours5, then use the top-down information to select the best representation. To clarify it further, let’s take the example sentence (1) presented in Chapter 1 on page : The day was breezy so the boy went outside to fly a___. Upon listening to this context, the listeners can form a high degree of belief that the next word will be “kite”. Before even hearing it, listeners preactivate the representation of “kite” in their mental lexicon. Alternatively, they could wait until they hear the auditory input “kite”, which activates “kite” and its phonological (and semantic) neighbours in the mental lexicon, then use the top-down information to select the most likely word that completes the sentence. Either way, top-down processes facilitate comprehension. While listening in an adverse condition, it is unlikely that a listener follows the latter strategy of waiting for the bottom-up input to activate the representations and then selecting the most likely one based on the top-down information (Kuperberg &amp; Jaeger, 2016). When speech is distorted, it is difficult to form the context representation in the first place (cf. cue-based retrieval, Kaufeld, 2021; Martin, 2016). Once a listener has formed a meaning representation of the context, she cannot afford to again wait for the bottom-up input to activate phonological and semantic representations of upcoming words; the uncertainty about the bottom-up information is persistent (see the phoneme restoration effect (Warren, 1970), the McGurk effect (McGurk &amp; MacDonald, 1976), and the Ganong effect (Ganong, 1980) in speech perception). Thus, once the listener has formed a representation of the context, she uses this top-down information to predictively preactivate what the upcoming word can be. Such predictive preactivation can take different forms: it can be a probabilistic (graded) or deterministic (all-or-nothing) prediction. These differences in the nature of prediction are discussed below. 2.2.2 Facilitatory effect of predictability We have discussed above that individuals make predictions about not-yet-encountered linguistic units based on available context information as a sentence unfolds: Top-down predictive and bottom-up perceptual processes interact dynamically in language comprehension. When the bottom-up perceptual input is less reliable, for example, in an adverse listening condition, it has been shown that listeners rely more on top-down processes by narrowing down the predictions to smaller sets of semantic categories or words (Corps &amp; Rabagliati, 2020; Strauß et al., 2013). Obleser and colleagues (Obleser et al., 2007; Obleser &amp; Kotz, 2010), for instance, used sentences of two levels of semantic predictability (high and low) and systematically degraded speech signals by passing them through various numbers of noise-vocoding channels ranging from 1 to 32 in a series of behavioural and neuroimaging studies (see also Hunter &amp; Pisoni, 2018). They found that semantic predictability facilitated language comprehension only at moderate levels of speech degradation. That is, participants relied more on the sentence context when the speech signal was degraded though intelligible enough than when it was not degraded or highly degraded. At such moderate levels of speech degradation, word recognition accuracy was found to be higher for the words in high predictability sentences than the words in low predictability sentences (Obleser &amp; Kotz, 2010). For the extremes, i.e., when the speech signal was highly degraded (making the speech almost entirely unintelligible) or when it was the least degraded (rendering the speech intelligible), the word recognition accuracy was similar across both levels of sentence predictability, meaning that predictability did not facilitate language comprehension. Sheldon et al. (2008b) estimated that for both younger and older adults, the number of noise-vocoding channels required to achieve 50% accuracy varied as a function of sentence context. A higher number of channels (i.e., more bottom-up information) was required in less constraining sentences to achieve the same level of accuracy as highly constraining sentences. They also concluded that word recognition is facilitated by predictability and sentence context when the speech is degraded. Taken together, these studies conclude that at moderate levels of degradation, participants rely more on the top-down predictions generated by a sentence context and less on the bottom-up perceptual processing of an unclear, less reliable, and degraded speech signal (Obleser, 2014). However, these studies are agnostic about the nature of prediction, i.e., if it is probabilistic or deterministic. 2.2.2.1 Nature of prediction A debate in the literature on predictive language processing pertains to this question: Is prediction probabilistic, or is it an all-or-nothing phenomenon? For instance, the garden path phenomenon was explained as a parser’s irreversible prediction about the sentence structure; if its predicted parsing fails (or if it turns out to be incorrect), then the parser reanalyzes the sentence and reformulates another prediction (e.g., Ferreira &amp; Clifton Jr, 1986; see also Demberg et al., 2013; Slattery et al., 2013). In recent days, the support for the probabilistic nature of prediction comes, for example, from ERP studies that show an inverse and graded relationship between the magnitude of the N400 effect evoked by a word and its predictability measured by cloze probability6 (e.g., DeLong et al., 2005; Federmeier et al., 2007b), or surprisal7 (Frank et al., 2015; cf. Brothers et al., 2015). These discussions come from reading studies and spoken language comprehension in clear speech. Although a few frameworks of language processing speculate that language comprehension in adverse listening conditions can be predictive (e.g., Lowder &amp; Ferreira, 2016; Ryskin et al., 2018), so far, only Strauß et al. (2013) have investigated the nature of prediction in degraded speech comprehension. They proposed an “expectancy searchlight model”, which suggests that listeners form narrowed expectations from a restricted semantic space only when the sentence endings are highly predictable. They rule out the graded nature of predictability. In contrast to their study, we systematically vary the predictability of the target word and examine the graded vs probabilistic nature of prediction in degraded speech comprehension. We argue that the facilitatory effect of predictability is graded in nature; it is not an all-or-nothing phenomenon focused solely on highly predictable sentence endings. 2.2.3 Limits of predictive language processing It is important to note and acknowledge that the ubiquity and universality of predictive language processing have not gone unquestioned (Huettig &amp; Mani, 2016). Apart from the debate on the nature of prediction, which we will come to later in this chapter, there is compelling evidence that questions the necessity of prediction in language comprehension. For example, Mishra et al. (2012) showed that literacy is a critical factor that limits listeners’ predictions about an upcoming word. In a visual world paradigm study, they found that individuals with lower literacy showed less anticipatory eye movements than those with higher literacy. They bolstered their finding in a neuroimaging study claiming that learning to read fundamentally changes the neural circuitry (Hervais-Adelman et al., 2019). It is, therefore, plausible that such structural change in the brain manifests in linguistic behaviour. Similarly, Scholman et al. (2020) demonstrated that reading experience is predictive of readers’ sensitivity to discourse signals available in the context for predictiong upcoming content. Cognitive ageing is also reported as a limiting factor in generating predictions (Federmeier et al., 2002, 2010). Another line of argument that critiques predictive processing comes from the observations of Huettig &amp; Guerra (2019). They analyzed participants’ anticipatory eye movements in the visual world paradigm and showed that listeners predict the target word only in an artificial setup — long preview time (4000ms) and slow speech (cf. Fernandez et al., 2020; Heyselaar et al., 2021). When presented with a short preview time (1000ms), such anticipatory eye movements were not significant towards the picture of the target word. In this thesis, we study additional top-down and bottom-up processes that can interact with and potentially limit the facilitatory effect of predictability. For example, current theories of predictive processing are poor in explaining the role of attention in semantic prediction (e.g., Christiansen &amp; Chater, 2015; Ferreira &amp; Lowder, 2016; Friston, Sajid, et al., 2020; Kuperberg &amp; Jaeger, 2016; Pickering &amp; Gambi, 2018). For example, in their prediction-by-production account, Pickering &amp; Gambi (2018) emphasize that listeners use their speech production mechanism in speech perception and comprehension to predict what their interlocutor will say next. Their framework attempts to paint a big picture of prediction — using the motor system — but it does not consider how a listener’s strategy of attending to only a part of a speech stream in adverse listening conditions influences linguistic predictions. We argue that attention to context information is critical in forming semantic predictions, especially in degraded speech comprehension (cf. Kok et al., 2012). By manipulating listeners’ attention allocation to parts of a speech stream and information content in the sentences, we show that attention to context information is a prerequisite for the listeners to generate predictions. We also investigate the effect of bottom-up processes, like speech rate, on top-down processes (i.e., predictability effect in degraded speech comprehension). The extant findings on the effects of speech rate on the facilitatory effect of predictability have been mixed both in clear and degraded speech comprehension (e.g., Aydelott &amp; Bates, 2004; Goy et al., 2013; Iwasaki et al., 2002; Winn &amp; Teece, 2021). We demonstrate a scope for current theories of predictive language processing to incorporate the instances of varying predictability effects at fast and slow speech rates and the effects of attention on degraded speech comprehension. 2.3 Adaptation to degraded speech Listeners quickly adapt to novel speech with artificial acoustic distortions (Dupoux &amp; Green, 1997). Repeated exposure to distorted speech improves listeners’ comprehension improves over time (Guediche et al., 2014; for reviews, see Samuel &amp; Kraljic, 2009). When the noise condition, like speech degradation level, is constant throughout the experiment, listeners adapt to it, and the performance (e.g., word recognition) improves with as little as 20 minutes of exposure (Rosen et al., 1999). For example, Davis et al. (2005) presented listeners with 6-channel noise-vocoded speech and found an increase in the proportion of correctly reported words over the course of the experiment. Similarly, Erb et al. (2013) presented participants with 4-channel noise-vocoded speech and reached a similar conclusion. In these experiments, only one speech degradation level (6- or 4-channel noise-vocoded speech) was presented in one block. There was no uncertainty about the next-trial speech degradation from the participants’ perspective. In contrast to our study, semantic feature (i.e., target word predictability) was not varied. When multiple types or levels of degraded speech signals are presented in a (pseudo-)randomized order within a block, a listener is uncertain about the signal quality of any upcoming trial. This can influence perceptual adaptation such that it might be totally absent with the change in the characteristics of the auditory signals throughout an experiment (Mattys et al., 2012). In addition, trial-by-trial variability in the characteristics of distorted speech can impair word recognition (Sommers et al., 1994; see also Dahan &amp; Magnuson, 2006). Only a limited number of studies have looked at how the (un)certainty about next-trial speech quality and semantic features influence adaptation. For example, in a word-recognition task, Vaden et al. (2013) presented words at +3dB SNR and +10dB SNR in a pseudo-random order; the goal was to minimize the certainty about the noise level within the block. They report that the magnitude and coherence of the activity in the cingulo-opercular network facilitated comprehension of noisy speech in a subsequent trial, however, we cannot make a firm conclusion about perceptual adaptation per se from their studies as they do not report the performance change throughout the experiment. Similarly, Obleser and colleagues (Hartwigsen et al., 2015; Obleser et al., 2007; Obleser &amp; Kotz, 2010) also presented listeners with noise-vocoded sentences (ranging from 2 to 32 channels noise-vocoding) in a pseudo-randomized order but did not report the presence or absence of perceptual adaptation. Their findings are primarily focused on the interaction of lexical-semantic and acoustic-phonetic cues in speech perception. On the one hand, repeated exposure is shown to lead to perceptual adaptation to degraded speech. On the other hand, uncertainty about speech quality is speculated to impair word recognition. We argue that a trial-by-trial variation in a higher-level semantic feature of speech hinders listeners’ perceptual system from retuning itself to adapt to the lower-level auditory features of the degraded speech (cf. Nahum et al., 2008). In contrast to prior studies, we show that listeners do not adapt to degraded speech despite repeated exposure to the same degraded speech as long as its semantic predictability is uncertain. 2.4 Summary In this chapter, we provided an overview of the concepts that will be repeated in the following chapters. We introduced the concept of speech distortion and degradation. Digital signal processing methods used in this process will be discussed in Chapter 3 (Section 3.1.2). Importantly, we provided an overview of how predictive language processing aids language comprehension, as well as its limitations. We discussed perceptual adaptation to degraded speech and the role of uncertainty about next-trial in adaptation. At each step, we presented the motivation behind the studies in this thesis and the gaps in the literature these studies fill in. In the next chapter, we will discuss the methods that are common in all the experiments (Chapters 5, 6, and 7) in developing materials and collecting data. Bibliography Altmann, G. T. M., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: restricting the domain of subsequent reference. Cognition, 73(3), 247–264. https://doi.org/10.1016/s0010-0277(99)00059-1 Ankener, C. S., Sekicki, M., &amp; Staudte, M. (2018). The influence of visual uncertainty on word surprisal and processing effort. Frontiers in Psychology, 9, 2387. https://doi.org/10.3389/fpsyg.2018.02387 Aydelott, J., &amp; Bates, E. (2004). Effects of acoustic distortion and semantic context on lexical access. Language and Cognitive Processes, 19(1), 29–56. https://doi.org/10.1080/01690960344000099 Bowers, J. S., &amp; Davis, C. J. (2012). Bayesian just-so stories in psychology and neuroscience. Psychological Bulletin, 138(3), 389–414. https://doi.org/10.1037/a0026450 Brothers, T., Swaab, T. Y., &amp; Traxler, M. J. (2015). Effects of prediction and contextual support on lexical processing: Prediction takes precedence. Cognition, 136, 135–149. https://doi.org/10.1016/j.cognition.2014.10.017 Christiansen, M. H., &amp; Chater, N. (2015). The Now-or-Never bottleneck: A fundamental constraint on language. Behavioral and Brain Sciences, 39, 1–72. https://doi.org/10.1017/S0140525X1500031X Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences, 36(3), 181–204. https://doi.org/10.1017/s0140525x12000477 Clendeninn, P. (1940). The vocoder. Nature, 145(3665), 157. https://doi.org/10.1038/145157a0 Corps, R. E., &amp; Rabagliati, H. (2020). How top-down processing enhances comprehension of noise-vocoded speech: Predictions about meaning are more important than predictions about form. Journal of Memory and Language, 113, 104114. https://doi.org/10.1016/j.jml.2020.104114 Dahan, D., &amp; Magnuson, J. S. (2006). Spoken word recognition. In M. J. Traxler &amp; M. A. Gernsbacher (Eds.), Handbook of psycholinguistics (2nd ed., pp. 249–283). Elsevier. https://doi.org/10.1016/b978-012369374-7/50009-2 Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 DeLong, K. A., Urbach, T. P., &amp; Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8), 1117–1121. https://doi.org/10.1038/nn1504 Demberg, V., Keller, F., &amp; Koller, A. (2013). Incremental, predictive parsing with psycholinguistically motivated tree-adjoining grammar. Computational Linguistics, 39(4), 1025–1066. Dudley, H. (1939). The vocoder. Bell Laboratories Record, 18(4), 122–126. Dupoux, E., &amp; Green, K. (1997). Perceptual adjustment to highly compressed speech: Effects of talker and rate changes. Journal of Experimental Psychology: Human Perception and Performance, 23(3), 914–927. https://doi.org/10.1037/0096-1523.23.3.914 Ehrlich, S. F., &amp; Rayner, K. (1981). Contextual effects on word perception and eye movements during reading. Journal of Verbal Learning and Verbal Behavior, 20(6), 641–655. https://doi.org/10.1016/S0022-5371(81)90220-6 Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Federmeier, K. D. (2007). Thinking ahead: The role and roots of prediction in language comprehension. Psychophysiology, 44(4), 491–505. https://doi.org/10.1111/j.1469-8986.2007.00531.x Federmeier, K. D., Kutas, M., &amp; Schul, R. (2010). Age-related and individual differences in the use of prediction during language comprehension. Brain and Language, 115(3), 149–161. https://doi.org/10.1016/j.bandl.2010.07.006 Federmeier, K. D., Mclennan, D., Ochoa, E. de, &amp; Kutas, M. (2002). The impact of semantic memory organization and sentence context information on spoken language processing by younger and older adults: An ERP study. Psychophysiology, 39(02), 133–146. https://doi.org/10.1111/1469-8986.3920133 Federmeier, K. D., Wlotko, E. W., De Ochoa-Dewald, E., &amp; Kutas, M. (2007a). Multiple effects of sentential constraint on word processing. Brain Research, 1146, 75–84. https://doi.org/10.1016/j.brainres.2006.06.101 Federmeier, K. D., Wlotko, E. W., De Ochoa-Dewald, E., &amp; Kutas, M. (2007b). Multiple effects of sentential constraint on word processing. Brain Research, 1146(1), 75–84. https://doi.org/10.1016/j.brainres.2006.06.101 Fernandez, L. B., Engelhardt, P. E., Patarroyo, A. G., &amp; Allen, S. E. M. (2020). Effects of speech rate on anticipatory eye movements in the visual world paradigm: Evidence from aging, native, and non-native language processing. Quarterly Journal of Experimental Psychology, 73(12), 2348–2361. https://doi.org/10.1177/1747021820948019 Ferreira, F., &amp; Chantavarin, S. (2018). Integration and prediction in language processing: A synthesis of old and new. Current Directions in Psychological Science, 27(6), 443–448. https://doi.org/10.1177/0963721418794491 Ferreira, F., &amp; Clifton Jr, C. (1986). The independence of syntactic processing. Journal of Memory and Language, 25(3), 348–368. https://doi.org/10.1016/0749-596X(86)90006-9 Ferreira, F., &amp; Lowder, M. W. (2016). Prediction, information structure, and good-enough language processing (Vol. 65, pp. 217–247). Elsevier Ltd. https://doi.org/10.1016/bs.plm.2016.04.002 Forster, K. I. (1981). Priming and the effects of sentence and lexical contexts on naming time: Evidence for autonomous lexical processing. The Quarterly Journal of Experimental Psychology, 33(4), 465–495. https://doi.org/10.1080/14640748108400804 Frank, S. L., Otten, L. J., Galli, G., &amp; Vigliocco, G. (2015). The ERP response to the amount of information conveyed by words in sentences. Brain and Language, 140, 1–11. https://doi.org/10.1016/j.bandl.2014.10.006 Frisson, S., Rayner, K., &amp; Pickering, M. J. (2005). Effects of contextual predictability and transitional probability on eye movements during reading. Journal of Experimental Psychology: Learning Memory and Cognition, 31(5), 862–877. https://doi.org/10.1037/0278-7393.31.5.862 Friston, K. J., Sajid, N., Quiroga-Martinez, D. R., Parr, T., Price, C. J., &amp; Holmes, E. (2020). Active listening. Hearing Research, xxxx, 107998. https://doi.org/10.1016/j.heares.2020.107998 Ganong, W. F. (1980). Phonetic categorization in auditory word perception. Journal of Experimental Psychology: Human Perception and Performance, 6(1), 110. https://doi.org/10.1037/0096-1523.6.1.110 Gibson, E., Bergen, L., &amp; Piantadosi, S. T. (2013). Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. Proceedings of the National Academy of Sciences, 110(20), 8051–8056. https://doi.org/10.1073/pnas.1216438110 Gibson, E., Futrell, R., Piantadosi, S. P., Dautriche, I., Mahowald, K., Bergen, L., &amp; Levy, R. (2019). How efficiency shapes human language. Trends in Cognitive Sciences, 23(5), 389–407. https://doi.org/10.1016/j.tics.2019.02.003 Goy, H., Pelletier, M., Coletta, M., &amp; Pichora-Fuller, M. K. (2013). The effects of semantic context and the type and amount of acoustic distortion on lexical decision by younger and older adults. Journal of Speech, Language, and Hearing Research, 56(6), 1715–1732. https://doi.org/10.1044/1092-4388(2013/12-0053) Greenberg, S. (1996). Auditory processing of speech. In N. J. Lass (Ed.), Principles of experimental phonetics (pp. 362–407). Mosby, St. Louis. Guediche, S., Blumstein, S. E., Fiez, J. A., &amp; Holt, L. L. (2014). Speech perception under adverse conditions: Insights from behavioral, computational, and neuroscience research. Frontiers in Systems Neuroscience, 7(Jan), 1–16. https://doi.org/10.3389/fnsys.2013.00126 Hale, J. (2001). A probabilistic earley parser as a psycholinguistic model. Second Meeting of the North American Chapter of the Association for Computational Linguistics, 1–8. https://doi.org/10.3115/1073336.1073357 Hartwigsen, G., Golombek, T., &amp; Obleser, J. (2015). Repetitive transcranial magnetic stimulation over left angular gyrus modulates the predictability gain in degraded speech comprehension. Cortex, 68, 100–110. https://doi.org/10.1016/j.cortex.2014.08.027 Hervais-Adelman, A., Kumar, U., Mishra, R. K., Tripathi, V. N., Guleria, A., Singh, J. P., Eisner, F., &amp; Huettig, F. (2019). Learning to read recycles visual cortical networks without destruction. Science Advances, 5(9), eaax0262. https://doi.org/10.1126/sciadv.aax0262 Heyselaar, E., Peeters, D., &amp; Hagoort, P. (2021). Do we predict upcoming speech content in naturalistic environments? Language, Cognition and Neuroscience, 36(4), 440–461. https://doi.org/10.1080/23273798.2020.1859568 Huettig, F., &amp; Guerra, E. (2019). Effects of speech rate, preview time of visual context, and participant instructions reveal strong limits on prediction in language processing. Brain Research, 1706(June 2017), 196–208. https://doi.org/10.1016/j.brainres.2018.11.013 Huettig, F., &amp; Mani, N. (2016). Is prediction necessary to understand language? Probably not. Language, Cognition and Neuroscience, 31(1), 19–31. https://doi.org/10.1080/23273798.2015.1072223 Hunter, C. R., &amp; Pisoni, D. B. (2018). Extrinsic cognitive load impairs spoken word recognition in high-and low-predictability sentences. Ear and Hearing, 39(2), 378–389. https://doi.org/10.1097/AUD.0000000000000493 Husband, E. M., &amp; Bovolenta, G. (2020). Prediction failure blocks the use of local semantic context. Language, Cognition and Neuroscience, 35(3), 273–291. https://doi.org/10.1080/23273798.2019.1651881 Iwasaki, S., Ocho, S., Nagura, M., &amp; Hoshino, T. (2002). Contribution of speech rate to speech perception in multichannel cochlear implant users. Annals of Otology, Rhinology and Laryngology, 111(8), 718–721. https://doi.org/10.1177/000348940211100811 Jackendoff, R. (2002). Précis of foundations of language: Brain, meaning, grammar, evolution. Behavioral and Brain Sciences, 26(6), 651–665. Jones, M., &amp; Love, B. C. (2011). Bayesian fundamentalism or enlightenment? on the explanatory status and theoretical contributions of bayesian models of cognition. Behavioral and Brain Sciences, 34(4), 169–188. https://doi.org/10.1017/S0140525X10003134 Kamide, Y., Altmann, G. T. M., &amp; Haywood, S. L. (2003). The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye movements. Journal of Memory and Language, 49(1), 133–156. https://doi.org/10.1016/s0749-596x(03)00023-8 Kaufeld, G. (2021). Investigating spoken language comprehension as perceptual inference (p. 183) [Doctoral dissertation, Max Planck Research School (IMPRS) for Language Sciences]. https://repository.ubn.ru.nl/handle/2066/228260 Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., &amp; De Lange, F. P. (2012). Attention reverses the effect of prediction in silencing sensory signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Kutas, M., &amp; Federmeier, K. D. (2011). Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP). Annual Review of Psychology, 62(1), 621–647. https://doi.org/10.1146/annurev.psych.093008.131123 Kutas, M., &amp; Hillyard, S. A. (1984). Brain potentials during reading reflect word expectancy and semantic association. Nature, 307(5947), 161–163. https://doi.org/10.1038/307161a0 Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3), 1126–1177. https://doi.org/10.1016/j.cognition.2007.05.006 Loizou, P. C., Dorman, M., &amp; Tu, Z. (1999). On the number of channels needed to understand speech. The Journal of the Acoustical Society of America, 106(4), 2097–2103. https://doi.org/10.1121/1.427954 Lowder, M. W., &amp; Ferreira, F. (2016). Prediction in the processing of repair disfluencies. Language, Cognition and Neuroscience, 31(1), 73–79. https://doi.org/10.1080/23273798.2015.1036089 Luce, P. A., &amp; Pisoni, D. B. (1998). Recognizing spoken words: The neighborhood activation model. Ear and Hearing, 19(1), 1. https://doi.org/10.1097/00003446-199802000-00001 Lupyan, G., &amp; Clark, A. (2015). Words and the World. Current Directions in Psychological Science, 24(4), 279–284. https://doi.org/10.1177/0963721415570732 Marques, T., Nguyen, J., Fioreze, G., &amp; Petreanu, L. (2018). The functional organization of cortical feedback inputs to primary visual cortex. Nature Neuroscience, 21(5), 757–764. https://doi.org/10.1038/s41593-018-0135-z Martin, A. E. (2016). Language processing as cue integration: Grounding the psychology of language in perception and neurophysiology. Frontiers in Psychology, 7, 120. https://doi.org/10.3389/fpsyg.2016.00120 Mattys, S. L., Davis, M. H., Bradlow, A. R., &amp; Scott, S. K. (2012). Speech recognition in adverse conditions: A review. Language and Cognitive Processes, 27(7-8), 953–978. https://doi.org/10.1080/01690965.2012.705006 McCullough, C. M. (1958). Context aids in reading. The Reading Teacher, 11(4), 225–229. https://www.jstor.org/stable/20197091 McGurk, H., &amp; MacDonald, J. (1976). Hearing lips and seeing voices. Nature, 264(5588), 746–748. https://doi.org/10.1038/264746a0 Metusalem, R., Kutas, M., Urbach, T. P., Hare, M., McRae, K., &amp; Elman, J. L. (2012). Generalized event knowledge activation during online sentence comprehension. Journal of Memory and Language, 66(4), 545–567. https://doi.org/10.1016/j.jml.2012.01.001 Miller, G. A., Heise, G. A., &amp; Lichten, W. (1951). The intelligibility of speech as a function of the context of the test materials. Journal of Experimental Psychology, 41(5), 329–335. https://doi.org/10.1037/h0062491 Mishra, R. K., Singh, N., Pandey, A., &amp; Huettig, F. (2012). Spoken language-mediated anticipatory eye- movements are modulated by reading ability - Evidence from Indian low and high literates. Journal of Eye Movement Research, 5(1), 1–10. https://doi.org/10.16910/jemr.5.1.3 Moon, I. J., &amp; Hong, S. H. (2014). What is temporal fine structure and why is it important? Korean Journal of Audiology, 18(1), 1–7. https://doi.org/10.7874/kja.2014.18.1.1 Moon, I. J., Won, J. H., Park, M. H., Ives, D. T., Nie, K., Heinz, M. G., Lorenzi, C., &amp; Rubinstein, J. T. (2014). Optimal combination of neural temporal envelope and fine structure cues to explain speech identification in background noise. Journal of Neuroscience, 34(36), 12145–12154. https://doi.org/10.1523/JNEUROSCI.1025-14.2014 Morton, J. (1964). The effects of context on the visual duration threshold for words. British Journal of Psychology, 55(2), 165–180. https://doi.org/10.1111/j.2044-8295.1964.tb02716.x Nahum, M., Nelken, I., &amp; Ahissar, M. (2008). Low-level information and high-level perception: The case of speech in noise. PLoS Biology, 6(5), 0978–0991. https://doi.org/10.1371/journal.pbio.0060126 Nieuwland, M. S. (2019). Do ‘early’ brain responses reveal word form prediction during language comprehension? A critical review. Neuroscience and Biobehavioral Reviews, 96, 367–400. https://doi.org/10.1016/j.neubiorev.2018.11.019 Obleser, J. (2014). Putting the Listening Brain in Context. Language and Linguistics Compass, 8(12), 646–658. https://doi.org/10.1111/lnc3.12098 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., &amp; Kotz, S. A. (2011). Multiple brain signatures of integration in the comprehension of degraded speech. NeuroImage, 55(2), 713–723. https://doi.org/10.1016/j.neuroimage.2010.12.020 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Orena, A. J., &amp; Colby, S. (2021). Recognizing voices through a cochlear implant: A systematic review. PsyArXiv. https://doi.org/10.31234/osf.io/e865q Patro, C., &amp; Mendel, L. L. (2020). Semantic influences on the perception of degraded speech by individuals with cochlear implants. The Journal of the Acoustical Society of America, 147(3), 1778–1789. https://doi.org/10.1121/10.0000934 Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Pierce, A. G. J., &amp; Ollason, J. G. (1987). Eight reasons why optimal foraging theory is a complete waste of time. Oikos, 49(1), 111–117. https://doi.org/10.2307/3565560 Roberts, B., Summers, R. J., &amp; Bailey, P. J. (2011). The intelligibility of noise-vocoded speech: Spectral information available from acrosschannel comparison of amplitude envelopes. Proceedings of the Royal Society B: Biological Sciences, 278(1711), 1595–1600. https://doi.org/10.1098/rspb.2010.1554 Rosen, S., Faulkner, A., &amp; Wilkinson, L. (1999). Adaptation by normal listeners to upward spectral shifts of speech: Implications for cochlear implants. The Journal of the Acoustical Society of America, 106(6), 3629–3636. https://doi.org/10.1121/1.428215 Ryskin, R., Futrell, R., Kiran, S., &amp; Gibson, E. (2018). Comprehenders model the nature of noise in the environment. Cognition, 181(July 2017), 141–150. https://doi.org/10.1016/j.cognition.2018.08.018 Samuel, A. G., &amp; Kraljic, T. (2009). Perceptual learning for speech. Attention, Perception, &amp; Psychophysics, 71(6), 1207–1218. https://doi.org/10.3758/app.71.6.1207 Schneider, B. A., &amp; Pichora-Fuller, M. K. (2001). Age-related changes in temporal processing: Implications for listening comprehension. Seminars in Hearing, 22(3), 227–239. https://doi.org/10.1055/s-2001-15628 Scholman, M. C., Demberg, V., &amp; Sanders, T. J. (2020). Individual differences in expecting coherence relations: Exploring the variability in sensitivity to contextual signals in discourse. Discourse Processes, 57(10), 844–861. https://doi.org/10.1080/0163853X.2020.1813492 Seth, A. K. (2013). Interoceptive inference, emotion, and the embodied self. Trends in Cognitive Sciences, 17(11), 565–573. https://doi.org/10.1016/j.tics.2013.09.007 Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(4), 623–656. https://doi.org/10.1002/j.1538-7305.1948.tb00917.x Shannon, R. V., Fu, Q.-J., &amp; Galvin Iii, J. (2004). The number of spectral channels required for speech recognition depends on the difficulty of the listening situation. Acta Oto-Laryngologica, 124(0), 50–54. https://doi.org/10.1080/03655230410017562 Shannon, R. V., Zeng, F.-G., Kamath, V., Wygonski, J., &amp; Ekelid, M. (1995). Speech Recognition with Primarily Temporal Cues. Science, 270(5234), 303–304. https://doi.org/10.1126/science.270.5234.303 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008b). Effect of age, presentation method, and learning on identification of noise-vocoded words. The Journal of the Acoustical Society of America, 123(1), 476–488. https://doi.org/10.1121/1.2805676 Slattery, T. J., Sturt, P., Christianson, K., Yoshida, M., &amp; Ferreira, F. (2013). Lingering misinterpretations of garden path sentences arise from competing syntactic representations. Journal of Memory and Language, 69(2), 104–120. https://doi.org/10.1016/j.jml.2013.04.001 Smith, N. J., &amp; Levy, R. (2008). Optimal processing times in reading: A formal model and empirical investigation. Proceedings of the 30th Annual Conference of the Cognitive Science Society, 30. Sohoglu, E., Peelle, J. E., Carlyon, R. P., &amp; Davis, M. H. (2012). Predictive top-down integration of prior knowledge during speech perception. Journal of Neuroscience, 32(25), 8443–8453. https://doi.org/10.1523/JNEUROSCI.5069-11.2012 Sommers, M. S., Nygaard, L. C., &amp; Pisoni, D. B. (1994). Stimulus variability and spoken word recognition. I. Effects of variability in speaking rate and overall amplitude. The Journal of the Acoustical Society of America, 96(3), 1314–1324. https://doi.org/10.1121/1.411453 Stadler, W., Ott, D. V. M., Springer, A., Schubotz, R. I., Schütz-Bosbach, S., &amp; Prinz, W. (2012). Repetitive TMS suggests a role of the human dorsal premotor cortex in action prediction. Frontiers in Human Neuroscience, 6. https://doi.org/10.3389/fnhum.2012.00020 Staub, A. (2015). The effect of lexical predictability on eye movements in reading: Critical review and theoretical interpretation. Language and Linguistics Compass, 9(8), 311–327. https://doi.org/10.1111/lnc3.12151 Staub, A. (2011). The effect of lexical predictability on distributions of eye fixation durations. Psychonomic Bulletin and Review, 18(2), 371–376. https://doi.org/10.3758/s13423-010-0046-9 Staub, A., Grant, M., Astheimer, L., &amp; Cohen, A. (2015). The influence of cloze probability and item constraint on cloze task response time. Journal of Memory and Language, 82, 1–17. https://doi.org/10.1016/j.jml.2015.02.004 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 Taylor, W. L. (1953). “Cloze procedure”: A new tool for measuring readability. Journalism Quarterly, 30(4), 415–433. https://doi.org/10.1177/107769905303000401 Tuthill, J. C., &amp; Azim, E. (2018). Proprioception. Current Biology, 28(5), R194–R203. https://doi.org/10.1016/j.cub.2018.01.064 Vaden, K. I., Kuchinsky, S. E., Cute, S. L., Ahlstrom, J. B., Dubno, J. R., &amp; Eckert, M. A. (2013). The cingulo-opercular network provides word-recognition benefit. Journal of Neuroscience, 33(48), 18979–18986. https://doi.org/10.1523/JNEUROSCI.1417-13.2013 Van Petten, C., &amp; Luka, B. J. (2012). Prediction during language comprehension: Benefits, costs, and ERP components. International Journal of Psychophysiology, 83(2), 176–190. https://doi.org/10.1016/j.ijpsycho.2011.09.015 Warren, R. M. (1970). Perceptual restoration of missing speech sounds. Science, 167(3917), 392–393. https://doi.org/10.1126/science.167.3917.39 Winn, M. B. (2016). Rapid release from listening effort resulting from semantic context, and effects of spectral degradation and cochlear implants. Trends in Hearing, 20, 1–17. https://doi.org/10.1177/2331216516669723 Winn, M. B., &amp; Teece, K. H. (2021). Slower speaking rate reduces listening effort among listeners with cochlear implants. Ear and Hearing, 42(3), 584. https://doi.org/10.1097/aud.0000000000000958 Throughout this thesis, speech distortion by noise-vocoding is referred to as speech degradation, or spectral degradation of speech.↩︎ N400 is a negative-going ERP component that peaks around 400 ms post-stimulus and is considered a neural marker of context-based semantic unexpectedness (Kutas &amp; Federmeier, 2011).↩︎ Henceforth, we use the word expectation and prediction interchangeably. ↩︎ The Neighborhood Activation Model of Luce &amp; Pisoni (1998) proposes that an auditory input of a word activates its neighbourhood words, which can be similar acoustically. The neighbourhood density is supposed to depend on the word frequency as well.↩︎ Cloze probability of a word is the proportion of participants who provide that word as the next word of a sentence, in an offline norming task, given the preceding words of the sentence (Staub et al., 2015; Taylor, 1953). Its value ranges from 0 to 1.↩︎ Surprisal is a measure of the change in probability mass (or simply put, the change in expectation) as predictions are proven wrong with an encounter of new words in a sentence, discourse, etc. (Hale, 2001; Smith &amp; Levy, 2008).↩︎ "],["chapter-methods.html", "3 General methods 3.1 Experimental materials 3.2 Data collection on the web", " 3 General methods This chapter provides an overview of the experimental materials used in the experiments described in Chapters 5, 6, and 7. Sentences used as experimental material were common in all the experiments, and the signal processing method was also common. Here, we also present an overview of online data collection. 3.1 Experimental materials As a part of a study in the research project A4 of SFB1102, sentences of different levels of predictability were created. Digital recordings of the sentences were degraded by noise-vocoding and used in all experiments reported in this thesis. The speech was also distorted by its compression and expansion. Below we briefly describe how the sentences of different levels of predictability were obtained and what methodology was used to create distorted versions of the speech. 3.1.1 Stimulus sentences With an aim to create sentences of three levels of predictability (low, medium, and high), a triplet of 120 sentences — a total of 360 sentences — were created from 120 nouns. Out of 120 nouns, 6 were repeated. All sentences were in present tense consisting of pronoun, verb, determiner, and object. These sentences were in Subject-Verb-Object form (e.g., Er fängt den Ball. EN: He catches the ball.). Some of these sentences were taken from Obleser &amp; Kotz (2010). For each sentence, cloze probability ratings were collected from a group of young adults (n = 60; age range = 18–30 years). Mean cloze probabilities of low, medium and high probability sentences are shown in Table 3.1 and the distribution of cloze probability across low, medium, and high predictability sentences is shown in Figure 3.1. The cloze probabilities of the target words in each sentence are shown in Appendix A. Table 3.1: Cloze probabilities of low, medium and high predictability sentences Cloze probability Predictability Mean \\(\\pm\\) SD Range Low 0.022 \\(\\pm\\) 0.027 0.00 – 0.09 Medium 0.274 \\(\\pm\\) 0.134 0.1 – 0.55 High 0.752 \\(\\pm\\) 0.123 0.56 – 1.00 Figure 3.1: Distribution of cloze probability ratings of target words in low, medium and high predictability sentences 3.1.2 Speech processing All 360 sentences were spoken by a female native speaker of German at a normal rate. The recordings were digitized at 44.1kHz with 32-bit linear encoding. Spoken sentences used in Chapters 5, 6, and 7 were degraded by noise-vocoding. In addition to degradation by noise-vocoding, the sentences were distorted by compression and expansion of speech signal in Chapter 7. 3.1.2.1 Noise-vocoding Noise-vocoding is used to parametrically vary and control the speech quality in a graded manner. It distorts a speech signal by dividing it into specific frequency bands corresponding to the number of vocoder channels. The frequency bands are analogous to the electrodes of a cochlear implant (Loizou et al., 1999; R. V. Shannon et al., 1995; R. V. Shannon et al., 2004). The amplitude envelope, i.e., the fluctuations of amplitude within each frequency band, is extracted, and the spectral information within it is replaced by noise. This noise-filtering makes the vocoded speech difficult to understand, although its temporal characteristics and periodicity of perceptual cues are preserved (Rosen et al., 1999). The spectral degradation conditions of 1, 4, 6, and 8 channels were achieved for each of the 360 recorded sentences using a customized script originally written by Darwin (2005) in Praat software (Boersma, 2001). The speech signal was divided into 1, 4, 6, and 8 frequency bands between 70Hz and 9,000Hz. The boundary frequencies were approximately logarithmically spaced following cochlear-frequency position functions (Erb, 2014; Greenwood, 1990; Rosen et al., 1999). The amplitude envelope of each band was extracted and applied to band-pass filtered white noise in the same frequency ranges; the upper and lower bounds for band extraction are specified in Table 3.2. Modulated noise bands were then combined to produce a degraded speech. Scaling was performed to equate the root-mean-square value of the original undistorted speech and the final degraded speech. This resulted in four levels of degradation: 1-, 4-, 6-, and 8-channel noise-vocoded speech. Spectrograms of clear speech and noise-vocoded speech for the sentence Er löest die Aufgabe are shown in Figure 3.2. It shows that with a decrease in the number of noise-vocoding channels, the information in speech signal reduces and becomes noise-like. Figure 3.2: Spectrograms of clear speech, and degraded speech arranged with a decreasing number of noise-vocoding channels (8, 6, 4 and 1 band) for the sentence `Er löest die Aufgabe.’ Table 3.2: Boundary frequencies (in Hz) for 1, 4, 6 and 8 channels noise-vocoding conditions Number of channels Boundary frequencies 1 70 9000 4 70 423 1304 3504 9000 6 70 268 633 1304 2539 4813 9000 8 70 207 423 764 1304 2156 3504 5634 9000 The primary motivation to degrade speech signals by noise-vocoding is twofold: On the practical side, noise-vocoding simulates the frequency selectivity with a cochlear implant or sensory-neural hearing loss. This provides insight into speech perception and language comprehension in special populations (older adults with hearing loss, patients with cochlear implants). On the experimental side, noise-vocoding preserves the temporal periodicity cues of the speech; we can investigate the importance of specific suprasegmental cues in speech perception. Noise-vocoding reduces the fine structure cues that carry the pitch-related suprasegmental information and allows the study of temporal amplitude envelope cues, which carry the suprasegmental information involved in lexical processing; noise-vocoding preserves these cues. It also provides a control over speech intelligibility by varying the number of vocoder channels. 3.1.2.2 Speech compression and expansion Temporal compression and expansion are used as a method to simulate fast and slow speech, and to study the effect of acoustic degradation (which is the change in speech rate) and the effect of increase or decrease in information flow. As early as the mid-twentieth century, investigators reported that intelligibility does not drop significantly when speech is speeded up to 2 times the normal speech rate (e.g., Garvey, 1953). Speech rate was increased by chopping physical tapes. Digital algorithms like PSOLA (Charpentier &amp; Stella, 1986; Moulines &amp; Charpentier, 1990) developed in the 1980s and later (e.g., Verhelst &amp; Roelands, 1993) now allow us to speed up and slow down the speech rate in a controlled fashion. In Chapter 7, we used Praat software that utilises a uniform time-compression algorithm (PSOLA) to create slow and fast speech with the compression factor of 1.35 and 0.65, respectively. A schematic representation of waveforms of different speech rates — normal, slow and fast — is shown in Figure 3.3. Figure 3.3: Schematic representation of waveforms of fast, normal, and slow speech rates for the sentence `Er löest die Aufgabe’ with the duration of each speech rates in second. Note the circled portion of the waveform, which examplifies that PSOLA eliminates and duplicates the parts of the original waveform to create fast and slow speech respectively. PSOLA creates fast or slow speech in three steps: analysis, modification, and synthesis (Charpentier &amp; Stella, 1986; Taleb, 2020). In the analysis step, it first sets pitch marks in an audio file and then creates segments of it (i.e., it segments the signal into successive analysis windows centred around those pitch marks). Then in the modification step, depending on the time-compression/expansion factor, it deletes (or duplicates) those segments and sets a new set of pitch marks. Finally, in the synthesis step, it adds the new segments back to the audio file (i.e., it rearranges the analysis window) and creates fast or slow speech as required. The distortion of phonemic properties of speech signals are minimal when accelerating and slowing down within the range of factor 2 or below (Moulines &amp; Charpentier, 1990; cf. Longster, 2003). We create fast and slow versions of 120 high and 120 low predictability sentences. These 480 recordings are then passed through 4 channels noise-vocoding to use as experimental materials. As discussed earlier, the main aim of manipulating this bottom-up process is to investigate the effect of change in the rate of information flow (i.e., change in the speech rate) on the top-down processes of contextual facilitation in degraded speech comprehension. 3.2 Data collection on the web Traditionally, behavioural experiments with human participants are conducted in a laboratory setup. In recent years, there has been a surge of experiments that are conducted on the web (Reips, 2021). The first generation of online experiments to study human cognition began in the mid-1990s (for reviews, Musch &amp; Reips, 2000) with the advent of the internet (Berners-Lee et al., 1992). Welch &amp; Krantz (1996) was the first online experiment that was conducted in 1995 as a part of tutorials in auditory perception (Musch &amp; Reips, 2000). In their survey of researchers, Musch &amp; Reips (2000) discovered that until 2000, there were already at least two psycholinguistic experiments conducted online, one of which studied the effect of context in shallow vs deep encoding of words. Despite the difficulty in conducting online experiments and scepticism of journals towards publishing results of online experiments, Musch &amp; Reips (2000) expressed optimism: At the moment, the number of Web experiments is still small, but a rapid growth can be predicted on the basis of the present results. We would not be surprised if within the next few years, a fair proportion of psychological experiments will be conducted on the Web. (p. 85) By 2022, there has been significant growth in online experiments as technical and technological barriers are greatly reduced. There are many software and online platforms which psychologists and psycholinguists can use with minimal knowledge of computer programming to design, host and run their experiments and retrieve the data in a fairly structured format (A. L. Anwyl-Irvine et al., 2020; Peirce et al., 2019; Prolific, 2014; see also, A. Anwyl-Irvine et al., 2021; Peer et al., 2022). Online experiments have demonstrated advantages over laboratory experiments (Gadiraju et al., 2017; Johnson et al., 2021). For example, a large pool of participants is available online, which is usually not possible in laboratory experiments. Similarly, the participants in online experiments are more diverse than in laboratory experiments. Considering these advantages, psychologists and psycholinguists have conducted online experiments for almost three decades. Scientists who only conducted laboratory experiments and occasional online experiments were forced to conduct their experiments almost exclusively on the web due to the restrictions imposed by covid-19 lockdown (Gagné &amp; Franzen, 2021; Reips, 2021). Since Welch &amp; Krantz (1996)’s auditory perception experiment, a number of experiments have been conducted online in the auditory domain (Leensen &amp; Dreschler, 2013; Seow &amp; Hauser, 2022; van Os et al., 2021b; Woods et al., 2017) replicating laboratory findings (e.g., Cooke &amp; Garcia Lecumberri, 2021). The experiments reported in this thesis were also conducted online. Initially, our experiments were designed to be conducted both in the laboratory and on the web. As the laboratory was shut down due to covid-19 pandemic-related lockdown (M. Schmitt, personal communication, March 16, 2020), we moved the laboratory experiments to the online platform. We recruited participants online via Prolific Academic (Prolific, 2014). We used Prolific’s filters to recruit only native speakers of German residing in Germany who reported not having any hearing loss, speech-language disorder, or cognitive impairment. Participants were redirected to the experiments that were designed and hosted in Lingoturk (Pusse et al., 2016). Lingoturk is a local hosting platform that manages crowdsourcing experiments — it runs the experiments and stores the data. We report the details of each experiment in Chapters 5, 6, and 7. Bibliography Anwyl-Irvine, A. L., Massonnié, J., Flitton, A., Kirkham, N., &amp; Evershed, J. K. (2020). Gorilla in our midst: An online behavioral experiment builder. Behavior Research Methods, 52(1), 388–407. https://doi.org/10.3758/s13428-019-01237-x Anwyl-Irvine, A., Dalmaijer, E. S., Hodges, N., &amp; Evershed, J. K. (2021). Realistic precision and accuracy of online experiment platforms, web browsers, and devices. Behavior Research Methods, 53(4), 1407–1425. https://doi.org/10.3758/s13428-020-01501-5 Berners-Lee, T., Cailliau, R., Groff, J. F., &amp; Pollermann, B. (1992). World-wide web: The information universe. Internet Research, 2(1), 52–58. https://doi.org/10.1108/eb047254 Boersma, P. (2001). Praat, a system for doing phonetics by computer. Glot. Int., 5(9), 341–345. Charpentier, F. J., &amp; Stella, M. G. (1986). Diphone synthesis using an overlap-add technique for speech waveforms concatenation. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 2015–2018. https://doi.org/10.1109/icassp.1986.1168657 Cooke, M., &amp; Garcia Lecumberri, M. (2021). Estimating the performance gap between lab and remote speech perception experiment. Acoustical Society of America Journal, 149(4), A111–A111. https://doi.org/10.1121/10.0004674 Darwin, C. (2005). Praat scripts for producing shannon AM speech. http://www.lifesci.sussex.ac.uk/home/Chris_Darwin/Praatscripts/. Erb, J. (2014). The neural dynamics of perceptual adaptation to degraded speech (p. 211) [Doctoral dissertation]. Universität Leipzig. Gadiraju, U., Möller, S., Nöllenburg, M., Saupe, D., Egger-Lampl, S., Archambault, D., &amp; Fisher, B. (2017). Crowdsourcing versus the laboratory: Towards human-centered experiments using the crowd. In D. Archambault, H. Purchase, &amp; T. Hoßfeld (Eds.), Evaluation in the crowd. Crowdsourcing and human-centered experiments (pp. 6–26). Springer, Cham. https://doi.org/10.1007/978-3-319-66435-4_2 Gagné, N., &amp; Franzen, L. (2021). How to run behavioural experiments online: Best practice suggestions for cognitive psychology and neuroscience. PsyArXiv. https://doi.org/10.31234/osf.io/nt67j Garvey, W. D. (1953). The intelligibility of speeded speech. Journal of Experimental Psychology, 45(2), 102–108. https://doi.org/10.1037/h0054381 Greenwood, D. D. (1990). A cochlear frequency-position function for several species—29 years later. Journal of the Acoustical Society of America, 87(6), 2592–2605. https://doi.org/10.1121/1.399052 Johnson, B. P., Dayan, E., Censor, N., &amp; Cohen, L. G. (2021). Crowdsourcing in cognitive and systems neuroscience. The Neuroscientist, 10738584211017018. https://doi.org/10.1177/10738584211017018 Leensen, M. C. J., &amp; Dreschler, W. A. (2013). Speech-in-noise screening tests by internet, Part 3: Test sensitivity for uncontrolled parameters in domestic usage. International Journal of Audiology, 52(10), 658–669. https://doi.org/10.3109/14992027.2013.803610 Loizou, P. C., Dorman, M., &amp; Tu, Z. (1999). On the number of channels needed to understand speech. The Journal of the Acoustical Society of America, 106(4), 2097–2103. https://doi.org/10.1121/1.427954 Longster, J. A. (2003). Concatenative speech synthesis : A framework for reducing perceived distortion when using the TD-PSOLA algorithm [Doctoral dissertation]. Bournemouth University. Moulines, E., &amp; Charpentier, F. (1990). Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Communication, 9(1990), 453–467. https://doi.org/10.1016/0167-6393(90)90021-Z Musch, J., &amp; Reips, U.-D. (2000). A brief history of web experimenting. In Psychological experiments on the internet (pp. 61–87). https://doi.org/10.1016/b978-012099980-4/50004-6 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Peer, E., Rothschild, D., Gordon, A., Evernden, Z., &amp; Damer, E. (2022). Data quality of platforms and panels for online behavioral research. Behavior Research Methods, 54(4), 1643–1662. https://doi.org/10.3758/s13428-021-01694-3 Peirce, J., Gray, J. R., Simpson, S., MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., &amp; Lindeløv, J. K. (2019). PsychoPy2: Experiments in behavior made easy. Behavior Research Methods, 51(1), 195–203. https://doi.org/10.3758/s13428-018-01193-y Prolific. (2014). Prolific academic. https://www.prolific.co. Pusse, F., Sayeed, A., &amp; Demberg, V. (2016). LingoTurk: Managing crowdsourced tasks for psycholinguistics. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, 57–61. https://doi.org/10.18653/v1/n16-3012 Reips, U.-D. (2021). Web-based research in psychology. Zeitschrift für Psychologie. https://doi.org/10.1027/2151-2604/a000475 Rosen, S., Faulkner, A., &amp; Wilkinson, L. (1999). Adaptation by normal listeners to upward spectral shifts of speech: Implications for cochlear implants. The Journal of the Acoustical Society of America, 106(6), 3629–3636. https://doi.org/10.1121/1.428215 Seow, T. X. F., &amp; Hauser, T. U. (2022). Reliability of web-based affective auditory stimulus presentation. Behavior Research Methods, 54(1), 378–392. https://doi.org/10.3758/s13428-021-01643-0 Shannon, R. V., Fu, Q.-J., &amp; Galvin Iii, J. (2004). The number of spectral channels required for speech recognition depends on the difficulty of the listening situation. Acta Oto-Laryngologica, 124(0), 50–54. https://doi.org/10.1080/03655230410017562 Shannon, R. V., Zeng, F.-G., Kamath, V., Wygonski, J., &amp; Ekelid, M. (1995). Speech Recognition with Primarily Temporal Cues. Science, 270(5234), 303–304. https://doi.org/10.1126/science.270.5234.303 Taleb, N. (2020). Assessing the intelligibility and acoustic changes of time-processed speech [Masters thesis, Case Western Reserve University]. http://rave.ohiolink.edu/etdc/view?acc_num=case1586637814204979 van Os, M., Kray, J., &amp; Demberg, V. (2021b). Recognition of minipairs in (un)predictive sentence contexts in two types of noise. Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 43(43), 2943–2949. Verhelst, W., &amp; Roelands, M. (1993). Overlap-add technique based on waveform similarity (WSOLA) for high quality time-scale modification of speech. IEEE International Conference on Acoustics, Speech and Signal Processing, 2, 554–557. https://doi.org/10.1109/icassp.1993.319366 Welch, N., &amp; Krantz, J. H. (1996). The World-Wide Web as a medium for psychoacoustical demonstrations and experiments: Experience and results. Behavior Research Methods, Instruments, and Computers, 28(2), 192–196. https://doi.org/10.3758/bf03204764 Woods, K. J. P., Siegel, M. H., Traer, J., &amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, and Psychophysics, 79(7), 2064–2072. https://doi.org/10.3758/s13414-017-1361-2 "],["chapter-stats.html", "4 General statistical approach 4.1 Linear mixed effects models 4.2 Model selection and Running mixed effects models in R 4.3 Summary", " 4 General statistical approach 4.1 Linear mixed effects models As the name suggests, the linear mixed effects model (LME) is a linear regression model that consists of both fixed and random effects. It allows modelling the underlying structure of the data, which includes the standard fixed effects like the levels of speech degradation and the levels of target word predictability, as well as random effects like items and participants. These random effects are assumed to be random samples drawn from the general population. In this thesis, the dependent variable (an outcome or a response variable) is binary (correct vs incorrect response). So, we use binomial logistic mixed effects models with crossed random effects to model the data (Baayen et al., 2008). A linear mixed effects model can be written as: \\[\\begin{align} y = \\alpha + u_{\\alpha} + w_{\\alpha} + (\\beta_{1} + u_{\\beta_{1}} + w_{\\beta_{1}})\\cdot {x_1} + \\notag \\\\ (\\beta_{2} + u_{\\beta_{2}} + w_{\\beta_{2}})\\cdot {x_2} + ... + \\nonumber \\\\ (\\beta_{n} + u_{\\beta_{n}} + w_{\\beta_{n}})\\cdot {x_n} \\tag{4.1} \\end{align}\\] where, \\(y\\) is the dependent variable, like participant’s response (correct vs. incorrect) \\(\\alpha\\) is the Intercept. Fixed effects: \\(\\beta_{1}, \\beta_{2}, ..., \\beta_{n}\\) are the coefficients (or effects) of \\(x_1, x_2, ...,x_n\\). \\(\\boldsymbol{u} = \\langle u_{\\alpha}, u_{\\beta_1}, u_{\\beta_2}, ..., u_{\\beta_n} \\rangle\\) : Varying intercept and slopes for random effect term like, subject. \\(\\boldsymbol{w} = \\langle w_{\\alpha}, w_{\\beta_1}, w_{\\beta_2}, ..., w_{\\beta_n} \\rangle\\) : Varying intercept and slopes for random effect term like, item. In contrast to linear regression models, mixed effects models allow to simultaneously account for the effects of two random variables, like item and participants. The variance in the categorical dependent variable is also preserved, which would otherwise be eliminated by averaging in linear regression models. We discuss these issues and the motivation to use the mixed effects model in this thesis in more detail below in this chapter. 4.1.1 Linear regression and its limitations In linear regression, a dependent variable (or an outcome) is modelled as a function of one or more independent predictor variables (factors or explanatory variables). That is, an outcome \\(y\\) is modelled as a function of explanatory variables \\(x_1, x_2, x_3..., x_n\\), and an error term \\(\\varepsilon\\). \\[\\begin{align} y = \\alpha + \\beta_{1}\\cdot{x_1} + \\beta_{2}\\cdot{x_2} + ... + \\beta_{n}\\cdot{x_n} + \\varepsilon \\tag{4.2} \\end{align}\\] Analysis of Variance (ANOVA), also a form of linear regression (Chatterjee &amp; Hadi, 2012; Vasishth et al., 2022), compares the means and variances of two or more conditions. As expressed in Equation (4.2), regression models can only model fixed effects. Although ANOVA can account for one random effect at a time, it still averages out the variance in the second random effect. These problems of using ANOVA in language sciences have been pointed out as early as the 1960s (H. H. Clark, 1973; Coleman, 1964). We elaborate on them in the context of the data of our experiments as follows. 4.1.1.1 Modeling two random effects simultaneously, and Variability in the data As mentioned above, a simple linear regression model, including ANOVA, does not model the effect of two random effects simultaneously, which a mixed effects model does. In the traditional ANOVA approach, researchers often run two separate regression models (Lorch &amp; Myers, 1990) by averaging raw data across participants, and items. Averaging eliminates the variability in the data. Additionally, comparing the means of a categorical variable (correct vs incorrect responses) even when transformed into accuracy or proportion scale is hard to interpret sensibly compared to a continuous variable like reaction time (for discussion, see Bolker et al., 2009; Jaeger, 2008). The statistical remedy for these problems in analysing the data of our experiments is to apply mixed effects models. 4.1.1.2 Unbalanced data sets In our studies, the data sets are unbalanced. The experimental design is intended to result in a balanced data set. However, after the removal of outliers and the trials that do not meet the inclusion criteria (for details, see Section 6.2.3), the final data sets become unbalanced, which introduces a bias in a regression model (Jaeger, 2008). A mixed effects model is best suited for such unbalanced data (Baayen et al., 2008). 4.1.1.3 Common mean for each predictor An intrinsic property or feature of the linear regression model is that it assumes a common mean for each predictor. It has been shown that this is, in fact, not true in the actual data: the effect of a predictor can vary depending on random variables like participant or item. Mixed effects models take into account such inter-participant and inter-item variability present in the data. For example, in mixed effects models, the random effects term with only varying intercept, e.g., participant as an intercept, assumes that if there are 100 participants, then the mean accuracy of those 100 participants is only a subset of possible global accuracies drawn from a set of the population mean. When a slope, e.g., levels of predictability, is included in the random effects structure in addition to the varying intercept (e.g., participants), then the model assumes that the effect of predictability on response accuracy varies across participants. Such variance across participants (or across items) is present in the real data and can be modelled in a mixed effects model but not in a linear regression model. 4.1.1.4 Bounded output variable, and Homogenity of variance Linear models assume an output variable to be on a continuous scale and not be restricted in a narrow range. In our data, the output variable has a binomially distributed binary outcome (correct or incorrect response) bounded on \\([0,1]\\). For every trial, there is a probability \\(p\\) (that ranges from 0 to 1) that the response will be correct, i.e. 1 (and a probability \\(1-p\\) that the response will be incorrect, i.e. 0). The variance of sample proportion is a function of \\(p\\), which is shown below. \\[\\begin{align} \\sigma^2_p = \\frac{p(1-p)}{n} \\tag{4.3} \\end{align}\\] That is, the variance of the sample proportions is highest at \\(p=0.5\\); it decreases symmetrically as \\(p\\) approaches to 0 or 1. Thus, for two samples with the proportions \\(p_1\\) and \\(p_2\\), the variances are similar if \\(p_1\\) and \\(p_2\\) are equidistant from 0.5. Moreover, the further away \\(p_1\\) and \\(p_2\\) are from 0.5, the more dissimilar the variances will be, and the more it matters. Critically, we do not know a priori what the value of \\(p\\) is for different samples under consideration in our experiments. In a linear model (like ANOVA), binary outcomes [0,1] can be transformed into a proportion scale across participants or across items. Even though it is a continuous variable, the proportion scale (i.e., response accuracy) has a range (0,1). Additionally, such a transformation of discrete variables brings a host of problems that we have already discussed above (e.g., loss of variability by averaging raw data). Binomial logistic mixed effects models, on the other hand, transform8 the output variable into a logit scale, \\(\\log\\) with base \\(e\\), i.e. \\(\\ln\\), with a range \\((-\\infty, +\\infty)\\). Therefore, these mixed effects models do not violate the model assumptions regarding the range of the outcome variables. In addition, this transformation in the logistic model also capture the fact that the closer the sample proportions are to the 0.5 the less they matter (Jaeger, 2008). Thus, Equation (4.1) can also be written as: \\[\\begin{align} \\ln (\\frac{p}{1-p}) = \\alpha + u_{\\alpha} + w_{\\alpha} + (\\beta_{1} + u_{\\beta_{1}} + w_{\\beta_{1}})\\cdot {x_1} + \\nonumber\\\\ (\\beta_{2} + u_{\\beta_{2}} + w_{\\beta_{2}})\\cdot {x_2} + \\nonumber\\\\ ... + (\\beta_{n} + u_{\\beta_{n}} + w_{\\beta_{n}})\\cdot {x_n} \\tag{4.4} \\end{align}\\] This is equivalent to: \\[\\begin{align} p = {\\frac{exp(\\ln(\\frac{p}{1-p}))}{1 + exp (\\ln(\\frac{p}{1-p}))}} \\tag{4.5} \\end{align}\\] where, \\[\\begin{align} \\ln(\\frac{p}{1-p}) = {logit}(p) \\tag{4.6} \\end{align}\\] Log-odds of correct response obtained from Equation (4.4) can also be transformed into the probability of correct response. Equations (4.5) and (4.6) provide the relationship between probability, logits (or log-odds), and odds (\\(\\frac{p}{1-p}\\)). We have presented the advantages of mixed effects models over linear (regression) models. Hence, we used the binomial logistic mixed effects model as the statistical analysis tool in all experiments reported in this thesis. Below we discuss how the model that best fits our data was selected. 4.2 Model selection and Running mixed effects models in R The underlying structure of given data can be modelled by different approximate statistical models. We intend to select a model that best fits our data. ‘Best fit’ can be objectively measured by Akaike Information Criterion, Bayesian Information Criterion, and Likelihood Ratio Test, among others (Akaike, 1973; Schwarz, 1978). In this thesis, we first build a complex (or maximal) model (e.g., by including all predictors, like target word predictability, speech degradation level, speech rate, their interactions, and co-variates, like trial number in the fixed effects) that is justified by the experimental design (cf. Bondell et al., 2010). The model is fitted with a maximal random effects structure that includes random intercepts for each participant and item (Barr et al., 2013). By-participant and by-item slopes included in the model are discussed in the Analyses sections of Chapters 5, 6, and 7. Model selection was based on the backward-selection heuristics on the fixed effects (cf. Matuschek et al., 2017). To find the best fitting model for the data, non-significant higher-order interactions were excluded from the fixed-effects structure in a stepwise manner. Similarly, random effects not supported by the data that explained zero variance according to singular value decomposition were excluded to prevent overparameterisation (Bates, Kliegl, et al., 2015). This gave a more parsimonious model, which was then extended separately with: i) item-related correlation parameters, ii) participant-related correlation parameters, and iii) both item- and participant-related correlation parameters. Among the parsimonious model and extended models, the model with the smallest AIC was selected as the best fitting model for our data (Grueber et al., 2011; Richards et al., 2011). Data preprocessing and analyses were performed in R (R Core Team, 2018) using R-Studio (Version 3.6.1, Version 3.6.3, and Version 4.1.3 at different time points). Accuracy was analysed with Generalized Linear Mixed Models (GLMMs) with lmerTest (Kuznetsova et al., 2017) and lme4 (Bates, Mächler, et al., 2015) packages. Binary responses (correct responses coded as 1 and incorrect responses coded as 0) for all participants were fit with a binomial logistic mixed effects model. Contrast coding of each factor and the model description are presented in the Analyses section of the chapters that follow. 4.3 Summary In this chapter, we introduced the statistical tool used for data analysis in this thesis. We discussed the limitations of traditional linear regression-based models like ANOVA and outlined the motivations for using mixed effects models. To capture the variability of our data without averaging out across participants or items, and to account for the effect of two (or more) random effects — participant and item — simultaneously, we fit mixed effects models to our data. Details of each data set corresponding to each experiment are presented in Chapters 5, 6, and 7. Bibliography Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In B. N. Petrov &amp; F. Csaksi (Eds.), Proceedings of the 2nd international symposium on information theory (pp. 267–281). Akademiai Kaido. Baayen, R. H., Davidson, D. J., &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Bates, D., Kliegl, R., Vasishth, S., &amp; Baayen, H. (2015). Parsimonious Mixed Models. arXiv. https://arxiv.org/abs/1506.04967 Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). https://doi.org/10.18637/jss.v067.i01 Bolker, B. M., Brooks, M. E., Clark, C. J., Geange, S. W., Poulsen, J. R., Stevens, M. H. H., &amp; White, J. S. S. (2009). Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology and Evolution, 24(3), 127–135. https://doi.org/10.1016/j.tree.2008.10.008 Bondell, H. D., Krishna, A., &amp; Ghosh, S. K. (2010). Joint variable selection for fixed and random effects in linear mixed-effects models. Biometrics, 66(4), 1069–1077. https://doi.org/10.1111/j.1541-0420.2010.01391.x Chatterjee, S., &amp; Hadi, A. S. (2012). Multiple linear regression. In Regression analysis by example (pp. 57–91). John Wiley &amp; Sons. Clark, H. H. (1973). The language-as-a-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12(4), 335–359. https://doi.org/10.1016/S0022-5371(73)80014-3 Coleman, E. B. (1964). Generalizing to a language population. Psychological Reports, 14(1), 219–226. https://doi.org/10.2466/pr0.1964.14.1.219 Grueber, C. E., Nakagawa, S., Laws, R. J., &amp; Jamieson, I. G. (2011). Multimodel inference in ecology and evolution: challenges and solutions. Journal of Evolutionary Biology, 24(4), 699–711. https://doi.org/10.1111/j.1420-9101.2010.02210.x Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007 Kuznetsova, A., Brockhoff, P. B., &amp; Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13). https://doi.org/10.18637/jss.v082.i13 Lorch, R. F., &amp; Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16(1), 149. https://doi.org/10.1037/0278-7393.16.1.149 Malik, W. A., Marco-Llorca, C., Berendzen, K., &amp; Piepho, H. P. (2020). Choice of link and variance function for generalized linear mixed models: a case study with binomial response in proteomics. Communications in Statistics - Theory and Methods, 49(17), 4313–4332. https://doi.org/10.1080/03610926.2019.1599021 Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp; Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001 R Core Team. (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Richards, S. A., Whittingham, M. J., &amp; Stephens, P. A. (2011). Model selection and model averaging in behavioural ecology: the utility of the IT-AIC framework. Behavioral Ecology and Sociobiology, 65(1), 77–89. https://doi.org/10.1007/s00265-010-1035-8 Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 461–464. https://doi.org/10.1214/aos/1176344136 Vasishth, S., Schad, D., Bürki, A., &amp; Kliegl, R. (2022). Hypothetical repeated sampling and the t-test. In Linear mixed models in linguistics and psychology: A comprehensive introduction (DRAFT). Such transformation is brought about in a generalized linear mixed effects model with a canonical logit link function (see Malik et al., 2020 for discussion).↩︎ "],["chapter-attention-prediction.html", "5 Predictability effects of degraded speech are reduced as a function of attention 5.1 Introduction 5.2 Background 5.3 Experiment 1: Attention to target word 5.4 Experiment 2: Attention to context 5.5 Conclusion 5.6 Summary", " 5 Predictability effects of degraded speech are reduced as a function of attention In adverse listening conditions, when the bottom-up perceptual input is degraded, listeners tend to rely on context information and form top-down semantic predictions, which provides contextual facilitation in understanding the degraded speech. Importantly, it is also affected by the allocation of attention to the context in a top-down manner. The aim of this study was to examine the role of attention in understanding linguistic information in an adverse listening condition, i.e., when the speech was degraded. To assess the role of attention, we varied task instructions in two experiments in which participants were instructed to listen to short sentences and thereafter type in the last word of the sentence they heard or type in the whole sentence. We were interested in how these task instructions influence the interplay between top-down predictions and bottom-up perceptual processes during language comprehension. The sentences varied in the degree of predictability (low, medium, and high) as well as in the levels of speech degradation (1-, 4-, 6-, and 8-channel noise-vocoding). Results indicated better word recognition for highly predictable sentences for moderate, though not for high, levels of speech degradation, but only when attention was directed to the whole sentence. 5.1 Introduction When there is noise in the signal, listeners overcome the difficulty of understanding speech by using context information. The ‘context information’ can be information in a given situation about a topic of conversation, semantic and syntactic information of a sentence structure, world knowledge, visual information, or even the information about neighbouring phonemes (Altmann &amp; Kamide, 2007; Kaiser &amp; Trueswell, 2004; Knoeferle et al., 2005; Xiang &amp; Kuperberg, 2015; for reviews, see Ryskin &amp; Fang, 2021; Stilp, 2020). For example, in the phoneme restoration effect (Samuel, 1996; Warren, 1970), a phoneme of one or more words in a sentence is replaced with white noise or a coughing sound. Participants are unable to notice such ‘noisy’ words in a sentence, as they perceptually restore the missing sound in those words from the context information. To utilise the context information in a sentence, listeners must attend to it and build a meaning representation of what has been said. Processing and comprehending degraded speech is more effortful and requires more attentional resources than clear speech (Eckert et al., 2016; Hunter &amp; Pisoni, 2018; Peelle, 2018; Wild et al., 2012). In this chapter, we examine how attention modulates the predictability effects brought about by context information at different levels of spectral degradation of speech. We address the existing unclarity in the literature regarding the distribution of attentional resources in an adverse listening condition: On the one hand, listeners can attend throughout the whole stream of speech and may thereby profit from the context information to predict sentence endings. On the other hand, listeners can focus their attention on linguistic material at a particular time point in the speech stream and, as a result, miss critical parts of the sentence context. If the goal is to understand a specific word in an utterance, there is a trade-off between allocating attentional resources to the perception of that word and allocating resources also to understanding the linguistic context and generating predictions. The study reported in this chapter was conducted to investigate how the allocation of attentional resources induced by different task instructions influences language comprehension and, in particular, the use of context information in communication through a noisy channel, i.e., when the speech is degraded. To examine the role of attention on predictive processing under degraded speech, we ran two experiments in which we manipulated the task instructions. In Experiment 1, participants were instructed to repeat only the final word of the sentence they heard, while in Experiment 2, they were instructed to repeat the whole sentence, drawing attention to the entire sentence, including the context. In both experiments, we varied the degree of predictability of sentence endings as well as the degree of speech degradation. 5.2 Background As we discussed earlier in Chapters 1 and 2, it is generally agreed upon that human language processing is predictive in nature, and comprehenders generate expectations about upcoming linguistic materials based on the context available to them (for reviews, see Kuperberg &amp; Jaeger, 2016; Nieuwland, 2019; Pickering &amp; Gambi, 2018; Staub, 2015). When the bottom-up speech signal is less informative in an adverse listening condition, listeners tend to rely more on top-down lexical-semantic cues from the context to support speech perception and language comprehension (Amichetti et al., 2018; Ganong, 1980; McGurk &amp; MacDonald, 1976; Obleser &amp; Kotz, 2010; Sheldon et al., 2008b; Warren, 1970). However, it is not just the quality of speech signal that determines and influences the reliance and use of predictive processing; attention to the auditory input is essential too. Auditory attention allows a listener to focus on the speech signal of interest (for reviews, see Fritz et al., 2007; Lange, 2013). For instance, a listener can attend to and derive information from one stream of sound among many competing streams, as demonstrated in the well-known cocktail party effect (Cherry, 1953; Hafter et al., 2007). When a participant is instructed to attend to only one of the two or more competing speech streams in a diotic or dichotic presentation, response accuracy to the attended speech stream is higher than to the unattended speech (e.g., Tóth et al., 2020). Similarly, when a listener is presented with a stream of tones (e.g., musical notes varying in pitch, pure tones of different harmonics) but attends to any one of the tones appearing at a specified time point, this is reflected in a larger amplitude of N19 (e.g., Lange &amp; Röder, 2010; see also Sanders &amp; Astheimer, 2008). Hence, listeners can draw attention to and process one among multiple competing speech streams, as well as orient their attention in the temporal dimension within an unfolding sound stream. So far, most previous studies have investigated listeners’ attention within a single speech stream using acoustic cues like accentuation and prosodic emphasis. For example, Li et al. (2014) examined whether the comprehension of critical words in a sentence context was influenced by a linguistic attention probe such as “ba” presented together with an accented or a de-accented critical word. The N1 amplitude was larger for words with such an attention probe than for words without a probe. These findings support the view that attention can be flexibly directed either by instructions towards a specific signal or by linguistic probes (Li et al., 2017; see also Brunellière et al., 2019). Thus, listeners are able to select a part or segment of a stream of auditory stimuli to selectively allocate their attention to. The findings on the interplay of attention and prediction mentioned above come from studies that mostly used a stream of clean speech or multiple streams of clean speech in their experiments. They are not informative about the attention-prediction interplay in degraded speech comprehension. Specifically, we do not know what role attention to a segment of the speech stream plays in the contextual facilitation of degraded speech comprehension. The studies that report predictability effects in degraded speech comprehension do not systematically examine the role of attention (e.g., Amichetti et al., 2018; Obleser &amp; Kotz, 2010; Sheldon et al., 2008b). Their conclusion that semantic predictability facilitates comprehension of degraded speech is based on listeners’ attention to the entire sentence, which is not compared to any other experimental condition manipulating attention allocation. Therefore, in two experiments, we examined whether context-based semantic predictions are automatic during effortful listening to degraded speech when participants are instructed to report either only the final word of the sentence or the entire sentence. We varied the task instructions to the listeners from Experiment 1 to Experiment 2, which required them to differentially attend to the target word; the instructions did not restrict the participants’ attention anywhere in the speech stream. We hypothesised that when listeners pay attention only to the contextually predicted target word, they do not form top-down predictions, i.e., there should not be a facilitatory effect of target word predictability. In contrast, when listeners attend to the whole sentence, they do form expectations such that the facilitatory effect of target word predictability will be observed replicating the prior behavioural findings (e.g., Obleser &amp; Kotz, 2010). 5.3 Experiment 1: Attention to target word This experiment was designed such that processing the context was not strictly necessary for the task. Listeners were asked to report the noun of the sentence that they heard, which was in the final position of the sentence. This instruction did not require listeners to pay attention to the context which preceded the target word. 5.3.1 Methods 5.3.1.1 Participants We recruited 50 participants online via Prolific Academic (Prolific, 2014). One participant whose response accuracy was less than 50% across all experimental conditions was removed. Among the remaining 49 participants (\\(M\\) age \\(\\pm SD=23.31\\pm 3.53\\) years; age range = 18-30 years), 27 were male, and 22 were female. All participants were native speakers of German and did not have any speech-language disorder, hearing loss, or neurological disorder (all self-reported). All participants received 6.20 Euro as monetary compensation for their participation in the approximately 40 minutes long experiment. 5.3.1.2 Materials Materials used in the experiment were created by the method described in Chapter 3 (Section 3.1). That is, there were 120 unique sentences in each of these three categories: low predictability, medium predictability and high predictability. Mean cloze probabilities of the target words of low, medium and high predictability sentences were \\(0.022\\pm0.027\\) (\\(M\\pm SD\\); range = 0.00-0.09), \\(0.274\\pm0.134\\) (\\(M\\pm SD\\); range = 0.1-0.55), and \\(0.752\\pm0.123\\) (\\(M\\pm SD\\); range = 0.56-1.00) respectively. All 360 sentences were then noise-vocoded through 1, 4, 6, and 8 channels to create degraded speech. 5.3.1.3 Procedure Participants were asked to use headphones or earphones. A sample of vocoded speech not used in the practice trial or the main experiment was provided so that the participants could adjust the volume to their preferred level of comfort at the beginning of the experiment. The participants were instructed to listen to the sentences and type in the target word (noun) using the keyboard. The time for typing in the response was not limited. They were informed at the beginning of the experiment that some of the sentences would be ‘noisy’ and not easy to understand. Guessing was encouraged. Eight practice trials with different levels of speech degradation were given to familiarise the participants with the task before presenting all 120 experimental trials with an intertrial interval of 1,000 ms. Each participant listened to 40 high predictability, 40 medium predictability, and 40 low predictability sentences. Levels of speech degradation were also balanced across each predictability level so that for each of the three predictability conditions (high, medium, and low predictability), ten 1-channel, ten 4-channel, ten 6-channel, and ten 8-channel noise-vocoded sentences were presented, resulting in 12 experimental lists. The sentences in each list were pseudo-randomised so that no more than three sentences of the same degradation and predictability condition appeared consecutively. 5.3.2 Analyses Accuracy was analysed using Generalized Linear Mixed Models (GLMMs) following the procedure described in Chapter 4 (Section 4.2) with lmerTest (Kuznetsova et al., 2017) and lme4 (Bates, Mächler, et al., 2015) packages. For the 1-channel speech degradation condition, there were only five correct responses, one each from 5 participants. Therefore, the 1-channel speech degradation condition was excluded from the analyses. Binary responses (categorical: correct and incorrect) for all participants were fit with a binomial linear mixed-effects model. Correct responses were coded as 1, and incorrect responses were coded as 0. Number of channels (categorical: 4-channel, 6-channel, and 8-channel noise-vocoding), target word predictability (categorical: high predictability sentences, medium predictability sentences, low predictability sentences), and the interaction of number of channels and target word predictability were included in the fixed effects. We fitted a model with a maximal random effects structure that included random intercepts for each participant and item (Barr et al., 2013). Both by-participant and by-item random slopes were included for number of channels, target word predictability, and their interaction, which was supported by the experiment design. Based on the previous findings on perceptual adaptation (e.g., Cooke et al., 2022; Davis et al., 2005; Erb et al., 2013), we further added trial number (centred) in the fixed effect structure to control for whether the listeners adapted to the degraded speech. We applied treatment contrast for number of channels (8-channel as a baseline) and sliding difference contrast for target word predictability (low predictability vs medium predictability and low predictability vs high predictability). 5.3.3 Results and discussion Mean response accuracies (in percentage) for all experimental conditions aggregated across all participants and items are shown in Table 5.1 and Figure 5.1. It shows that accuracy increases with an increase in the number of noise-vocoding channels, i.e., with the decrease in speech degradation. However, accuracy does not increase with an increase in target word predictability. These observations aligned with the results of the statistical analyses (Table 5.2). Table 5.1: Response accuracy (mean and standard error of the mean) across all levels of speech degradation and target word predictability in Experiment 1 Number of channels Target word predictability Mean Standard error 4 High 62.65 2.24 Medium 63.43 2.03 Low 63.99 1.83 6 High 95.60 0.94 Medium 95.54 1.05 Low 95.16 1.10 8 High 98.16 0.84 Medium 96.75 1.04 Low 97.91 0.97 Figure 5.1: Mean response accuracy across all conditions in Experiment 1. Accuracy increased only with an increase in the number of noise-vocoding channels. There is no change in accuracy with an increase or decrease in target word predictability. Error bars represent the standard error of the means. There was a significant main effect of number of channels, indicating that response accuracy for the 8-channel noise-vocoded speech was higher than for both 4-channel (\\(\\beta\\) = -3.50, SE = .22, z(4,410) = -16.19, p &lt; .001) and 6-channel noise-vocoded speech (\\(\\beta\\) = -.70, SE = .21, z(4,410) = -3.29, p = .001), that is, when the number of channels increased to 8, listeners gave more correct responses (see Figure 5.1). There was, however, no significant main effect of target word predictability (\\(\\beta\\) = .30, SE = .36, z(4,410) = .84, p = .40, and \\(\\beta\\) = .50,SE = .43, z(4,410) = 1.16, p = .25), and no interaction between number of channels and target word predictability (all ps &gt; .05). There was also no significant main effect of trial number (\\(\\beta\\) = .001, SE = .002, z(4,410) = .48, p = .63) suggesting that the listeners’ performance did not improve over time. These results indicated a decrease in response accuracy with an increase in speech degradation from the 8-channel to the 6-channel noise-vocoding condition and from the 8-channel to the 4-channel noise-vocoding condition. However, response accuracy did not increase with an increase in target word predictability, and the interaction between number of channels and target word predictability was also absent, in contrast to previous findings (e.g., Obleser et al., 2007; Obleser &amp; Kotz, 2011; see also Hunter &amp; Pisoni, 2018). These results suggest that the task instruction, which asked the participants to report only the final word, indeed led to neglecting the context. Although participants were able to neglect the context, there was still uncertainty about the speech quality of each subsequent trial; hence, they could not adapt to the different levels of degraded speech. To confirm that the predictability effect (or contextual facilitation) is replicable and dependent on attentional focus, we conducted a second experiment in which we changed the task instruction to draw participants’ attention to decoding the whole sentence. Table 5.2: Estimated effects of the model accounting for the correct word recognition in Experiment 1 Fixed effects Estimate Std. error z value p value Intercept 4.17 .25 16.73 &lt;.001 Noise condition (4-channel) -3.50 .22 -16.19 &lt;.001 Noise condition (6-channel) -.70 .21 -3.29 &lt;.001 Target word predictability (Low-Medium) .30 .36 .84 .40 Target word predictability (High-Low) .50 .43 1.16 .25 Noise condition (4-channel) \\(\\times\\) Target word predictability (Low-Medium) -.22 .39 -.57 .57 Noise condition (6-channel) \\(\\times\\) Target word predictability (Low-Medium) -.34 .44 -.76 .44 Noise condition (4-channel) \\(\\times\\) Target word predictability (High-Low) -.54 .45 -1.18 .24 Noise condition (6-channel) \\(\\times\\) Target word predictability (High-Low). .04 .50 .09 .03 Trial number .001 .002 .48 .63 5.4 Experiment 2: Attention to context Following up on Experiment 1, we conducted Experiment 2 on a separate group of participants with a different task instruction. This experiment was intended to test the hypothesis that the facilitatory effect of top-down predictions is observed only when the listeners’ attention is unrestricted so that the context information is also included within the listener’s attentional focus. 5.4.1 Methods 5.4.1.1 Participants and Materials We recruited a new group of 48 participants (\\(M\\) age \\(\\pm SD = 24.44 \\pm 3.5\\) years; age range = 18-31 years; 32 males) online via Prolific Academic. The same stimuli were used as in Experiment 1. 5.4.1.2 Procedure Participants were presented with sentences at a comfortable volume level. They were asked to use headphones or earphones, and a prompt was presented before the experiment began to adjust the volume to their level of comfort. Eight practice trials were presented, followed by 120 experimental trials. In contrast to Experiment 1, the participants were instructed to report the entire sentence, instead of reporting only the sentence-final word, by typing in what they heard. We did not limit the response time. 5.4.2 Analyses We followed the same data analysis procedure as in Experiment 1. The 1-channel speech degradation condition was excluded from further analyses. For the analyses and results of the two experiments to be comparable, we did not consider whether listeners reported other words in a sentence correctly; only the final words of the sentences (target words) were considered as either correct or incorrect responses. As in Experiment 1, we report the results from the maximal model supported by the design. 5.4.3 Results and discussion Mean response accuracy for different conditions is shown in Table 5.3 and Figure 5.2. We found that accuracy increased when the number of noise-vocoding channels increased, as well as when the target word predictability increased. The results of the statistical analysis confirmed these observations (Table 5.4). Table 5.3: Response accuracy (mean and standard error of the mean) across all levels of speech degradation and target word predictability in Experiment 2 Number of channels Target word predictability Mean Standard error 4 High 62.71 2.14 Medium 59.58 1.88 Low 58.13 1.88 6 High 96.88 0.93 Medium 92.29 1.21 Low 91.46 1.12 8 High 98.54 0.86 Medium 95.21 1.19 Low 95.00 1.23 Figure 5.2: Mean response accuracy across all conditions in Experiment 2. Accuracy increased with an increase in number of noise-vocoding channels and target word predictability. Error bars represent the standard error of the means. We again found a main effect of number of channels, such that response accuracy at 8-channel was higher than for both 4-channel (\\(\\beta\\) = -3.51, SE = .24, z(4,320) = -14.64, p &lt; .001), and 6-channel noise-vocoding (\\(\\beta\\) = -.65, SE = .22, z(4,320) = -2.93, p = .003). Similar to Experiment 1, the main effect of trial number was not significant (\\(\\beta\\) = .002, SE = .002, z(4,320) = 1.11, p = .27) indicating that the response accuracy did not increase over the course of the experiment. In contrast to Experiment 1, there was also a main effect of target word predictability: Response accuracy in high predictability sentences was significantly higher than in low predictability sentences (\\(\\beta\\) = 1.42, SE = .47, z(4,320) = 3.02, p = .003). We also found a statistically significant interaction between speech degradation and target word predictability (\\(\\beta\\) = -1.14, SE = .50, z(4,320) = -2.30, p = .02). Subsequent subgroup analyses of each channel condition showed that the interaction was driven by the difference in response accuracy between high predictability sentences and low predictability sentences in the 8-channel (\\(\\beta\\) = 1.42, SE = .62, z(1,440) = 2.30, p = .02) and 6-channel noise-vocoding conditions (\\(\\beta\\) = 1.14, SE = .34, z(1,440) = 3.31, p &lt; .001); at 4 channel, the difference in response accuracy between high and low predictability sentences was not significant (\\(\\beta\\) = .28, SE = .18, z(1,440) = 1.59, p = .11). In contrast to Experiment 1, these results indicate an effect of target word predictability; that is, response accuracy was higher when the target word predictability was high as compared to low. Also, the interaction between target word predictability and speech degradation, which was not observed in Experiment 1, showed that semantic predictability facilitated the comprehension of degraded speech already at moderate levels (like 6- or 8-channel). In line with the findings from Experiment 1, response accuracy was better with a higher number of channels. Table 5.4: Estimated effects of the model accounting for the correct word recognition in Experiment 2 Fixed effects Estimate Std. error z value p value Intercept 4.09 .24 16.79 &lt;.001 Noise condition (4-channel) -3.51 .24 -14.64 &lt;.001 Noise condition (6-channel) -.65 .22 -2.93 .003 Target word predictability (Low-Medium) -.08 .34 -.23 .82 Target word predictability (High-Low) 1.42 .47 3.02 .003 Noise condition (4-channel) \\(\\times\\) Target word predictability (Low-Medium) .02 .38 .05 .96 Noise condition (6-channel) \\(\\times\\) Target word predictability (Low-Medium) -.13 .43 -.31 .76 Noise condition (4-channel) \\(\\times\\) Target word predictability (High-Low) -1.14 .50 -2.30 .02 Noise condition (6-channel) \\(\\times\\) Target word predictability (High-Low) -.23 .57 -.41 .68 Trial number .002 .002 1.11 .27 We combined the data from both experiments in a single analysis to test whether participants’ response accuracy changes across the experiments, that is, to test whether the difference between experimental manipulations is statistically significant. We ran a binomial linear mixed-effects model on response accuracy and followed the same procedure as in Experiments 1 and 2. A full random effects structure supported by the study design was modelled. The model revealed no significant main effect of experimental group (\\(\\beta\\) = .04, SE = .26, z(8,730) = .15, p = .88), indicating that the overall response accuracy did not change with the change in instructions from Experiment 1 to Experiment 2. However, the critical interaction between experimental group and target word predictability was statistically significant (\\(\\beta\\) = .46, SE = .20, z(8,730) = 2.34, p = .02). That is, the effect of predictability was larger in the group that was asked to type in the whole sentence (Experiment 2) than in the group that was asked to type only the sentence-final target word (Experiment 1). Together, these findings suggest that the change in task instruction, which draws attention either to the entire sentence or only to the final word, is critical to whether the context information is used under degraded speech. Nonetheless, degraded speech comprehension is not reduced by binding listeners’ attention allocation to one part of the speech stream. The model summary is shown in Table 5.5. Table 5.5: Estimated effects of the best-fitting model accounting for the correct word recognition in both experiments Fixed effects Estimate Std. error z value p value Intercept 4.19 .20 20.72 &lt;.001 Noise condition (4-channel) -3.56 .20 -18.19 &lt;.001 Noise condition (6-channel) -.59 .18 -3.28 .001 Target word predictability (Low-Medium) .13 .26 .50 .62 Target word predictability (High-Low) .98 .34 2.93 &lt;.003 Experimental group .04 .26 .15 .88 Noise condition (4-channel) \\(\\times\\) Target word predictability (Low-Medium) -.12 .29 -.40 .69 Noise condition (6-channel) \\(\\times\\) Target word predictability (Low-Medium) -.30 .34 -.87 .38 Noise condition (4-channel) \\(\\times\\) Target word predictability (High-Low) -.84 .35 -2.42 .02 Noise condition (6-channel) \\(\\times\\) Target word predictability (High-Low) -.11 .38 -.29 .77 Noise condition (4-channel) \\(\\times\\) Experimental group -.10 .25 -.41 .68 Noise condition (6-channel) \\(\\times\\) Experimental group -.10 .28 -.36 .72 Target word predictability (High-Low) \\(\\times\\) Experimental group -.47 .20 2.34 .02 Trial number .001 .001 .93 .35 5.5 Conclusion The main goals of the present study were to investigate whether online semantic predictions are formed in comprehension of degraded speech when task instructions encourage attention to the processing of the context information or only to the critical target word. The results of two experiments revealed that attentional processes clearly modulate the use of context information for predicting sentence endings when the speech signal is degraded. In contrast to the first experiment, the results of the second experiment show an interaction between target word predictability and degraded speech. This is in line with existing studies that found a facilitatory effect of predictability at different levels of speech degradation when the participants were instructed to pay attention to the entire sentence (e.g., Obleser et al., 2007). Obleser and colleagues reported that at the 8-channel noise-vocoded speech, key word recognition10 was higher in high predictability sentences than in low predictability sentences. Listeners were required to attend to the entire sentence in those studies as well. Therefore, the findings of Experiment 2 replicate this facilitatory effect of predictability that was observed in Obleser and colleagues’ behavioural experiments. The important new finding that our study adds to the present literature is that this effect of semantic predictability (i.e., contextual facilitation of degraded speech comprehension) may be weakened or lost when listeners are instructed to report only the final word of the sentence that they heard (Experiment 1). The lack of predictability effect (or contextual facilitation) can most likely be attributed to listeners not successfully decoding the meaning of the verb of the sentence, as the verb is the primary predictive cue in our stimuli (e.g., Sie jongliert die Bälle) for the target word (noun: Bälle). Findings from auditory attention literature also support this explanation: When listeners’ attention is focused on one feature of an auditory stimulus and the rest are not attended to, then they are not accessed (filter mechanism, Lange, 2013; change detection, Sanford et al., 2006; Sturt et al., 2004). Hence, this small change in task instructions from Experiment 1 to Experiment 2 sheds light on the role of top-down attention regulation in using context for language comprehension in adverse listening conditions. In a noisy channel created by degraded speech, language comprehension is generally effortful, so focusing attention on only a part of the speech signal seems beneficial in order to enhance stimulus decoding. However, the results of this study also show that this comes at the cost of neglecting the context information that could be beneficial for language comprehension. Our findings hence demonstrate that there is a trade-off between the use of context for generating top-down predictions vs focusing all attention on a target word. Specifically, the engagement in the use of context and generation of top-down predictions may change as a function of attention (see also Li et al., 2014). This claim is also corroborated by the significant change in predictability effects (or contextual facilitation) from Experiment 1 to Experiment 2 in the combined dataset. Findings from the irrelevant-speech paradigm also support our conclusion. It has been shown that the predictability of unattended speech has no effect on the main experimental task (e.g., memorisation of auditorily presented digits). Wöstmann &amp; Obleser (2016) did not find predictability effects when the participants ignored the degraded speech (see also Ellermeier et al., 2015). An alternative explanation of ‘participants neglecting the context’ could be that the participants did not listen to the context at all, or they heard but did not process the context. However, irrelevant-speech paradigm studies show that listeners cannot avoid listening to the speech presented to them; to-be-ignored speech has been shown to interfere with the main experimental task (e.g., Lecompte, 1995). It is plausible that the participants listened to the context but did not do deep processing (cf. Ferreira &amp; Lowder, 2016). This is not incompatible with our first explanation, as in either case, attention to the final word leaves the listeners with limited resources to process and form a representation of the context information. Considering the theoretical accounts of predictive language processing (Friston, Parr, et al., 2020; Kuperberg &amp; Jaeger, 2016; McClelland &amp; Elman, 1986; Norris et al., 2016; Pickering &amp; Gambi, 2018), one would expect that listeners automatically form top-down predictions about upcoming linguistic units/events based on prior context. Also, when speech is degraded, top-down predictions render a benefit in word recognition and language comprehension (e.g., Sheldon et al., 2008a, 2008b). Results of our study revealed new theoretical insights by showing that this is not always the case. Top-down predictions are dependent on attentional processes (see also Kok et al., 2012), directed by task instructions; thus, they are not always automatic, and predictability does not always facilitate comprehension of degraded speech. To this point, our findings shed light on the growing body of literature indicating limitations of predictive language processing accounts (Huettig &amp; Guerra, 2019; Huettig &amp; Mani, 2016; Mishra et al., 2012; Nieuwland et al., 2018). Results from both experiments show that the effect of trial number is not significant. In contrast to previous studies (e.g., Davis et al., 2005; Erb et al., 2013), we did not observe adaptation to noise-vocoded speech. In those studies, there was certainty about the speech quality of the next trial, as the participants were presented with only one level of spectral degradation (only 4-channel or only 6-channel noise-vocoding) and crucially with no specific regard to semantic predictability. On the contrary, in our study, listeners were always uncertain about the speech quality of the next trial as well as its semantic predictability. Because of this changing context, the perceptual system of the participants may not retune itself (Goldstone, 1998; Mattys et al., 2012). However, there was no experimental condition in the current study in which participants were certain about the next-trial speech degradation. It cannot be discarded entirely that certainty about speech degradation would retune the perceptual system, and listeners would adapt to the degraded speech. This is one of the limitations of the current study. One could object to the metric of calculating accuracy in Experiment 2, but it should be noted that for a valid comparison of the results between the two experiments, we can only consider the accuracy of the sentence-final target word in Experiment 2. Participants’ response of the words preceding the sentence-final target word in Experiment 1 was not available; in fact, it was the whole point of the instruction given to the participants: Direct their attention only to the sentence-final target word but not to its preceding words. Hence, we find a discrepancy between the result of prior studies (Obleser et al., 2007; Obleser &amp; Kotz, 2010) and our study (Experiment 2) regarding the degradation level at which contextual facilitation is observed. Nonetheless, the conclusion from these studies and our study is consistent: as long as listeners attend to the sentence context, semantic predictability facilitates language comprehension, but, such a facilitatory effect is not observed when the degradation is at an extreme level, like 1-channel noise-vocoding. In conclusion, this study provides a novel insight into the modulatory role of attention in the interaction between top-down predictive and bottom-up auditory processes. We show that task instructions affect the distribution of attention to the degraded speech signal. This, in turn, means that when insufficient attention is given to the context, top-down predictions cannot be generated, and the facilitatory effect of predictability is substantially reduced. The findings of this study indicate limitations to predictive processing accounts of language comprehension. 5.6 Summary This chapter reported studies which replicated the previous finding that semantic predictability facilitates language comprehension when speech degradation is not at an extreme level. That is, when the channel of transmission is noisy, listeners put less weight on the degraded auditory input and more weight on the priors derived from the context information that facilitates language comprehension. Importantly, we showed in this chapter that contextual facilitation (i.e., facilitatory effect of predictability) is observed only when the listeners attend to the entire sentence, including the context. In the next chapter, we further investigate this effect; specifically, we examine the granularity of the predictability effect at moderate levels of speech degradation. We also examine whether (un)certainty about next-trial speech degradation and predictability influences perceptual adaptation to degraded speech. Bibliography Altmann, G. T. M., &amp; Kamide, Y. (2007). The real-time mediation of visual attention by language and world knowledge: Linking anticipatory (and other) eye movements to linguistic processing. Journal of Memory and Language, 57(4), 502–518. https://doi.org/10.1016/j.jml.2006.12.004 Amichetti, N. M., Atagi, E., Kong, Y.-Y., &amp; Wingfield, A. (2018). Linguistic context versus semantic competition in word recognition by younger and older adults with cochlear implants. Ear &amp; Hearing, 39(1), 101–109. https://doi.org/10.1097/aud.0000000000000469 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). https://doi.org/10.18637/jss.v067.i01 Brunellière, A., Auran, C., &amp; Delrue, L. (2019). Does the prosodic emphasis of sentential context cause deeper lexical-semantic processing? Language, Cognition and Neuroscience, 34(1), 29–42. https://doi.org/10.1080/23273798.2018.1499945 Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. Journal of the Acoustical Society of America, 25(5), 975–979. https://doi.org/10.1121/1.1907229 Cooke, M., Scharenborg, O., &amp; Meyer, B. T. (2022). The time course of adaptation to distorted speech. The Journal of the Acoustical Society of America, 151(4), 2636–2646. https://doi.org/10.1121/10.0010235 Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 Eckert, M. A., Teubner-Rhodes, S., &amp; Vaden, K. I. (2016). Is Listening in Noise Worth It? The Neurobiology of Speech Recognition in Challenging Listening Conditions. Ear &amp; Hearing, 37(1), 101S–110S. https://doi.org/10.1097/aud.0000000000000300 Ellermeier, W., Kattner, F., Ueda, K., Doumoto, K., &amp; Nakajima, Y. (2015). Memory disruption by irrelevant noise-vocoded speech: Effects of native language and the number of frequency bands. The Journal of the Acoustical Society of America, 138(3), 1561–1569. https://doi.org/10.1121/1.4928954 Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Ferreira, F., &amp; Lowder, M. W. (2016). Prediction, information structure, and good-enough language processing (Vol. 65, pp. 217–247). Elsevier Ltd. https://doi.org/10.1016/bs.plm.2016.04.002 Friston, K. J., Parr, T., Yufik, Y., Sajid, N., Price, C. J., Holmes, E., &amp; Square, Q. (2020). Generative models, linguistic communication and active inference. Neuroscience &amp; Biobehavioral Reviews, 118, 42–64. https://doi.org/10.1016/j.neubiorev.2020.07.005 Fritz, J. B., Elhilali, M., David, S. V., &amp; Shamma, S. A. (2007). Auditory attention - focusing the searchlight on sound. Current Opinion in Neurobiology, 17(4), 437–455. https://doi.org/10.1016/j.conb.2007.07.011 Ganong, W. F. (1980). Phonetic categorization in auditory word perception. Journal of Experimental Psychology: Human Perception and Performance, 6(1), 110. https://doi.org/10.1037/0096-1523.6.1.110 Goldstone, R. L. (1998). Perceptual learning. Annual Review of Psychology, 49(1), 585–612. https://doi.org/10.1146/annurev.psych.49.1.585 Hafter, E. R., Sarampalis, A., &amp; Loui, P. (2007). Auditory attention and filters. In Auditory perception of sound sources (Vol. 29, pp. 115–142). https://doi.org/10.1007/978-0-387-71305-2_5 Huettig, F., &amp; Guerra, E. (2019). Effects of speech rate, preview time of visual context, and participant instructions reveal strong limits on prediction in language processing. Brain Research, 1706(June 2017), 196–208. https://doi.org/10.1016/j.brainres.2018.11.013 Huettig, F., &amp; Mani, N. (2016). Is prediction necessary to understand language? Probably not. Language, Cognition and Neuroscience, 31(1), 19–31. https://doi.org/10.1080/23273798.2015.1072223 Hunter, C. R., &amp; Pisoni, D. B. (2018). Extrinsic cognitive load impairs spoken word recognition in high-and low-predictability sentences. Ear and Hearing, 39(2), 378–389. https://doi.org/10.1097/AUD.0000000000000493 Kaiser, E., &amp; Trueswell, J. (2004). The role of discourse context in the processing of a flexible word-order language. Cognition, 94(2), 113–147. https://doi.org/10.1016/j.cognition.2004.01.002 Knoeferle, P., Crocker, M. W., Scheepers, C., &amp; Pickering, M. J. (2005). The influence of the immediate visual context on incremental thematic role-assignment: evidence from eye-movements in depicted events. Cognition, 95(1), 95–127. https://doi.org/10.1016/j.cognition.2004.03.002 Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., &amp; De Lange, F. P. (2012). Attention reverses the effect of prediction in silencing sensory signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Kuznetsova, A., Brockhoff, P. B., &amp; Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13). https://doi.org/10.18637/jss.v082.i13 Lange, K. (2013). The ups and downs of temporal orienting: A review of auditory temporal orienting studies and a model associating the heterogeneous findings on the auditory N1 with opposite effects of attention and prediction. Frontiers in Human Neuroscience, 7, 1–14. https://doi.org/10.3389/fnhum.2013.00263 Lange, K., &amp; Röder, B. (2010). Temporal orienting in audition, touch, and across modalities. In Attention and time (pp. 393–405). Oxford University Press, Oxford, United Kingdom. Lecompte, D. C. (1995). An irrelevant speech effect with repeated and continuous background speech. Psychonomic Bulletin &amp; Review, 2(3), 391–397. https://doi.org/10.3758/BF03210978 Li, X., Lu, Y., &amp; Zhao, H. (2014). How and when predictability interacts with accentuation in temporally selective attention during speech comprehension. Neuropsychologia, 64, 71–84. https://doi.org/10.1016/j.neuropsychologia.2014.09.020 Li, X., Zhang, Y., Li, L., Zhao, H., &amp; Du, X. (2017). Attention is shaped by semantic level of event-structure during speech comprehension: An electroencephalogram study. Cognitive Neurodynamics, 11(5), 467–481. https://doi.org/10.1007/s11571-017-9442-4 Mattys, S. L., Davis, M. H., Bradlow, A. R., &amp; Scott, S. K. (2012). Speech recognition in adverse conditions: A review. Language and Cognitive Processes, 27(7-8), 953–978. https://doi.org/10.1080/01690965.2012.705006 McClelland, J. L., &amp; Elman, J. L. (1986). The TRACE model of speech perception. Cognitive Psychology, 18(1), 1–86. https://doi.org/10.1016/0010-0285(86)90015-0 McGurk, H., &amp; MacDonald, J. (1976). Hearing lips and seeing voices. Nature, 264(5588), 746–748. https://doi.org/10.1038/264746a0 Mishra, R. K., Singh, N., Pandey, A., &amp; Huettig, F. (2012). Spoken language-mediated anticipatory eye- movements are modulated by reading ability - Evidence from Indian low and high literates. Journal of Eye Movement Research, 5(1), 1–10. https://doi.org/10.16910/jemr.5.1.3 Näätänen, R., &amp; Picton, T. (1987). The N1 wave of the human electric and magnetic response to sound: A review and an analysis of the component structure. Psychophysiology, 24(4), 375–425. https://doi.org/10.1111/j.1469-8986.1987.tb00311.x Nieuwland, M. S. (2019). Do ‘early’ brain responses reveal word form prediction during language comprehension? A critical review. Neuroscience and Biobehavioral Reviews, 96, 367–400. https://doi.org/10.1016/j.neubiorev.2018.11.019 Nieuwland, M. S., Politzer-Ahles, S., Heyselaar, E., Segaert, K., Darley, E., Kazanina, N., Von Grebmer Zu Wolfsthurn, S., Bartolozzi, F., Kogan, V., Ito, A., Mézière, D., Barr, D. J., Rousselet, G. A., Ferguson, H. J., Busch-Moreno, S., Fu, X., Tuomainen, J., Kulakova, E., Husband, E. M., … Huettig, F. (2018). Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. eLife, 7, 1–24. https://doi.org/10.7554/elife.33468 Norris, D., McQueen, J. M., &amp; Cutler, A. (2016). Prediction, Bayesian inference and feedback in speech recognition. Language, Cognition and Neuroscience, 31(1), 4–18. https://doi.org/10.1080/23273798.2015.1081703 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., &amp; Kotz, S. A. (2011). Multiple brain signatures of integration in the comprehension of degraded speech. NeuroImage, 55(2), 713–723. https://doi.org/10.1016/j.neuroimage.2010.12.020 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Peelle, J. E. (2018). Listening effort: How the cognitive consequences of acoustic challenge are reflected in brain and behavior. Ear and Hearing, 39(2), 204–214. https://doi.org/10.1097/AUD.0000000000000494 Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Prolific. (2014). Prolific academic. https://www.prolific.co. Ryskin, R., &amp; Fang, X. (2021). The many timescales of context in language processing. In Psychology of learning and motivation (Vol. 75, pp. 201–243). Elsevier. https://doi.org/10.1016/bs.plm.2021.08.001 Samuel, A. G. (1996). Does lexical information influence the perceptual restoration of phonemes? Journal of Experimental Psychology: General, 125(1), 28. Sanders, L. D., &amp; Astheimer, L. B. (2008). Temporally selective attention modulates early perceptual processing: Event-related potential evidence. Perception and Psychophysics, 70(4), 732–742. https://doi.org/10.3758/PP.70.4.732 Sanford, A. J., Sanford, A. J., Molle, J., &amp; Emmott, C. (2006). Shallow processing and attention capture in written and spoken discourse. Discourse Processes, 42(2), 109–130. https://doi.org/10.1207/s15326950dp4202_2 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008a). Priming and sentence context support listening to noise-vocoded speech by younger and older adults. The Journal of the Acoustical Society of America, 123(1), 489–499. https://doi.org/10.1121/1.2783762 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008b). Effect of age, presentation method, and learning on identification of noise-vocoded words. The Journal of the Acoustical Society of America, 123(1), 476–488. https://doi.org/10.1121/1.2805676 Staub, A. (2015). The effect of lexical predictability on eye movements in reading: Critical review and theoretical interpretation. Language and Linguistics Compass, 9(8), 311–327. https://doi.org/10.1111/lnc3.12151 Stilp, C. (2020). Acoustic context effects in speech perception. Wiley Interdisciplinary Reviews: Cognitive Science, 11(1), 1–18. https://doi.org/10.1002/wcs.1517 Sturt, P., Sanford, A. J., Stewart, A., &amp; Dawydiak, E. (2004). Linguistic focus and good-enough representations: An application of the change-detection paradigm. Psychonomic Bulletin &amp; Review, 11(5), 882–888. Thornton, A. R. D., Harmer, M., &amp; Lavoie, B. A. (2007). Selective attention increases the temporal precision of the auditory N100 event-related potential. Hearing Research, 230(1-2), 73–79. https://doi.org/10.1016/j.heares.2007.04.004 Tóth, B., Honbolygó, F., Szalárdy, O., Orosz, G., Farkas, D., &amp; Winkler, I. (2020). The effects of speech processing units on auditory stream segregation and selective attention in a multi-talker (cocktail party) situation. Cortex, 130, 387–400. https://doi.org/10.1016/j.cortex.2020.06.007 Warren, R. M. (1970). Perceptual restoration of missing speech sounds. Science, 167(3917), 392–393. https://doi.org/10.1126/science.167.3917.39 Wild, C. J., Yusuf, A., Wilson, D. E., Peelle, J. E., Davis, M. H., &amp; Johnsrude, I. S. (2012). Effortful listening: The processing of degraded speech depends critically on attention. Journal of Neuroscience, 32(40), 14010–14021. https://doi.org/10.1523/JNEUROSCI.1528-12.2012 Wöstmann, M., &amp; Obleser, J. (2016). Acoustic detail but not predictability of task-irrelevant speech disrupts working memory. Frontiers in Human Neuroscience, 10, 538. https://doi.org/10.3389/fnhum.2016.00538 Xiang, M., &amp; Kuperberg, G. R. (2015). Reversing expectations during discourse comprehension. Language, Cognition and Neuroscience, 30(6), 648–672. https://doi.org/10.1080/23273798.2014.995679 N1 or N100 is a negative-going EEG component that peaks around 100 ms post-stimulus. It is considered as a neural marker of auditory selective attention (Näätänen &amp; Picton, 1987; Thornton et al., 2007).↩︎ Notice the difference in measurement metrics. Obleser et al. (2007) calculated response accuracy as the number of correct keywords identified, while we calculated it as the correct identification of the sentence-final target word.↩︎ "],["chapter-graded-prediction.html", "6 Semantic predictability facilitates comprehension of degraded speech in a graded manner 6.1 Introduction 6.2 Background 6.3 Methods 6.4 Analyses 6.5 Results and discussion 6.6 Conclusion 6.7 Summary", " 6 Semantic predictability facilitates comprehension of degraded speech in a graded manner In the previous chapter, we concluded that in a noisy channel, predictability facilitates comprehension of degraded speech only when listeners attend to the context. We also pointed out a few limitations of the study. In Experiment 2 (“Attention to context”) of Chapter 5, there was an implicit assumption that all the noun-correct responses were borne out of correctly identifying the context-evoking words (i.e., verbs). For a comparable analysis and results between Experiment 1 (“Attention to target word”) and Experiment 2 (“Attention to context”), we only considered the accuracy of noun identification. In this chapter, we take into account listeners’ identification of context (i.e., the verb that precedes the noun) while calculating the response accuracy. Importantly, we replicate the predictability effects in degraded speech comprehension reported in the previous chapter, showing a difference between comprehension of high and low predictability sentences. In this chapter, we extend it further and examine if the predictability is graded or all-or-nothing. Previously, we only showed that listeners do not adapt to degraded speech when there is a trial-by-trial variability in speech degradation. Here we report two experiments investigating if listeners’ adaptation to degraded speech is affected by such variability by comparing it against another condition in which speech degradation is kept constant. The results showed that in contrast to the “narrowed expectations” view postulated for the predictive processing of degraded speech, listeners probabilistically preactivate upcoming words from a wide range of semantic space, not limiting only to the highly probable sentence endings. We also did not find any learning effect on repeated exposure to degraded speech. 6.1 Introduction In the literature on speech perception and sentence processing, studies have argued that prediction is either probabilistic and graded or all-or-nothing (e.g., Ferreira &amp; Clifton Jr, 1986; Kuperberg &amp; Jaeger, 2016; Luke &amp; Christianson, 2016). Few studies have investigated such theoretical questions within the domain of adverse listening conditions, specifically in degraded speech comprehension (e.g., Corps &amp; Rabagliati, 2020; Strauß et al., 2013; cf. van Os et al., 2021b). Strauß et al. (2013) posit that listeners cannot preactivate less predictable sentence endings in an adverse listening condition. They propose that the facilitatory effect of predictability is limited to only highly predictable sentence endings at a moderate level of spectral degradation of speech. Although many studies support the general idea of Strau and colleagues that predictability facilitates comprehension of degraded speech (e.g., Hunter &amp; Pisoni, 2018; Obleser &amp; Kotz, 2010; Sheldon et al., 2008a), there have been no studies so far after Strau and colleagues, to our knowledge, which examined the nature of predictability specifically in degraded speech comprehension. In this chapter, our main aim is to attempt a replication of the previous findings of these predictability effects and extend them further by testing if listeners form narrowed expectations while listening to moderately degraded speech. In line with Strauß et al. (2013)’s argument, it is possible that listeners form predictions that are restricted to only highly probable sentence endings. On the opposite, listeners might generate expectations about an upcoming word based on how likely the word is to appear in the given context and hence form a probabilistic prediction. We also test the presence of perceptual adaptation and its interplay with contextual facilitation. We set a metric of measurement of language comprehension that considers whether listeners correctly identified the context information. 6.2 Background 6.2.1 Predictability effects in degraded speech perception We discussed in Chapter 2 (Section 2.2.2) that some studies (Obleser et al., 2007; Obleser &amp; Kotz, 2010; Sheldon et al., 2008a; see also Hunter &amp; Pisoni, 2018) have already shown the facilitatory effect of predictability in comprehension of degraded speech. For example, Obleser and colleagues compared high and low predictability sentences and observed contextual facilitation, in terms of the difference in response accuracy between high and low predictability sentences, at 8- and 4-channel noise-vocoded speech in their (2007) and (2010) studies, respectively. However, these neuroimaging studies were not designed to test the nature of predictability effects. In a modified experimental design, Strauß et al. (2013) varied the target word predictability by manipulating its expectancy (i.e., how expected the target word is given the verb) and typicality (i.e., co-occurrence of the target word and the preceding verb). They reported that at a moderate level of spectral degradation, N400 responses at highly predictable (strong-context and high-typical) words were the smallest. In contrast, they found that the N400 effect (in terms of the amplitude of the N400 component) was largest at the strong-context, low typical words and the weak-context, low-typical words; the responses at the latter two were not statistically different from each other. The authors interpreted these findings as a facilitatory effect of sentence predictability which might be limited to only highly predictable sentence endings at a moderate level of spectral degradation. They proposed it as an expectancy searchlight model and suggested that listeners form narrowed expectations from a restricted semantic space when the sentence endings are highly predictable. Their model posits that when the sentence endings are less predictable, listeners cannot preactivate those less predictable sentence endings in an adverse listening condition, namely, a severe spectral degradation. It is similar to the earlier observations made by Rayner et al. (2006; see Brothers &amp; Kuperberg, 2021 for discussion), who found that reading times at low and medium predictability words were shorter than high predictability words, but it is contrary to the view that readers and listeners form a probabilistic prediction of an upcoming word in a sentence. For example, Nieuwland et al. (2018)11 showed in a large-scale replication study of DeLong et al. (2005) that the N400 amplitude at the sentence-final noun is directly proportional to its cloze probability across a range of high- and low-cloze words (see also Kochari &amp; Flecken, 2019; Nicenboim et al., 2020). Heilbron et al. (2022) also showed that a probabilistic prediction model outperforms a constrained guessing model in predicting listeners’ neural activities in MEG and EEG. They suggested that linguistic prediction is probabilistic, and it is not limited only to the highly predictable sentence endings, but operates broadly in a wide range of probable sentence endings. However, when put in perspective with our research question, these studies were conducted in conditions without noise or degraded speech. Furthermore, the ones that examined degraded speech comprehension used only two levels of semantic predictability (high and low). The granularity and the nature of prediction remain to be tested in degraded speech comprehension. 6.2.2 Adaptation to degraded speech We discussed in Chapter 2 that studies have shown evidence for perceptual adaptation to artificially distorted speech. When exposed to noise-vocoded speech, listeners’ word recognition accuracy is shown to increase over the course of the experiment. Davis et al. (2005) and Erb et al. (2013) presented participants with 4- and 6-channel noise-vocoded speech in a single block. They found that the proportion of correctly reported words increased over the course of the experiment. It is important to note that in these experiments, listeners were not uncertain about the speech quality of any upcoming trial, i.e., the global channel context was certain or predictable. Additionally, there was no systematic variation in the semantic features of the words presented to the participants. Listeners gradually map the representation of degraded lexicons around their “true” (or clear) schema/exemplars on repeated exposure (Goldstone, 1998; Nosofsky, 1986). With repeated exposure, the representation of degraded input comes closer to the exemplar, thereby improving the performance. This feature-mapping mechanism proposes that the listeners map the whole feature of the sensory input. However, the higher level features of a speech (e.g., semantic property, like predictability) can also influence the acoustic realisation of a degraded word, i.e., bottom-up processing of the degraded speech and perceptual adaptation (Gold &amp; Watanabe, 2010; Goldstone, 1998; cf. Nahum et al., 2008). Listeners can assign weight to different dimensions of speech stimuli (acoustic-phonetic and lexical-semantic). Performance improves over time when listeners give more attentional weight to the acoustic-phonetic dimension (cf. Haider &amp; Frensch, 1996). Thus, when multiple levels of degraded speech signals are presented in a (pseudo-)randomised order, then the listener is uncertain about the speech quality of any upcoming trial, i.e., the global channel context is certain or predictable. Such changes in the auditory features of the speech signal throughout the experiment are likely to render the perceptual adaptation totally absent (Mattys et al., 2012). Additionally, the trial-by-trial variability in the characteristics of speech signals can also impair word recognition (Sommers et al., 1994; see also Dahan &amp; Magnuson, 2006). Very few studies have tried to study the influence of (un)certainty about next-trial speech quality and semantic feature in perceptual adaptation. For example, in a study by Vaden et al. (2013), participants were presented with words in background noise at +3dB SNR and +10dB SNR in a pseudo-random order. They argued that an adaptive control system is involved to optimise task performance when there is uncertainty about the next trial. Similarly, Obleser and colleagues (Hartwigsen et al., 2015; Obleser et al., 2007; Obleser &amp; Kotz, 2010) also presented listeners with multiple noise-vocoded speech (ranging from 2 to 32 channels noise-vocoding) in a pseudo-random order. However, none of these studies reported a change in listeners’ performance over the course of the experiment. So, we cannot derive a conclusion from these studies about the effect of (un)certainty of the global channel context in perceptual adaptation and contextual facilitation. As previously mentioned in Chapter 2 (Section 2.3), there are two conflicting findings in the literature on perceptual adaptation: On the one hand, repeated exposure to degraded speech leads to perceptual adaptation, consequently improving word recognition throughout the experiment. On the other hand, uncertainty about the next-trial speech quality is detrimental to word recognition. 6.2.3 Measurement of language comprehension How we measure language comprehension has rarely been guided by any specific theoretical motive in the existing literature (cf. Amichetti et al., 2018). There is a discrepancy across studies in how language comprehension in degraded speech is quantified. Some studies that reported contextual facilitation in degraded speech comprehension used the proportion of correctly reported final words only (e.g., Hunter &amp; Pisoni, 2018; Obleser &amp; Kotz, 2010; Sheldon et al., 2008a). Obleser et al. (2007) quantified language comprehension as the proportion of correctly identified key words in SPIN sentences. Erb et al. (2013) and Hakonen et al. (2017) used report scores (Peelle, 2013) that measure the proportion of correctly recognised words per sentence as an index of language comprehension. Such inconsistencies make cross-study comparison difficult. None of the metrics outlined here take into account if listeners have correctly identified the context, which should be the most important factor to be considered in the first place: If the context is misidentified, then the listeners are highly likely to misidentify the succeeding words (Marrufo-Pérez et al., 2019). 6.2.4 The present study Stemming from the results of Chapter 5 and the motivations outlined at the beginning of this chapter, the goals of the present study were threefold: The first goal was to attempt to replicate the facilitatory effect of predictability examining the nature of predictability, i.e., to test if listeners form narrowed expectations. Obleser and colleagues (e.g., Obleser et al., 2007; Obleser &amp; Kotz, 2010) have shown predictability effects (or contextual facilitation) to appear only at a moderate level of speech degradation by using only two levels of predictability (low and high). The use of three levels of target word predictability (low, medium and high) will let us test the narrowed expectations view (Strauß et al., 2013) by also taking into account the accuracy of context. If the listeners form narrowed predictions only for the target words with high cloze probability, then the facilitatory effect of semantic prediction will be observed only at these highly predictable sentence endings. Listeners’ response to the target words with medium and low cloze probability would be quite similar since these two fall out of the range of narrow prediction. However, if the listeners’ predictions are not restricted to highly predictable target words, then they form predictions across a wide context proportional to the probability of occurrence of the target word. In addition to highly predictable sentence endings, listeners will also form predictions for less predictable sentence endings. Such predictions will depend on the probability of occurrence of the target words. In other words, listeners also form predictions for less expected sentence endings; the semantic space of prediction depends on the probability of occurrence of those sentence endings. In contrast to prior studies (e.g., Obleser &amp; Kotz, 2010), the inclusion of sentences with medium cloze target words thus allows us to differentiate whether listeners form all-or-nothing prediction restricted to high cloze target words or a probabilistic prediction for words across a wide range of cloze probability. There is a variation in the sentences we use, i.e., they are low, medium and high predictability sentences, and they are degraded at different levels of spectral degradation. So our second goal was to investigate the role of uncertainty about the next-trial speech features on perceptual adaptation by varying the global channel context on the comprehension of degraded speech. To study this, we presented sentences randomised across all levels of predictability but i) blocked by each noise-vocoding channel, i.e. spectral degradation (predictable channel context) and ii) pseudo-randomised across all noise-vocoding channels (unpredictable channel context). Based on the previous findings, we expected that in the unpredictable channel context (i.e., when sentences are presented in a random order of spectral degradation), participants’ word recognition performance would be worse than in the predictable channel context (i.e., when the sentences are blocked by noise-vocoding, Garrido et al., 2011; Sommers et al., 1994; Vaden et al., 2013). To further examine perceptual adaptation, we also considered the effect of trial number in the analyses. And third, we aimed to measure language comprehension with a metric that reflects the participants’ correct or incorrect identification of the context. If participants do not understand the context and we only measure the recognition of the final word, this might not truly reflect the effect of contextual facilitation. 6.3 Methods 6.3.1 Participants We recruited a group of participants via Prolific Academic (Prolific, 2014) and assigned them to the predictable channel context (n = 50; \\(M\\) age \\(\\pm SD=23.6\\pm\\) 3.2 years; age range = 18-30 years; 14 females). Another group of 48 participants (n = 48; \\(M\\) age \\(\\pm SD=24.44\\pm\\) 3.5 years; age range = 18-31 years; 16 females) from Experiment 2 of Chapter 5 were recruited and assigned to the unpredictable channel context. All participants were native speakers of German and reportedly did not have any speech-language disorder, hearing loss, or neurological disorder. They received monetary compensation of 6.20 Euro for participating in the approximately 40 minutes long experiment. 6.3.2 Materials We used the same stimuli described in Chapter 3 (Section 3.1). One hundred twenty sentences each for low predictability, medium predictability, and high predictability sentences that differed in the cloze probability of sentence-final target words were used. Their mean cloze probabilities in the low, medium and high predictability sentences were \\(0.022\\pm0.027\\) (\\(M\\pm SD\\); range = 0.00-0.09), \\(0.274\\pm0.134\\) (\\(M\\pm SD\\); range = 0.1-0.55), and \\(0.752\\pm0.123\\) (\\(M\\pm SD\\); range = 0.56-1.00), respectively. All 360 sentences were then noise-vocoded through 1, 4, 6, and 8 channels to create degraded speech. 6.3.3 Procedure Participants were asked to use earphones or headphones. A sample of vocoded speech not used in the practice trial or the main experiment was presented to adjust the volume to their preferred comfort level at the beginning of the experiment. The participants were instructed to listen and report the entire sentences by typing in everything they heard using the keyboard. The time for typing in the response was not limited. They were informed at the beginning of the experiment that some of the sentences would be ‘noisy’ and not easy to understand. Guessing was encouraged. Eight practice trials with different levels of speech degradation were provided to the participants to familiarise them with the task before presenting all 120 experimental trials with an intertrial interval of 1,000 ms. Each participant was presented in the unpredictable channel context with 120 unique sentences: 40 low predictability, 40 medium predictability, and 40 high predictability sentences. Degradation level was also balanced across each sentence type, i.e., in each of the low, medium, and high predictability sentences, 10 sentences passed through each noise-vocoding channels — 1, 4, 6, and 8 — were presented. This resulted in 12 experimental lists. The sentences in each list were pseudo-randomised. No more than three sentences of the same degradation and predictability condition appeared consecutively. This randomisation confirmed the uncertainty of next-trial speech quality (or degradation) in the global context of the experiment. The same set of stimuli and experimental lists were used in the predictable channel context. Each participant was presented with 120 unique sentences blocked by degradation level, i.e., by noise-vocoding channels. There were four blocks of stimuli, one for each degradation level. Thirty sentences were presented in each of the 4 blocks. In the first block, all sentences were 8-channel noise-vocoded, followed by the blocks of 6-, 4-, and 1-channel noise-vocoded speech consecutively (Sheldon et al., 2008a). Within each block, 10 low predictability, 10 medium predictability and 10 high predictability sentences were presented. All the sentences were pseudo-randomised so that not more than three sentences of the same predictability condition appeared consecutively in each block. This confirmed the certainty of next-trial speech quality (within each block) and uncertainty of next-trial sentence predictability across all four blocks. For the present study, the data for the unpredictable channel context was the same as the data from Experiment 2 of Chapter 5, while the data for the predictable channel context was newly collected. 6.4 Analyses In the sentences used in our experiment, verbs evoke predictability of the sentence-final noun. Therefore, the effect of predictability (evoked by the verb) on language comprehension can be rightfully measured if we consider only those trials in which participants identify the verbs correctly. Verb-correct trials were considered as the sentence in which participants identified the context independent of the succeeding words. Morphological inflections and typos were considered as correct. We first filtered out those trials in which verbs were not identified correctly, i.e., trials with incorrect verbs. The 1-channel noise-vocoding condition was dropped from the analyses as there were no correct responses in any of the remaining trials in this condition. Accuracy was analysed using Generalized Linear Mixed Models (GLMMs) with lmerTest (Kuznetsova et al., 2017) and lme4 (Bates, Mächler, et al., 2015) packages. Binary responses (categorical: correct and incorrect) for all participants in both groups (predictable and unpredictable channel contexts) were fit with a binomial linear mixed-effects model. Correct responses were coded as 1, and incorrect responses were coded as 0. Number of channels (categorical: 4-channel, 6-channel, and 8-channel noise-vocoding), target word predictability (categorical: high predictability, medium predictability, and low predictability), global channel context (categorical: predictable channel context and unpredictable channel context) and the interaction of number of channels and target word predictability were included in the fixed effects. Separately for each group (i.e., for predictable and unpredictable channel context), we first fitted a model with a maximal random effects structure that included random intercepts for each participant and item (Barr et al., 2013). Both by-participant and by-item random slopes were included for number of channels, target word predictability, and their interaction, which was supported by the experiment design. Following the procedure described in Chapter 4 (Section 4.2), we selected the optimal model that best fit our data. We applied treatment contrast for number of channels (8-channel as a baseline; factor levels: 8-channel, 4-channel, 6-channel) and sliding difference contrast for target word predictability (low vs medium predictability and low vs high predictability) and channel context (factor levels: unpredictable vs predictable). 6.5 Results and discussion Mean response accuracy for different conditions in both channel contexts is shown in Tables ??, ??, and Figure 6.1. We found that accuracy increased when the number of noise-vocoding channels increased and when the target word predictability increased. The results of the statistical analysis confirmed these observations (Table ??). Figure 6.1: Mean response accuracy across all conditions in Experiment 2. Accuracy increased only with an increase in the number of noise-vocoding channels in both channel contexts. Only in the unpredictable global channel context, at the 4-channel noise-vocoding condition, a graded effect of prediction is observed. Error bars represent the standard error of the means. We found a significant main effect of number of channels. The response accuracy at 8-channel was higher than for both 4-channel (\\(\\beta\\) = -2.87, SE = .22, z(6917) = -13.1, p &lt;.001), and 6-channel noise-vocoding (\\(\\beta\\) = -.66, SE = .19, z(6917) = -3.42, p &lt; .001). There was a significant main effect of target word predictability suggesting that the response accuracy in low predictability sentences was lower than in high predictability sentences (\\(\\beta\\) = 2.18, SE = .3, z(6917) = 7.2, p &lt; .001) and medium predictability sentences (\\(\\beta\\) = -.52, SE = .27, z(6917) = -1.97, p = .049). We also found a significant interaction between number of channels and target word predictability (\\(\\beta\\) = -.71, SE = .29, z(6917) = -2.44, p = .015). The interaction was driven by the effect of predictability at 4-channel condition: The accuracy in high predictability sentences was higher than in medium predictability sentences (\\(\\beta\\) = 1.14, SE = .37, z(1608) = 3.1, p &lt; .001) which in turn was higher than low predictability sentences (\\(\\beta\\) = 1.01, SE = .24, z(1608) = 4.2, p &lt; .001). There was no significant difference in response accuracy between low predictability and high predictability sentences at both 6-channel (\\(\\beta\\) = .33, SE = .32, z(2590) = 1.04, p = .3) and 8-channel (\\(\\beta\\) = -.014, SE = .32, z(2719) = -.04, p = .97) conditions. A subgroup analysis was also performed on each channel context. There was a significant main effect of global channel context which showed that the response accuracy was higher in predictable channel context than in unpredictable channel context (\\(\\beta\\) = -.27, SE = .14, z(6917) = -2.02, p = .04). To further address the question of perceptual adaptation, following the findings of Chapter 5, we also added trial number in the fixed effect. Note that there were 30 trials in each block in the predictable channel context (i.e., blocked design). For comparability, we divided unpredictable channel context (i.e., randomised design) into four blocks in the analysis. We did not find a significant main effect of trial number indicating that the response accuracy did not change throughout the experiment (\\(\\beta\\) = -.0004, SE = .01, z(6917) = -.05, p = .97). It remained constant within each block in the predictable channel context (\\(\\beta\\) = -.02, SE = .01, z(3291) = -1.43, p = .15) as well as in the unpredictable channel context (\\(\\beta\\) = .01 SE = .01, z(3291) = 1.05, p = .29). 6.6 Conclusion The present study had three goals: i) to examine if the previously reported facilitatory effect of semantic predictability is restricted to only highly predictable sentence endings, ii) to assess the role of perceptual adaptation on the facilitation of language comprehension by sentence predictability, and iii) to use and establish a sensitive metric to measure language comprehension that takes into account whether listeners benefited from the semantic context of the sentence. Results of our study showed the expected interaction between predictability and degraded speech. Language comprehension was better for high-cloze than for low-cloze target words when the speech signal was moderately degraded by noise-vocoding through 4 channels, while the effect of predictability was absent when speech was not intelligible by noise-vocoding through 1 channel. Listeners could not even identify the context at this severe degradation level. These results align with Obleser &amp; Kotz (2010); we partly included identical sentences from their study in the present study (see Appendix A). Importantly, in contrast to their study, we also created sentences with medium-cloze target words (which were intermediate between high-cloze and low-cloze target words). We found that the effect of predictability was also significant when comparing sentences with medium-cloze target words against the sentences with low-cloze and high-cloze target words in 4-channel noise-vocoding condition. Recognition of a target word was dependent on its level of predictability (measured by cloze probability), and correct recognition was not just limited to high-cloze target words. These significant differences in response accuracy between medium-cloze and low-cloze target words and between medium-cloze and high-cloze target words at 4-channel noise-vocoding condition show that the sentence-final word recognition is facilitated by semantic predictability in a graded manner, especially at a moderate level of speech degradation. This is in line with the findings from other experimental paradigms, including but not limited to the ERP literature, where it has been observed that the semantic predictability, in terms of cloze probability of the target word of a sentence, modulates semantic processing, indexed by N400, in a graded manner (DeLong et al., 2005; Nieuwland et al., 2018; Wlotko &amp; Federmeier, 2012). The interpretation of the observed graded effect of semantic predictability at the moderate level of spectral degradation provides a novel insight into how listeners form predictions when the bottom-up input is compromised. That is, in an adverse listening condition, listeners rely more on top-down semantic prediction than bottom-up acoustic-phonetic cues. Notably, such a reliance on top-down prediction is not an all-or-nothing phenomenon. Instead, listeners form a probabilistic prediction about the target word. The effect of target word predictability on comprehension is not sharply focused solely on high-cloze target words like a ‘searchlight’ as proposed by Strau and colleagues. Rather, it is spread across a wide range, including low- and medium-cloze target words. As the cloze probability of the target words decreases from high to low, the focus of the searchlight becomes less precise. One could argue that the participants in our experiment “guessed” the verb after first correctly identifying the noun in a sentence, i.e., instead of forward prediction (Bälle from jongliert), participants guessed backwards (jongliert from Bälle). To rule out this possible explanation of our findings, we conducted an additional analysis comparing the forward predictability effect (from verb to noun) to the size of the backward guessing effect (correct identification of the verb based on the final noun). If the observed effect is a guessing phenomenon, then we would expect the backward guessing effect to be larger than the effect of predictability. If, on the other hand, understanding the verb helps to shape the predictions of the upcoming noun, and this helps intelligibility, then the forward prediction effect should be larger than the effect of guessing. The results of this complementary analysis (see Appendix B) support the findings of the main analysis reported in the Results section. In the backward guessing analysis, there was no graded effect of predictability, and the backward effect of “guessing” the verb jongliert after recognising the noun Bälle, if present at all, was smaller than the forward effect of predicting the noun after recognising the verb in the sentence Sie jongliert die Bälle. This further corroborates our argument that the listeners, in fact, made use of the verb-evoked context to form predictions about the upcoming noun, not the other way around, and did so in a graded manner when the speech was moderately degraded. There was no learning effect or perceptual adaptation to degraded speech at the trial-by-trial level. We reason that the adaptation was hampered by a constant variation in the higher-level semantic feature (i.e., target word predictability). The results of the analyses of trial number on the effect of channel context to capture trial-by-trial perceptual adaptation showed that the response accuracy did not increase over the course of the experiment. This suggests that listeners’ performance remained constant throughout the experiment regardless of certainty about the next-trial spectral degradation. One way by which perceptual adaptation occurs is when the perceptual system of a listener retunes itself to the sensory properties of the auditory signal, which can be facilitated by feedback from higher-level lexical information (Goldstone, 1998; Mattys et al., 2012; cf. Davis et al., 2005). We reason that the trial-by-trial variability in the spectral resolution of the speech signal in the unpredictable channel context prevented perceptual adaptation. Although there was certainty about the quality of speech signal within a block in the predictable channel context, we did not observe trial-by-trial perceptual adaptation in this condition either. This is contrary to previous studies showing that listeners adapt to degraded speech when the global context of speech quality is predictable (Davis et al., 2005; Erb et al., 2013). However, the crucial difference between those studies and the study reported here is the manipulation of target word predictability. For example, Erb et al. (2013) presented sentences with only low predictability target words from the G-SPIN test. We, on the contrary, parametrically varied target word predictability from low to medium and high. Note that we presented target words in a randomised order in both channel contexts. This alone introduces trial-by-trial uncertainty in the predictable channel context and possibly hinders trial-by-trial perceptual adaptation. As Goldstone notes, “one way in which perception becomes adapted to tasks and environments is by increasing the attention paid to perceptual dimensions and features that are important, and/or by decreasing attention to irrelevant [perceptual] dimensions and features” (Goldstone, 1998, p. 588; see also Gold &amp; Watanabe, 2010). A similar prediction is made by the Reverse Hierarchy Theory on auditory perception (Ahissar et al., 2009; Nahum et al., 2008). It posits that listeners first have access to the higher-level features of a speech signal. If their task is to comprehend language, then they may not be able to access the lower-level perceptual features. Consequently, they cannot adapt to the speech in an adverse listening condition. In our study, listeners paid more attention to the semantic properties of the sentences (i.e., contextual cues and target word predictability) than to the perceptual properties (i.e., spectral resolution or speech quality) as the instruction focused on “language comprehension” rather than “perception”. We speculate this might have resulted in the absence of trial- by-trial perceptual adaptation to degraded speech, even when next-trial speech quality was predictable. Therefore, in both predictable and unpredictable channel contexts, perceptual learning of the degraded speech was hindered by trial-by-trial variation of either one (target word predictability) or both properties (target word predictability and spectral degradation level) of the speech stimuli. We also argue that for the examination of semantic predictability effects during language comprehension, the analyses of response accuracy should be based on the trials in which context evoking words are correctly identified in the first place to make sure that listeners make use of the contextual cues instead of analyzing general word recognition scores. Use of crude word recognition score, or key-word recognition score do not reflect whether a comprehender formed a representation of the available context information. To conclude, we show the interaction between top-down predictive and bottom-up auditory processes: Only when the speech is moderately degraded, predictability facilitates language comprehension, and the nature of such predictability effect is graded. We also show that listeners do not adapt to degraded speech on a trial-by-trial basis even when the quality of next-trial speech signal is certain, which is likely due to the interference from the higher-level semantic features (e.g., predictability). We propose and argue for the consideration of the context use/identification in the metric of language comprehension. 6.7 Summary This chapter primarily investigated the nature of the predictability effect. The experiment reported here provides a novel insight into predictive language processing when bottom-up signal quality is compromised and uncertain: We showed that while processing a moderately degraded speech, listeners form top-down predictions across a wide range of semantic space that is not restricted within highly predictable sentence endings. In contrast to the narrowed expectations view, comprehension of words ranging from low- to high-cloze probability, including the medium-cloze probability, is facilitated in a graded manner. This contextual facilitation is observed while listening to a moderately degraded speech. Regardless of (un)certainty about the next-trial speech quality, we found that listeners do not adapt to the degraded speech when semantic predictability constantly varies, i.e., higher-level semantic features interfere with the lower-level perceptual properties. All in all, these findings revealed that the bottom-up perceptual property of speech (i.e., speech quality) interacts with the top-down predictive processes. Together with the preceding chapter, this chapter showed that when listeners attend to the sentence context, predictability facilitates comprehension of moderately degraded speech in a graded manner. In the next chapter, we investigate the effect of further changes in the lower-level bottom-up processing on the top-down semantic predictions. Specifically, we examine how the change in speech rate affects semantic predictions at the moderate level of speech degradation. We ask the question: Does an increase or decrease in the speech rate further increase or decrease the contextual facilitation? Bibliography Ahissar, M., Nahum, M., Nelken, I., &amp; Hochstein, S. (2009). Reverse hierarchies and sensory learning. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1515), 285–299. https://doi.org/10.1098/rstb.2008.0253 Amichetti, N. M., Atagi, E., Kong, Y.-Y., &amp; Wingfield, A. (2018). Linguistic context versus semantic competition in word recognition by younger and older adults with cochlear implants. Ear &amp; Hearing, 39(1), 101–109. https://doi.org/10.1097/aud.0000000000000469 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). https://doi.org/10.18637/jss.v067.i01 Brothers, T., &amp; Kuperberg, G. R. (2021). Word predictability effects are linear, not logarithmic: Implications for probabilistic models of sentence comprehension. Journal of Memory and Language, 116, 104174. https://doi.org/10.1016/j.jml.2020.104174 Corps, R. E., &amp; Rabagliati, H. (2020). How top-down processing enhances comprehension of noise-vocoded speech: Predictions about meaning are more important than predictions about form. Journal of Memory and Language, 113, 104114. https://doi.org/10.1016/j.jml.2020.104114 Dahan, D., &amp; Magnuson, J. S. (2006). Spoken word recognition. In M. J. Traxler &amp; M. A. Gernsbacher (Eds.), Handbook of psycholinguistics (2nd ed., pp. 249–283). Elsevier. https://doi.org/10.1016/b978-012369374-7/50009-2 Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 DeLong, K. A., Urbach, T. P., &amp; Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8), 1117–1121. https://doi.org/10.1038/nn1504 Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Ferreira, F., &amp; Clifton Jr, C. (1986). The independence of syntactic processing. Journal of Memory and Language, 25(3), 348–368. https://doi.org/10.1016/0749-596X(86)90006-9 Garrido, M. I., Dolan, R. J., &amp; Sahani, M. (2011). Surprise Leads to Noisier Perceptual Decisions. I-Perception, 2(2), 112–120. https://doi.org/10.1068/i0411 Gold, J. I., &amp; Watanabe, T. (2010). Perceptual learning. Current Biology, 20(2), R46–R48. https://doi.org/10.1016/j.cub.2009.10.066 Goldstone, R. L. (1998). Perceptual learning. Annual Review of Psychology, 49(1), 585–612. https://doi.org/10.1146/annurev.psych.49.1.585 Haider, H., &amp; Frensch, P. A. (1996). The role of information reduction in skill acquisition. Cognitive Psychology, 30(3), 304–337. https://doi.org/10.1006/cogp.1996.0009 Hakonen, M., May, P. J. C., Jääskeläinen, I. P., Jokinen, E., Sams, M., &amp; Tiitinen, H. (2017). Predictive processing increases intelligibility of acoustically distorted speech: Behavioral and neural correlates. Brain and Behavior, 7(9), e00789. https://doi.org/10.1002/brb3.789 Hartwigsen, G., Golombek, T., &amp; Obleser, J. (2015). Repetitive transcranial magnetic stimulation over left angular gyrus modulates the predictability gain in degraded speech comprehension. Cortex, 68, 100–110. https://doi.org/10.1016/j.cortex.2014.08.027 Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P., &amp; De Lange, F. P. (2022). A hierarchy of linguistic predictions during natural language comprehension. Proceedings of the National Academy of Sciences, 119(32), e2201968119. https://doi.org/10.1073/pnas.2201968119 Hunter, C. R., &amp; Pisoni, D. B. (2018). Extrinsic cognitive load impairs spoken word recognition in high-and low-predictability sentences. Ear and Hearing, 39(2), 378–389. https://doi.org/10.1097/AUD.0000000000000493 Kochari, A. R., &amp; Flecken, M. (2019). Lexical prediction in language comprehension: a replication study of grammatical gender effects in Dutch. Language, Cognition and Neuroscience, 34(2), 239–253. https://doi.org/10.1080/23273798.2018.1524500 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Kuznetsova, A., Brockhoff, P. B., &amp; Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13). https://doi.org/10.18637/jss.v082.i13 Luke, S. G., &amp; Christianson, K. (2016). Limits on lexical prediction during reading. Cognitive Psychology, 88, 22–60. https://doi.org/10.1016/j.cogpsych.2016.06.002 Marrufo-Pérez, M. I., Eustaquio-Martı́n, A., &amp; Lopez-Poveda, E. A. (2019). Speech predictability can hinder communication in difficult listening conditions. Cognition, 192, 103992. https://doi.org/10.1016/j.cognition.2019.06.004 Mattys, S. L., Davis, M. H., Bradlow, A. R., &amp; Scott, S. K. (2012). Speech recognition in adverse conditions: A review. Language and Cognitive Processes, 27(7-8), 953–978. https://doi.org/10.1080/01690965.2012.705006 Nahum, M., Nelken, I., &amp; Ahissar, M. (2008). Low-level information and high-level perception: The case of speech in noise. PLoS Biology, 6(5), 0978–0991. https://doi.org/10.1371/journal.pbio.0060126 Nicenboim, B., Vasishth, S., &amp; Rösler, F. (2020). Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a Bayesian random-effects meta-analysis using publicly available data. Neuropsychologia, 142, 107427. https://doi.org/10.1016/j.neuropsychologia.2020.107427 Nieuwland, M. S., Politzer-Ahles, S., Heyselaar, E., Segaert, K., Darley, E., Kazanina, N., Von Grebmer Zu Wolfsthurn, S., Bartolozzi, F., Kogan, V., Ito, A., Mézière, D., Barr, D. J., Rousselet, G. A., Ferguson, H. J., Busch-Moreno, S., Fu, X., Tuomainen, J., Kulakova, E., Husband, E. M., … Huettig, F. (2018). Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. eLife, 7, 1–24. https://doi.org/10.7554/elife.33468 Nosofsky, R. M. (1986). Attention, similarity, and the identification–categorization relationship. Journal of Experimental Psychology: General, 115(1), 39. https://doi.org/10.1037/0096-3445.115.1.39 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Peelle, J. E. (2013). Cortical responses to degraded speech are modulated by linguistic predictions. Proceedings of Meetings on Acoustics ICA2013, 19, 060108. Prolific. (2014). Prolific academic. https://www.prolific.co. Rayner, K., Reichle, E. D., Stroud, M. J., Williams, C. C., &amp; Pollatsek, A. (2006). The effect of word frequency, word predictability, and font difficulty on the eye movements of young and older readers. Psychology and Aging, 21(3), 448. https://doi.org/10.1037/0882-7974.21.3.448 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008a). Priming and sentence context support listening to noise-vocoded speech by younger and older adults. The Journal of the Acoustical Society of America, 123(1), 489–499. https://doi.org/10.1121/1.2783762 Sommers, M. S., Nygaard, L. C., &amp; Pisoni, D. B. (1994). Stimulus variability and spoken word recognition. I. Effects of variability in speaking rate and overall amplitude. The Journal of the Acoustical Society of America, 96(3), 1314–1324. https://doi.org/10.1121/1.411453 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 Vaden, K. I., Kuchinsky, S. E., Cute, S. L., Ahlstrom, J. B., Dubno, J. R., &amp; Eckert, M. A. (2013). The cingulo-opercular network provides word-recognition benefit. Journal of Neuroscience, 33(48), 18979–18986. https://doi.org/10.1523/JNEUROSCI.1417-13.2013 van Os, M., Kray, J., &amp; Demberg, V. (2021b). Recognition of minipairs in (un)predictive sentence contexts in two types of noise. Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 43(43), 2943–2949. Wlotko, E. W., &amp; Federmeier, K. D. (2012). Age-related changes in the impact of contextual strength on multiple aspects of sentence comprehension. Psychophysiology, 49(6), 770–785. https://doi.org/10.1111/j.1469-8986.2012.01366.x However, Nieuwland et al. (2018) could not replicate DeLong et al. (2005)’s finding that comprehenders predict word-form. The N400 effect at the English articles a/an were not replicable.↩︎ "],["chapter-speech-rate.html", "7 Comprehension of degraded speech is modulated by the rate of speech 7.1 Introduction 7.2 Background 7.3 Experiment 1: Normal vs fast speech 7.4 Experiment 2: Normal vs slow speech 7.5 Conclusion 7.6 Summary", " 7 Comprehension of degraded speech is modulated by the rate of speech On the one hand, clean speech perception and reading studies have shown that contextual facilitation decreases with an increase in presentation rate (e.g., fast speech), with mixed evidence on the enhancement of contextual facilitation with a decrease in presentation rate (e.g., slow speech). On the other hand, it has been shown that semantic predictability facilitates language comprehension at a moderate level of spectral degradation (e.g., at 4-channel noise-vocoded speech) while degraded speech perception is inherently effortful. Considering these two lines of research and their inconsistencies, the study reported in this chapter aimed to examine how a change in speech rate modulates contextual facilitation in language comprehension when the speech is moderately degraded by noise-vocoding through 4 channels. To this end, we conducted two experiments: In Experiment 1, we compared participants’ word recognition in a sentence while they listened to the moderately degraded speech presented at a normal and fast speech rates (compressed by a factor of 0.65). In Experiment 2, we compared a separate group of participants’ word recognition in a moderately degraded speech presented at a normal and slow speech rates (expanded by a factor of 1.35). The sentences varied in the degree of predictability of the sentence-final target word (high and low predictability). Results of this study demonstrated that fast speech limits the time for lexical processing. This time constraint interferes with the lexical processing of words disproportionately affecting the low predictability sentences on top of effortful listening of the moderately degraded speech In contrast, slow speech does not amplify the contextual facilitation — we found the lexical processing, context representation, and semantic predictions to be optimal at the normal speech rate when the speech was moderately degraded. 7.1 Introduction When speech is degraded, its intelligibility and comprehension are reduced. For example, degradation by noise-vocoding reduces the spectral properties of speech rendering it difficult to understand (Davis et al., 2005; R. V. Shannon et al., 1995). Studies have shown that semantic predictability facilitates comprehension of moderately degraded speech (e.g., 4-channels noise-vocoded speech, Obleser &amp; Kotz, 2010), which we have also replicated in the study presented in the previous chapter (Chapter 5). That is, listeners utilise context information and form predictions about upcoming linguistic units, which in turn facilitates the comprehension of the degraded speech. However, prediction is a time- and resource-consuming mechanism (Pickering &amp; Gambi, 2018) such that an increase or decrease in speech rate can modulate a listener’s ability to use available context information and generate linguistic predictions (cf. Cole, 2020; Ito et al., 2016). More processing time is available at slow presentation rates (slow speech) and less at fast presentation rates (fast speech). So, the contextual facilitation is reduced in fast speech. However, the evidence of enhanced contextual facilitation in slow speech is mixed (Aydelott &amp; Bates, 2004; Goy et al., 2013; Koch &amp; Janse, 2016). Specifically for degraded speech, there is no clear evidence on how different speech rates affect the facilitatory effect (contextual facilitation) at a moderate level of degradation (Iwasaki et al., 2002; Meng et al., 2019; Winn &amp; Teece, 2021). Degraded speech is intrinsically effortful to listen to (Eckert et al., 2016; Wild et al., 2012). An increase (or decrease) in listening effort by a change in speech rate limits the cognitive resources available to encode the context information and form predictions (cf. Huettig &amp; Janse, 2016). Therefore, the present study aimed to investigate the effect of a change in speech rate on listeners’ ability to generate predictions while listening to a moderately degraded speech. One line of studies shows that at a moderate level of degradation (e.g., 4-channel noise-vocoding), semantic predictions facilitate language comprehension (e.g., Obleser et al., 2007). Another line of studies shows that an increase or decrease in speech rate modulates the predictability effect, i.e., contextual facilitation (e.g., Aydelott &amp; Bates, 2004; Goy et al., 2013). The current study is driven by an interest to bring these two lines of research together to understand the effect of speech rate on the facilitatory effect of predictability in degraded speech comprehension. We wanted to investigate how contextual facilitation at a moderate level of spectral degradation is affected by the change in speech rate. We expected that the contextual facilitation in degraded speech comprehension would be reduced by an increase in speech rate, while a decrease in speech rate would increase the contextual facilitation. 7.2 Background 7.2.1 Comprehension of fast and slow speech A change in speech rate (by uniform time-compression or expansion) manipulates the speech signal but does not by itself produce a spectral degradation (Charpentier &amp; Stella, 1986; Moulines &amp; Charpentier, 1990; Schlueter et al., 2014; but see Longster, 2003). Understanding fast speech is more effortful compared to normal and slow speech (e.g., Müller et al., 2019; Winn &amp; Teece, 2021; see also Simantiraki &amp; Cooke, 2020), and its intelligibility and comprehension are reduced (Fairbanks &amp; Kodman Jr., 1957; Peelle &amp; Wingfield, 2005; Schlueter et al., 2014). With an increased speech rate, processing demand increases as less time is available to process the incoming information (Gordon-Salant &amp; Fitzgibbons, 1995; Rodero, 2016; see also Rönnberg et al., 2013). Furthermore, some authors argue that the cognitive resources required for language processing are exhausted (Gordon-Salant &amp; Fitzgibbons, 2004; Janse, 2009). Since cognitive resources are also required to encode and process the context information for generating predictions (Pickering &amp; Gambi, 2018), it can be expected that the effect of predictability is reduced in fast speech. Studies comparing older and younger adults show that reduced intelligibility and comprehension in fast speech is associated with the limit of the central auditory processing system to process fast speech, identify the word, and activate its meaning (Wingfield et al., 1999; Wingfield et al., 2006; see also Connolly et al., 1990; Poldrack et al., 1998). Lerner et al. (2014) also show that the central auditory-language processing system is flexible and can rescale itself according to the speed of incoming information. That is, the information processing speed in the auditory-language processing system can change per the change in speech rate, however, there is an upper limit to the system’s flexibility. Beyond a certain maximum speed of speech rate, the processing of fast speech becomes difficult. In contrast, the central auditory-language comprehension system is shown to be flexible in processing slow speech without reducing its intelligibility to a certain lower limit of its rescaling capacity (Lerner et al., 2014). So, it can be expected that slow speech does not limit cognitive resources, and therefore processing context information to generate predictions in slow speech is not different from normal speech. Alternatively, slow speech provides more time to buffer the auditory information at the lower level of the information processing hierarchy (Ghitza &amp; Greenberg, 2009; Vagharchakian et al., 2012) and consequently provides more time for the central auditory-language comprehension system to use the context information and form predictions. Studies from the visual world paradigm also support this claim that slow speech provides more time for speech processing and semantic predictions (Fernandez et al., 2020; Huettig &amp; Janse, 2016). However, some studies have cast doubt on the processing advantage of slow speech, arguing that slow speech is perceived as overly artificial and demands high working memory (e.g., Kemper &amp; Harden, 1999; Nejime &amp; Moore, 1998; see also Liu &amp; Zeng, 2006; Love et al., 2009). In both younger and older adults, Sommers et al. (2020) found that slow speech does not render additional benefit in a sentence comprehension task in noise, even when supported by a visual context. Therefore, given these competing accounts, it is unclear whether the effect of predictability increases in slow speech compared to normal speech. Nonetheless, it is clear that a change in speech rate has different effects on speech intelligibility and language comprehension: Fast speech reduces intelligibility and comprehension, but the evidence for the beneficial/neutral effect of slow speech on language comprehension is mixed. A few studies have directly examined the role of fast and slow speech in listeners’ use of and benefit from semantic context by using clean speech. For instance, Aydelott &amp; Bates (2004) used a priming paradigm to examine the effects of contextual cues, which were target words embedded in sentences, and compared fast speech to normal speech. Target words were either congruent to the sentence context (100% cloze probability, i.e., in a constraining sentence context), incongruent (0% cloze probability, i.e., in an implausible sentence), or neutral (cloze probability not mentioned). Results indicated no reduction in the facilitatory effect of contextual cues (congruent versus neutral target words) at fast speech compared to normal speech. In contrast, they found a reduced inhibitory effect (incongruent versus neutral target words). They argued that the constraining sentence context was easy to process — fast speech did not interfere with the earlier stage of activation of words that matched the context (i.e., in the congruent trials). In contrast, the inhibition effect was reduced because there was less time to build up the representation of words in implausible sentence contexts, so less inhibition of the incongruent target word was needed. However, in a replication study of Aydelott &amp; Bates (2004), Goy et al. (2013) found that the facilitatory effect was reduced in fast speech compared to normal speech. They argued that the fast speech slowed down the activation of potential target words that matched the context, effectively reducing the contextual facilitation. In a recent study, Winn &amp; Teece (2021) did not observe an increase in contextual facilitation for slow speech compared to normal speech, although the intelligibility was slightly higher for slow speech among cochlear implantees. In another experiment, Koch &amp; Janse (2016) presented participants with a question-answer sequence of varying lengths across a wide range of normal and fast speech from the clean speech of Spoken Dutch Corpus (Oostdijk, 2000). They did not find any effect of predictability on word recognition. However, their study did not systematically control target word predictability and target word position in the sentences. The effects of varying presentation rates on semantic predictability have also been investigated with self-paced reading studies. For example, Wlotko &amp; Federmeier (2015) presented participants with context-evoking sentences followed by sentences containing a target word that was either expected (mean cloze probability of 74%) or unexpected (either same or different semantic category, both with cloze probability of approximately 0%). They found that the facilitation effect (as reflected in the N400 amplitude) was reduced in the sentences that were presented fast compared to the ones that were presented slow. They suggested that at a fast presentation rate, predictive preactivation of words was not common: There was not enough time to activate proper representation to process upcoming words. In the same study, however, the semantic facilitation effect was not reduced when the slow presentation followed the fast presentation in separate blocks. That is, an increase in the flow of information did not always impair the ability to predict. They argued that once the brain is engaged in predictive comprehension mode, for example, first in the slow presentation rate, it then continues to allocate resources in the same mode under a faster presentation rate. Dambacher et al. (2012) also showed that the N400 effect was delayed and smaller at a fast presentation rate compared to slow presentation rates. To summarise, there is already some evidence from studies applying various paradigms that the predictability of the sentence context interacts with the presentation rate of incoming information (Aydelott &amp; Bates, 2004; Dambacher et al., 2012; Ito et al., 2016; Sharit et al., 2003; Winn &amp; Teece, 2021; Wlotko &amp; Federmeier, 2015). The predictability effect is generally reduced for fast speech, while the findings are inconsistent in the case of slow speech. Fast speech interferes with the lexical processing and activation of words that match the context, as limited time would be available to form expectations about an upcoming word (Dambacher et al., 2012; Goy et al., 2013). In contrast, slow speech can provide more time for listeners to form a rich context representation and generate a prediction about an upcoming word than a normal speech rate (cf. Huettig &amp; Guerra, 2019). 7.2.2 Speech rate and contextual facilitation of moderately degraded speech Predictions about upcoming linguistic units are generated as a listener forms a meaning representation of context information from a speech signal. Such linguistic predictions facilitate comprehension of degraded speech when the degradation is at a moderate level. However, the effect of predictability observed at the moderate degradation level no longer exists if the listener does not understand the context. Therefore, it is essential that the speech rate remains within the listener’s limit to buffer and process the auditory information (Vagharchakian et al., 2012) so that the listener can form the representation of the context and have sufficient time to generate predictions. Several studies have examined the role of speech rate on the intelligibility and comprehension of degraded speech but without considering predictability effects. For example, Meng et al. (2019) found that an increase in speech rate had a much more severe effect on spectrally degraded speech (4-channel sine-wave vocoded) than on clean speech. To achieve the same level of accuracy, listeners required degraded speech to be much slower than normal speech rate. Among cochlear implantees whose speech input is spectrally degraded (R. V. Shannon et al., 2004), Iwasaki et al. (2002) found that a change in speech rate from slow to fast reduced word recognition accuracy. Their speech perception was impaired with an increased speech rate, and it was improved when the speech rate was decreased (e.g., Dincer D’Alessandro et al., 2018). Winn &amp; Teece (2021)‘s study showed no significant difference in the facilitatory effect of semantic predictability between slow and normal speech rates. This was attributed to listeners’ “repair” strategy at normal speech rate such that they made sensible guesses about the words that fit the given context. Similar to the studies conducted with clean speech, these studies also indicate that an increase in the speed of degraded speech is detrimental to its intelligibility, while its intelligibility increases with a decrease in speech rate. Taken together, the utility of semantic predictability in comprehension of degraded speech is fairly established. However, the findings about the effect of speech rate on predictability effect in degraded speech comprehension are inconsistent. Similarly, prediction itself is a time- and resource-consuming mechanism (Pickering &amp; Gambi, 2018) which is affected by a comprehender’s processing speed (e.g., Huettig &amp; Janse, 2016). However, the role of the speed of incoming information (i.e., speech rate of a degraded speech) on a listener’s ability to form predictions, and hence its interplay with the facilitatory effect of semantic predictability at a moderately degraded speech, remains unclear. It can be speculated from the findings of the abovementioned studies that fast speech reduces the availability of time and resources to process the speech signal and generate predictions, in addition to the effortful listening of degraded speech. It can similarly be speculated that slow speech provides listeners more time than normal speech to process the words and form a representation of context information, in addition to reducing the effortful listening of degraded speech. 7.2.3 The present study Semantic predictability has been shown to facilitate degraded speech comprehension when the degradation level is moderate at normal speech rate. The aim of this study was to investigate whether an increase (and decrease) in speech rate reduces (and amplifies) the facilitatory effect of semantic predictability. We systematically examined whether contextual facilitation at a moderate level of degradation varies with a change in speech rate for which semantic predictability was manipulated by varying the cloze probability of target words in a sentence, and moderate degradation was achieved by noise-vocoding of speech through 4 channels. 4-channel noise-vocoding has been shown to be the moderate degradation level in the previous chapter, similar to the findings of Obleser &amp; Kotz (2010). Speech rate was manipulated by time-compression (and expansion) of the moderately degraded speech to make it fast (and slow). To achieve our aim, we conducted two experiments in which listeners were instructed to listen to the sentences and type in the entire sentence they heard. Sentence comprehension (word recognition accuracy) for high and low predictability sentences was assessed in fast speech (Experiment 1) and slow speech (Experiment 2). The processing demand increases, and a limited time is available to process the context and generate predictions with an increase in speech rate (e.g., Aydelott &amp; Bates, 2004; Wlotko &amp; Federmeier, 2015; see also Pickering &amp; Gambi, 2018). Therefore, we expected that the contextual facilitation (i.e., the increase in word recognition accuracy in high predictability sentences compared to low predictability sentences) would be reduced for fast speech compared to normal speech (Experiment 1). However, for slow speech, due to an abundance of time to process the degraded speech and the context and reduced listening effort (e.g., Winn &amp; Teece, 2021), we expected contextual facilitation to be increased compared to normal speech (Experiment 2). We expected that both increase and decrease in contextual facilitation would be primarily driven by the ease of processing high predictability sentences compared to low predictability sentences (Aydelott &amp; Bates, 2004; Goy et al., 2013). 7.3 Experiment 1: Normal vs fast speech This experiment was conducted to investigate the effect of an increase in speech rate on predictability effect in the comprehension of the 4-channel noise-vocoded speech. We examined if the facilitatory effect of predictability decreased as the speech sped up by a compression factor of 0.65. 7.3.1 Methods 7.3.1.1 Participants We recruited one group of participant (n=101; (\\(M\\) age \\(\\pm SD=23.14\\pm 3.31\\) years; age range = 18-31 years; 66 females, 1 preferred not to say) online via Prolific Academic (Prolific, 2014). All participants were native speakers of German and did not have any speech-language disorder, hearing loss, or neurological disorder (all self-reported). All participants received 6.20 Euro as monetary compensation for their participation in the approximately 40 minutes long experiment. 7.3.1.2 Materials In this experiment, we used a subset of materials created by the method described in Chapter 3 (Section 3.1). One hundred twenty sentences each for low predictability and high predictability sentences that differed in the cloze probability of sentence-final target words were used. Mean cloze probabilities of the target words of low and high predictability sentences were \\(0.022\\pm0.027\\) (\\(M\\pm SD\\); range = 0.00-0.09) and \\(0.752\\pm0.123\\) (\\(M\\pm SD\\); range = 0.56-1.00) respectively. The audio recodings of all 240 sentences were compressed by a factor of 0.65 in Praat software to create fast speech (see Chapter 3 Section 3.1.2.2 for details). Then the recordings of speech signal at fast rate and normal rate were passed through 4 channels of noise-vocoding to create moderately degraded speech stimuli of two types: fast speech and normal speech (see Chapter 3 Section 3.1.2.1 for details). Each participant was presented with 120 unique sentences: 60 HP and 60 LP sentences. Speech rate was also balanced across each predictability level. The participants received 30 sentences with normal speed and 30 with fast speed in each of the predictability conditions resulting into 4 experimental lists. The sentences in each list were pseudo-randomised, that is, not more than 3 sentences of same speed, or same predictability condition appeared consecutively. 7.3.1.3 Procedure Participants were asked to use earphones or headphones. A sample of vocoded speech not used in the main experiment and the practice trial was provided so that the participants could adjust the volume to a preferred level of comfort at the beginning of the experiment. The participants were instructed to listen and report the entire sentences by typing in the everything they heard using the keyboard. The time for typing in the response was not limited. They were informed at the beginning of the experiment that some of the sentences would be ‘noisy’ and not easy to understand. Guessing was encouraged. To familiarise the participants with the task, eight practice trials with different levels of speech degradation were provided before presenting all 120 experimental trials with an intertrial interval of 1,000 ms. Each participant was presented 60 high and 60 low predictability sentences. Speech rate was also balanced across each predictability condition. For each predictability condition, 30 sentences with fast speech and 30 with normal speech were presented. Sentences were pseudo-randomised so that no more than three sentences of the same predictability level or speech rate appeared consecutively. A total of four lists were constructed. 7.3.2 Analyses We have already shown in the previous chapters that predictability effects, i.e., contextual facilitation in language comprehension can be rightfully measured only if we consider the trials in which participants accurately identify the context. Verbs are predictive of the nouns in our stimuli (e.g., Sie jongliert die Bälle). Therefore, we discarded the trials in which verbs were identified incorrectly. Accuracy was analysed using Generalized Linear Mixed Models (GLMMs) following the procedure described in Chapter 4 (Section 4.2) similar to the preceding chapters. Binary responses (categorical: correct and incorrect) for all participants were fit with a binomial linear mixed-effects model. Correct responses were coded as 1, and incorrect responses were coded as 0. Speech rate (categorical: fast speech and slow speech), target word predictability (categorical: high predictability sentences and low predictability sentences), and their interaction were included in the fixed effects. We fitted a model with a maximal random effects structure that included random intercepts for each participant and item (Barr et al., 2013). Both by-participant and by-item random slopes were included for speech rate, target word predictability, and their interaction, which was supported by the experiment design. We applied treatment contrast for both predictability and speech rate, mapping low predictability and normal speech to the intercept. 7.3.3 Results and discussion Mean response accuracies (in percentage) for all experimental conditions aggregated across all participants and items are shown in Table ?? and Figure 7.1. It shows that accuracy increases with an increase in target word predictability, but it decreases with an increase in speech rate. The results of the statistical analysis confirmed these observations (Table ??). Figure 7.1: Mean response accuracy across all conditions in Experiment 1. Accuracy increased only with an increase in the target-word predictability and a decrease in speech rate. Error bars represent the standard error of the means. We found a significant main effect of target word predictability (\\(\\beta\\) = 2.42, SE = .28, z = 8.55, p &lt; .001) and a significant main effect of speech rate (\\(\\beta\\) = -.98, SE = .24, z = 4.16, p &lt; .001) suggesting that participants’ response accuracy was higher for the high predictability sentences than for the low predictability sentences and for normal speech than for fast speech. We also found a significant interaction between target word predictability and speech rate (\\(\\beta\\) = 1.06, SE = .42, z = 2.50, p = .01). These findings show that the effect of target word predictability, i.e., contextual facilitation was reduced at fast speech (see Figure 7.1). Separate planned analyses of each predictability level were performed. There was no significant main effect of speech rate at high predictability condition (\\(\\beta\\) = .02, SE = .34, z = .05, p = .96). At low predictability condition, in contrast, we found a significant main effect of speech rate (\\(\\beta\\) = -.99, SE = .27, z = -3.72, p &lt; .001). Hence, response accuracy decreased at fast speech, but only for the low predictability sentences. Separate planned analyses of each speech rate revealed that there was significant main effect of predictability in both normal speech (\\(\\beta\\) = 1.98, SE = .28, z = 7.05, p &lt; .001) and fast speech (\\(\\beta\\) = 2.67, SE = .37, z = 7.14, p &lt; .001), but the effect appeared to be higher for fast speech (\\(\\beta\\) = 2.67) than for normal speech (\\(\\beta\\) = 1.98). This resulted from a significant drop in accuracy at the low predictability condition rather than a rise in accuracy at high predictability condition in the fast speech. These results indicated an increase in response accuracy with an increase in target word predictability only at the normal speech rate. Fast speech rate significantly reduced the accuracy in the low predictability condition. It suggests that fast speech incurs a cost in processing the low predictability sentences and reduces the contextual facilitation in degraded speech comprehension. These findings also align with previous studies conducted with clean speech, which reported that fast speech reduces contextual facilitation (e.g., Aydelott &amp; Bates, 2004). We conducted another experiment to examine if slowing down the speech rate eases the processing of both low and high predictability sentences and increases the contextual facilitation in comprehension of moderately degraded speech. 7.4 Experiment 2: Normal vs slow speech Following up on Experiment 1, we conducted Experiment 2 on a separate group of participants to investigate the effect of a decrease in speech rate on predictability effect in the comprehension of the 4-channel noise-vocoded speech. We examined if the facilitatory effect of predictability increased as the speech slowed down by an expansion factor of 1.65. 7.4.1 Methods 7.4.1.1 Participants and Materials We recruited another group of participant (n=101; \\(M\\) age \\(\\pm SD=23.49\\pm 3.26\\) years; age range = 18-30 years; 60 females, 1 preferred not to say) online via Prolific Academic (Prolific, 2014). The same sentences were used as stimuli as in Experiment 1. Instead of compression, the auditory recordings were expanded by a factor of 1.35 to create slow speech. The rest of the procedure, including the noise-vocoding through 4 channels, to create stimuli were the same as in Experiment 1. This resulted in two types of 4-channel noise-vocoded speech: slow speech and normal speech. 7.4.1.2 Procedure The same procedure was followed as in Experiment 1. Participants were asked to use earphones or headphones. They were instructed to report the entire sentence by typing in what they heard. Four experimental lists were constructed to present each participant with 60 high and 60 low predictability sentences. Speech rate was also balanced across each predictability condition in each list. For each predictability condition, 30 sentences with slow speech and 30 with normal speech were presented. Sentences were pseudo-randomised so that no more than three sentences of the same predictability level or speech rate appeared consecutively. 7.4.2 Analyses The data analysis procedure was the same as Experiment 1. Accuracy was analysed using Generalized Linear Mixed Models (GLMMs). We fit a model with maximal random effects structure. Treatment contrast was applied for both predictability and speech rate, mapping low predictability and normal speech to the intercept. 7.4.3 Results and discussion Mean response accuracies (in percentage) for all experimental conditions aggregated across all participants and items are shown in Table ?? and Figure 7.2. It shows that accuracy increases with an increase in target word predictability but did not increase with a decrease in speech rate. The results of the statistical analysis confirmed these observations (Table ??). Figure 7.2: Mean response accuracy across all conditions in Experiment 2. Accuracy increased only with an increase in the target-word predictability, but a change in speech rate had no significant effect on accuracy. Error bars represent the standard error of the means. We again found a significant main effect of target word predictability, indicating that participants’ response accuracy was higher for the high predictability condition than for the low predictability condition (\\(\\beta\\) = 2.58, SE = .30, z = 8.65, p &lt; .001), replicating the effect in the previous experiment. In contrast to Experiment 1, we did not find a significant main effect of speech rate (\\(\\beta\\) = -.08, SE = .15, z = .57, p = .568), nor there was a significant interaction between speech rate and target word predictability (\\(\\beta\\) = .44, SE = .27, z = 1.65, p = .099). The absence of the main effect of speech rate and the interaction between speech rate and predictability suggested that there was no change in participants’ response accuracy with a reduction in speech rate, and the contextual facilitation did not significantly increase or decrease with slowing down of the speech rate. In contrast to Experiment 1, Experiment 2 did not indicate a differential effect of speech rates in the comprehension of high and low predictability sentences. While Experiment 1 showed that speeding up the speech rate significantly reduced the accuracy of low predictability sentences, such a reduction was not observed in Experiment 2 when the speech rate was slowed down. Although listeners’ response accuracy was reduced in both fast and slow speech than in normal speech, their ability to utilise context information was only impaired by the fast speech in the low predictability sentences. 7.5 Conclusion The main goal of the present study was to examine whether the contextual facilitation (i.e., the facilitatory effect of semantic predictability) in comprehension of a moderately degraded speech is modulated by changes in speech rate. The results of two experiments revealed that fast speech selectively impedes the comprehension of low predictability sentences, while slow speech has no detrimental or beneficial effect on contextual facilitation. In both experiments, our results showed a significant main effect of predictability at normal speech rate, i.e., we observed a facilitatory effect of semantic predictability at normal speech rate under moderate degradation level by noise-vocoding through 4 channels. This replicates the findings of earlier studies (e.g., Obleser &amp; Kotz, 2010) and the study presented in Chapter 6 in which participants were presented only with normal speech rate, and contextual facilitation was observed at the spectral degradation through 4-channel noise-vocoding. At this moderate degradation level, listeners could decode the context and form its meaning representation. Consequently, they generated predictions about the upcoming target word in a sentence even in low predictability conditions depending on the contextual constraint of the sentences (cf. Strauß et al., 2013). The expected interaction between speech rate and target word predictability in Experiment 1 showed that comprehension of degraded speech was significantly impaired for low predictability sentences but not for high predictability sentences in fast speech. Listening to degraded speech is effortful and requires more attentional resources than clean speech (Wild et al., 2012). When presented as a fast speech, spectral degradation imposes additional cognitive demands; and less time is available to process the speech signal. The central auditory-language processing system does not rescale itself according to the speed of the fast speech presented in Experiment 1 (Lerner et al., 2014). It is then difficult to process the fast speech, decode the context information and form its meaning representation from the degraded speech to generate predictions about upcoming target words. This disproportionately affects low predictability sentences as fast speech interferes with the lexical processing of words, likely reducing the activation of target words in less constraining sentence contexts (Aydelott &amp; Bates, 2004; Dambacher et al., 2012; cf. Goy et al., 2013). As a result, language comprehension in the low predictability condition is impaired more than in the high predictability condition in Experiment 1. In contrast to Experiment 1, we did not find the expected interaction between speech rate and target word predictability in Experiment 2, i.e., a decrease in speech rate did not differentially affect the comprehension of high or low predictability sentences. Unlike Experiment 1, we did not observe a significant change in contextual facilitation in the slow speech at 4-channel noise-vocoding level in Experiment 2. Slowing down the speech gives listeners more time to process the information, including the context that is important to generate predictions. However, our findings show that the added time in slow speech does not benefit intelligibility and comprehension of sentences more than the normally available time at a normal speech rate. Comprehenders’ lexical processing is optimal at the normal presentation rate (Dambacher et al., 2012). Although slow speech reduces effortful listening of degraded speech, the resources thus freed up by the slow speech are not allocated to enhance contextual facilitation. This argument is in line with Winn &amp; Teece (2021), who reported that contextual facilitation does not increase when the speech is slowed down. Alternatively, it is plausible that the artificial expansion of speech introduced distortion in the speech signal. Although speech intelligibility and comprehension are increased by slow speech (Dincer D’Alessandro et al., 2018; Iwasaki et al., 2002), acoustic distortion due to artificial expansion reduces intelligibility and comprehension (Longster, 2003). As a result, we did not observe an overall amplification of contextual facilitation in the slow speech at the moderate degradation level in Experiment 2. Accounts from speech perception and predictive language processing point to a common expectation: contextual facilitation is enhanced when comprehenders have more time to process the presented information (Huettig &amp; Guerra, 2019; Ito et al., 2016; Kuperberg &amp; Jaeger, 2016). However, there is conflicting empirical evidence on whether an increase or a decrease in speech rate benefits intelligibility, comprehension, and contextual facilitation. Our findings show this interplay among spectral degradation, speech rate, and semantic prediction. Although reducing the speech rate provides time to process the information (including the context) in the degraded speech, this does not necessarily ease the processing of high or low predictability sentences differentially. Thus, no increased facilitatory effect is observed at the slow speech rate. In contrast, increasing the speech rate adds more cognitive load on top of the effort required to listen to the degraded speech. This results in difficulty processing and understanding the rapidly unfolding sentences; this difficulty further increases when the target words are not easily predictable. Similarly, in the “narrowed expectations” framework of degraded speech comprehension, Strauß et al. (2013) argue that lexical-semantic cues are more robust to degradation and the target words can be processed faster and relatively more easily in a highly constraining context than in a less constraining context. They posit that listeners can activate a narrow range of most likely target words from the context in a high predictability sentence. Our results show that when the moderately degraded speech is sped up, the context is still robust: There is enough processing time available in the fast speech to process the context and generate a small range of lexical predictions about an upcoming target word in a high predictability sentence. Therefore, the comprehension of high predictability sentences is not reduced due to an increase in speech rate. However, in a low predictability sentence, the range of probable sentence endings is too wide to generate enough lexical predictions that facilitate comprehension. When the processing time is short, it further reduces the activation of likely sentence endings, especially when the context is less constraining (Aydelott &amp; Bates, 2004). An alternative explanation of our findings of contextual facilitation could be that the listeners first identified the noun (e.g., Bälle), then integrated it with the verb (e.g., jongliert) instead of first identifying the verb and predicting the noun. To rule this out, we conducted an additional analysis, the results of which supported the prediction-based explanation. We compared the effect (estimates) of forward-predictability (from the verb to noun) with that of backward guessing (identifying the verb from the correct identification of the noun). In both experiments, the forward predictability effect was larger than the backward guessing. The results of these complementary analyses (see Appendix C) favour the explanation that the contextual facilitation we observed is due mainly to predictability rather than backward guessing (or backward integration). Of note, the generalisation of our results is limited. First, we tested only with one expansion factor of 1.35 and one compression factor of 0.65. It can be speculated that an increase in facilitatory effect could be observed when the speech is expanded to different levels by including other expansion factors. There could be an optimal trade-off between slowing down the speech with more time to process (Fernandez et al., 2020) and the speech still remaining intelligible. This could be further explored in alternate experimental designs or methods: Instead of using PSOLA to create slow speech, silent pauses can be inserted in a speech stream (in the semantic or syntactic boundaries between the words) to create slow speech that sounds more natural (Wingfield et al., 1999), or the speech can be recorded in a desired (slow or fast) natural-sounding speech rate (Adank &amp; Janse, 2009) before noise-vocoding it. These methods have disadvantages like the omission of coarticulatory cues due to the insertion of pauses, slow-sounding speech being different from conversational speech, and difficulty maintaining a constant speed in the slower speech rates while recording. Nevertheless, one can reduce the additional degradation that can be caused by algorithmic speech expansion and, at the same time, use a wide range of slow speech rates that allow listeners to process the spectrally degraded speech at meaningful semantic and syntactic boundaries. Second, we only tested younger adults. We did not examine the effect of cognitive ageing on contextual facilitation of comprehension of fast and slow speech. Older adults have delayed processing speed such that slow speech generally improves their speech intelligibility and language comprehension. Furthermore, semantic context benefits older adults more than younger adults in adverse listening conditions. Therefore, the effect of slow speech could be different in older adults than what we found in younger adults under adverse listening conditions. To conclude, we show that access (or restriction) to lexical processing is associated with the speed of information flow, and the constraints in attentional and cognitive resources are key factors that influence contextual facilitation of moderately degraded speech. Lexical processing is restricted in an effortful listening of the rapidly unfolding degraded speech; consequently, understanding the words that are not easily predictable from the context is difficult. On the contrary, auditory-language comprehension is optimal in the normal speech rate, and thus, slowing down the degraded speech does not necessarily amplify contextual facilitation. 7.6 Summary This chapter reported studies investigating the effect of changes in speech rate on contextual facilitation of a moderately degraded speech. While it was already shown in the preceding chapter that at a normal speech rate, predictability facilitates comprehension of degraded speech at 4-channel noise-vocoding level, in this chapter, the effect of increase and decrease of speech rate on this predictability effect was investigated. When the speech rate is increased, we found that activating target words using the context information is more difficult in the low predictability sentences than in the high predictability sentences. This is on top of the fact that fast speech adds further load in processing the inherently effortful degraded speech. In contrast, slowing down the moderately degraded speech did not provide any benefit in the comprehension of high or low predictability sentences. Although slow speech reduces the effort on listening to degraded speech, this chapter concluded that thus freed up resources are not necessarily used to amplify the predictability effects. All in all, this chapter revealed that the bottom-up perceptual property, like speech rate, interacts with the top-down predictive processes, in addition to other perceptual properties, like speech degradation; however, the nature of this interaction depends on the nature of the speech (i.e., fast vs slow). In the next chapter, we provide a concluding remark on the studies presented in Chapters 5, 6, and 7. We discuss the limitations of the studies, their generalisation, and future outlook. Bibliography Adank, P., &amp; Janse, E. (2009). Perceptual learning of time-compressed and natural fast speech. The Journal of the Acoustical Society of America, 126(5), 2649–2659. https://doi.org/10.1121/1.3216914 Aydelott, J., &amp; Bates, E. (2004). Effects of acoustic distortion and semantic context on lexical access. Language and Cognitive Processes, 19(1), 29–56. https://doi.org/10.1080/01690960344000099 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Charpentier, F. J., &amp; Stella, M. G. (1986). Diphone synthesis using an overlap-add technique for speech waveforms concatenation. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 2015–2018. https://doi.org/10.1109/icassp.1986.1168657 Cole, A. (2020). The effects of prediction and speech rate on lexical processing (p. 35) [Masters thesis, University of Maryland]. https://doi.org/10.13016/37my-jxp7 Connolly, J. F., Stewart, S., &amp; Phillips, N. (1990). The effects of processing requirements on neurophysiological responses to spoken sentences. Brain and Language, 39(2), 302–318. Dambacher, M., Dimigen, O., Braun, M., Wille, K., Jacobs, A. M., &amp; Kliegl, R. (2012). Stimulus onset asynchrony and the timeline of word recognition: Event-related potentials during sentence reading. Neuropsychologia, 50(8), 1852–1870. https://doi.org/10.1016/j.neuropsychologia.2012.04.011 Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 Dincer D’Alessandro, H., Boyle, P. J., Ballantyne, D., De Vincentiis, M., &amp; Mancini, P. (2018). The role of speech rate for Italian-speaking cochlear implant users: insights for everyday speech perception. International Journal of Audiology, 57(11), 851–857. https://doi.org/10.1080/14992027.2018.1498139 Eckert, M. A., Teubner-Rhodes, S., &amp; Vaden, K. I. (2016). Is Listening in Noise Worth It? The Neurobiology of Speech Recognition in Challenging Listening Conditions. Ear &amp; Hearing, 37(1), 101S–110S. https://doi.org/10.1097/aud.0000000000000300 Fairbanks, G., &amp; Kodman Jr., F. (1957). Word intelligibility as a function of time compression. The Journal of the Acoustical Society of America, 29(5), 636–641. Fernandez, L. B., Engelhardt, P. E., Patarroyo, A. G., &amp; Allen, S. E. M. (2020). Effects of speech rate on anticipatory eye movements in the visual world paradigm: Evidence from aging, native, and non-native language processing. Quarterly Journal of Experimental Psychology, 73(12), 2348–2361. https://doi.org/10.1177/1747021820948019 Ghitza, O., &amp; Greenberg, S. (2009). On the possible role of brain rhythms in speech perception: Intelligibility of time-compressed speech with periodic and aperiodic insertions of silence. Phonetica, 66(1-2), 113–126. https://doi.org/10.1159/000208934 Gordon-Salant, S., &amp; Fitzgibbons, P. J. (1995). Recognition of multiply degraded speech by young and elderly listeners. Journal of Speech and Hearing Research, 38(5), 1150–1156. https://doi.org/10.1044/jshr.3805.1150 Gordon-Salant, S., &amp; Fitzgibbons, P. J. (2004). Effects of stimulus and noise rate variability on speech perception by younger and older adults. The Journal of the Acoustical Society of America, 115(4), 1808–1817. https://doi.org/10.1121/1.1645249 Goy, H., Pelletier, M., Coletta, M., &amp; Pichora-Fuller, M. K. (2013). The effects of semantic context and the type and amount of acoustic distortion on lexical decision by younger and older adults. Journal of Speech, Language, and Hearing Research, 56(6), 1715–1732. https://doi.org/10.1044/1092-4388(2013/12-0053) Huettig, F., &amp; Guerra, E. (2019). Effects of speech rate, preview time of visual context, and participant instructions reveal strong limits on prediction in language processing. Brain Research, 1706(June 2017), 196–208. https://doi.org/10.1016/j.brainres.2018.11.013 Huettig, F., &amp; Janse, E. (2016). Individual differences in working memory and processing speed predict anticipatory spoken language processing in the visual world. Language, Cognition and Neuroscience, 31(1), 80–93. https://doi.org/10.1080/23273798.2015.1047459 Ito, A., Corley, M., Pickering, M. J., Martin, A. E., &amp; Nieuwland, M. S. (2016). Predicting form and meaning: Evidence from brain potentials. Journal of Memory and Language, 86, 157–171. https://doi.org/10.1016/j.jml.2015.10.007 Iwasaki, S., Ocho, S., Nagura, M., &amp; Hoshino, T. (2002). Contribution of speech rate to speech perception in multichannel cochlear implant users. Annals of Otology, Rhinology and Laryngology, 111(8), 718–721. https://doi.org/10.1177/000348940211100811 Janse, E. (2009). Processing of fast speech by elderly listeners. The Journal of the Acoustical Society of America, 125(4), 2361–2373. https://doi.org/10.1121/1.3082117 Kemper, S., &amp; Harden, T. (1999). Experimentally disentangling what’s beneficial about elderspeak from what’s not. Psychology and Aging, 14(4), 656–670. https://doi.org/10.1037/0882-7974.14.4.656 Koch, X., &amp; Janse, E. (2016). Speech rate effects on the processing of conversational speech across the adult life span. The Journal of the Acoustical Society of America, 139(4), 1618–1636. https://doi.org/10.1121/1.4944032 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Lerner, Y., Honey, C. J., Katkov, M., &amp; Hasson, U. (2014). Temporal scaling of neural responses to compressed and dilated natural speech. Journal of Neurophysiology, 111(12), 2433–2444. https://doi.org/10.1152/jn.00497.2013 Liu, S., &amp; Zeng, F.-G. (2006). Temporal properties in clear speech perception. The Journal of the Acoustical Society of America, 120(1), 424–432. https://doi.org/10.1121/1.2208427 Longster, J. A. (2003). Concatenative speech synthesis : A framework for reducing perceived distortion when using the TD-PSOLA algorithm [Doctoral dissertation]. Bournemouth University. Love, T., Walenski, M., &amp; Swinney, D. (2009). Slowed speech input has a differential impact on on-line and off-line processing in children’s comprehension of pronouns. Journal of Psycholinguistic Research, 38(3), 285–304. https://doi.org/10.1007/s10936-009-9103-9 Meng, Q., Wang, X., Cai, Y., Kong, F., Buck, A. N., Yu, G., Zheng, N., &amp; Schnupp, J. W. H. (2019). Time-compression thresholds for Mandarin sentences in normal-hearing and cochlear implant listeners. Hearing Research, 374, 58–68. https://doi.org/10.1016/j.heares.2019.01.011 Moulines, E., &amp; Charpentier, F. (1990). Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Communication, 9(1990), 453–467. https://doi.org/10.1016/0167-6393(90)90021-Z Müller, J. A., Wendt, D., Kollmeier, B., Debener, S., &amp; Brand, T. (2019). Effect of speech rate on neural tracking of speech. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.00449 Nejime, Y., &amp; Moore, B. C. J. (1998). Evaluation of the effect of speech-rate slowing on speech intelligibility in noise using a simulation of cochlear hearing loss. The Journal of the Acoustical Society of America, 103(1), 572–576. https://doi.org/10.1121/1.421123 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Oostdijk, N. (2000, May). The spoken Dutch corpus. Overview and first evaluation. Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00). http://www.lrec-conf.org/proceedings/lrec2000/pdf/110.pdf Peelle, J. E., &amp; Wingfield, A. (2005). Dissociations in perceptual learning revealed by adult age differences in adaptation to time-compressed speech. Journal of Experimental Psychology: Human Perception and Performance, 31(6), 1315–1330. https://doi.org/10.1037/0096-1523.31.6.1315 Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Poldrack, R., Protopapas, A., Nagarajan, S., Tallal, P., Merzenich, M., Temple, E., &amp; Gabrieli, J. (1998). Auditory processing of temporally compressed speech: An fMRI study. Journal of Cognitive Neuroscience, 10, 126–126. Prolific. (2014). Prolific academic. https://www.prolific.co. Rodero, E. (2016). Influence of speech rate and information density on recognition: The moderate dynamic mechanism. Media Psychology, 19(2), 224–242. https://doi.org/10.1080/15213269.2014.1002942 Rönnberg, J., Lunner, T., Zekveld, A., Sörqvist, P., Danielsson, H., Lyxell, B., Dahlström, Ö., Signoret, C., Stenfelt, S., Pichora-Fuller, M. K., &amp; Rudner, M. (2013). The Ease of Language Understanding (ELU) model: Theoretical, empirical, and clinical advances. Frontiers in Systems Neuroscience, 7(31), 1–17. https://doi.org/10.3389/fnsys.2013.00031 Schlueter, A., Lemke, U., Kollmeier, B., &amp; Holube, I. (2014). Intelligibility of time-compressed speech: The effect of uniform versus non-uniform time-compression algorithms. The Journal of the Acoustical Society of America, 135(3), 1541–1555. https://doi.org/10.1121/1.4863654 Shannon, R. V., Fu, Q.-J., &amp; Galvin Iii, J. (2004). The number of spectral channels required for speech recognition depends on the difficulty of the listening situation. Acta Oto-Laryngologica, 124(0), 50–54. https://doi.org/10.1080/03655230410017562 Shannon, R. V., Zeng, F.-G., Kamath, V., Wygonski, J., &amp; Ekelid, M. (1995). Speech Recognition with Primarily Temporal Cues. Science, 270(5234), 303–304. https://doi.org/10.1126/science.270.5234.303 Sharit, J., Czaja, S. J., Nair, S., &amp; Lee, C. C. (2003). Effects of age, speech rate, and environmental support in using telephone voice menu systems. Human Factors, 45(2), 234–251. https://doi.org/10.1518/hfes.45.2.234.27245 Simantiraki, O., &amp; Cooke, M. (2020). Exploring listeners’ speech rate preferences. Proc. Interspeech 2020, 1346–1350. https://doi.org/10.21437/Interspeech.2020-1832 Sommers, M. S., Spehar, B., Tye-Murray, N., Myerson, J., &amp; Hale, S. (2020). Age differences in the effects of speaking rate on auditory, visual, and auditory-visual speech perception. Ear and Hearing, 41(3), 549–560. https://doi.org/10.1097/AUD.0000000000000776 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 Vagharchakian, L., Dehaene-Lambertz, G., Pallier, C., &amp; Dehaene, S. (2012). A temporal bottleneck in the language comprehension network. Journal of Neuroscience, 32(26), 9089–9102. https://doi.org/10.1523/JNEUROSCI.5685-11.2012 Wild, C. J., Yusuf, A., Wilson, D. E., Peelle, J. E., Davis, M. H., &amp; Johnsrude, I. S. (2012). Effortful listening: The processing of degraded speech depends critically on attention. Journal of Neuroscience, 32(40), 14010–14021. https://doi.org/10.1523/JNEUROSCI.1528-12.2012 Wingfield, A., McCoy, S. L., Peelle, J. E., Tun, P. A., &amp; Cox, C. L. (2006). Effects of Adult Aging and Hearing Loss on Comprehension of Rapid Speech Varying in Syntactic Complexity. Journal of the American Academy of Audiology, 17(07), 487–497. https://doi.org/10.3766/jaaa.17.7.4 Wingfield, A., Tun, P. A., Koh, C. K., &amp; Rosen, M. J. (1999). Regaining lost time: Adult aging and the effect of time restoration on recall of time-compressed speech. Psychology and Aging, 14(3), 380–389. https://doi.org/10.1037/0882-7974.14.3.380 Winn, M. B., &amp; Teece, K. H. (2021). Slower speaking rate reduces listening effort among listeners with cochlear implants. Ear and Hearing, 42(3), 584. https://doi.org/10.1097/aud.0000000000000958 Wlotko, E. W., &amp; Federmeier, K. D. (2015). Time for prediction? The effect of presentation rate on predictive sentence comprehension during word-by-word reading. Cortex, 68, 20–32. https://doi.org/10.1016/j.cortex.2015.03.014 "],["chapter-conclusion.html", "8 Discussion and conclusion 8.1 Overview of the main findings 8.2 Implications of the findings 8.3 Limitations and outlook on future studies 8.4 Concluding remark", " 8 Discussion and conclusion In this chapter, we present an overview of the experimental findings reported in the preceding chapters, and their conclusions are discussed in the broader context of probabilistic accounts of language processing. The noisy channel model of communication (see Chapter 1 for elaborate discussion) is revisited and discussed concerning the findings of our studies. We also discuss the implications, limitations, outlook and considerations for future research. Finally, we conclude with the closing remarks on the interaction of top-down predictive and bottom-up auditory processes in spoken language comprehension and the status of the thesis in the research domain of predictive language processing. 8.1 Overview of the main findings A prominent view in the field of language science is that readers and listeners use context information to form linguistic predictions (for reviews, Kuperberg &amp; Jaeger, 2016), which is akin to the view that humans are prediction machines constantly generating predictions about upcoming events (A. Clark, 2013). Studies have shown that it is not always the case in language processing, to the extent that some even question the necessity of prediction (e.g., Huettig &amp; Mani, 2016). For example, differences in age and literacy have been shown to limit listeners’ ability to form linguistic predictions (e.g., Federmeier et al., 2010; Mishra et al., 2012; Sheldon et al., 2008b). Chapter 5 showed that prediction does not always occur in language comprehension. While listening to degraded speech, when listeners attended only to the sentence-final target word but not the preceding context, the facilitatory effect of predictability was absent. However, when the sentence context was attended to, it facilitated the comprehension of the degraded speech. These results showed that attention (or lack thereof) can limit the predictability effect in degraded speech comprehension: no attention, no prediction. We also replicated prior findings that when the bottom-up input is less reliable due to degradation, listeners rely more on the lexical-semantic cues (e.g., Obleser &amp; Kotz, 2010). In Chapter 6, we further examined the effect of predictability and its nature (all-or-nothing vs graded). Since we already showed in Chapter 5 that attention to the context is necessary for the contextual facilitation, our instruction did not restrict participants’ attention to only the target word (and away from the context) in the studies reported in Chapter 6. Participants, thus, attended to the context and formed its meaning representation. This chapter revealed that predictability facilitates language comprehension in a graded manner when the speech is moderately degraded at 4-channel noise-vocoding. At the extremes, listeners either did not utilise the context (unintelligible at 1-channel noise-vocoding), or the context and target word were clearly intelligible (least degradation at 8-channel noise-vocoding) — in the latter case, listeners identified the target word based on the bottom-up information rather than the context. That is, listeners processed the language rationally at different levels of speech degradation (see Ryskin et al., 2018 for a similar argument). These findings also refute the claim of the narrowed expectations framework proposed by Strauß et al. (2013). Contrary to their claim that predictions are made only for highly predictable sentence endings, we found that listeners predict target words across a wide range of semantic space, including the sentence endings in the low and medium predictability sentences. At this point, we note and clarify the seemingly discrepant findings of Chapters 5 and 6. We found a graded effect of predictability in Chapter 6 but not in Chapter 5. The primary reason for this discrepancy is the difference in the metric of language comprehension used in the studies in these two chapters. In Chapter 5, only the identification of the sentence-final target word was considered. On the other hand, in Chapter 6, context identification was also considered prior to identifying the target word. There was no inherent qualitative difference in the data in these two experiments: the data for one group of participants in Chapter 6 (in the randomised design) was taken from the group of participants instructed to report the entire sentence in Chapter 5. Hence, on closer inspection, the findings from Chapter 6 and Chapter 5 support our general conclusions rather than contradict each other. After we showed in Chapter 6 that predictability effects are observed at the moderate degradation level, our goal in Chapter 7 was to examine if a change in speech rate at 4-channel noise-vocoding further increases or decreases the predictability effect. In two experiments, we manipulated the bottom-up processes by changing the speech rates: We compared the contextual facilitation at the moderate degradation level in normal and fast speech rates, then in normal and slow speech rates. The experiments presented in that chapter showed that slow speech does not amplify the contextual facilitation that is observed in normal the speech rate. Listeners already performed at their optimal level in normal speech rate; slowing down the speech rate did not necessarily benefit the contextual facilitation. On the contrary, fast speech impaired the processing of low predictability sentences. These findings showed that with a restricted time in the processing of fast speech, lexical access and activation of target words in less constraining sentence contexts are difficult. Chapters 5 and 6 also showed that regardless of (un)certainty about the quality of subsequent trials, listeners do not adapt to degraded speech across all levels of speech degradation; their performance do not improve over the course of the experiment. We reasoned that a constant trial-by-trial variability in the higher-level feature of the speech stimuli (e.g., predictability) interferes with the perceptual retuning of the auditory system to the sensory properties of the speech stimuli. Hence, the identification of words did not improve throughout the experiment. In Chapters 6 and 7, we argued for a new approach to calculate response accuracy that takes into account a listener’s context identification instead of other word recognition scores (e.g., the proportion of correctly identified words per sentence) that do not consider whether listeners correctly identified the context. In our analyses, we included only those trials in which the context-evoking words were identified correctly. Taken together, the conclusion of the empirical findings from this thesis can be summarised as follows: When a listener attends to a sentence context, semantic predictability facilitates language comprehension at a moderate level of spectral degradation in a graded manner as opposed to being an all-or-nothing phenomenon. Such contextual facilitation is optimal at a normal speech rate, which is not necessarily amplified by slowing down the speech. However, increasing the speech rate reduces contextual facilitation by restricting lexical access in the less predictable sentence endings. 8.2 Implications of the findings 8.2.1 Probabilistic prediction in a noisy channel The studies in this thesis were based on the theoretical accounts of predictive language processing and the noisy channel model of communication. Speech degradation by noise-vocoding created a noisy communication channel that interfered with a listener’s perception and understanding of a speaker’s utterance (schematically represented in Figure 1.1 and formalised in Equation (1.2) in Chapter 1). The results of the experiments presented in this thesis suggest that listeners are rational comprehenders: They weigh the top-down and bottom-up processes to comprehend a degraded speech. When the speech signal is degraded, and listeners have difficulty understanding an utterance, they rely more on the context (e.g., 4-channel noise-vocoded speech in Chapter 6). On the contrary, when this bottom-up information is reliable, i.e., when the speech signal is least degraded, and listeners understand an utterance, they do not necessarily rely on the top-down information. Regardless of their predictions from the context, the listeners’ comprehension results from what they actually perceive (e.g., 8-channel noise-vocoded speech in Chapter 6). Under this rational account, the present thesis showed that listeners predict an upcoming word based on its probability of occurrence in a given context. To our knowledge, the empirical evidence presented in this thesis is the only one since Strauß et al. (2013) to examine the nature of the predictability effect in degraded speech comprehension. We did not find prediction to be restrictive or deterministic, as reported in Strauß et al. (2013) in degraded speech comprehension. Instead, our findings are in line with the probabilistic accounts of predictive language processing (DeLong et al., 2005; Kuperberg &amp; Jaeger, 2016; cf. Nieuwland et al., 2018) proposed in the literature on speech perception and reading studies. 8.2.2 Attention and prediction Chapter 5 showed that attention and prediction can operate independently (i.e., they are two separate processes) in the predictive processing of degraded speech (see also Astheimer &amp; Sanders, 2011; Li et al., 2017). When we restricted listeners’ attention to the contextually predicted target word, the facilitatory effect of predictability was absent. Only attention to the context provided contextual facilitation. This role of attention is not fully addressed in the current frameworks of predictive language processing. In their good enough processing framework, Ferreira &amp; Lowder (2016) speculate that a comprehender can focus and process one part of a sentence “deeper”, and other parts can be ignored or processed at a “shallow” level. However, their proposal is unclear on how attention to different parts of a speech stream can be strategically allocated and how it moderates contextual facilitation. Another influential account of predictive language processing proposed by Pickering &amp; Gambi (2018) states that prediction involves speech production mechanism, i.e., listeners simulate the production of the perceived speech to predict upcoming linguistic units. It is supposed that listeners ideally have access to all the stages of lexical processing. However, their account does not include the role of attention at any stage of lexical processing in speech production, comprehension, and prediction. In an fMRI study of visual perception, Kok et al. (2012) found a significant difference in the amplitude of the BOLD signal between predicted and unpredicted stimuli location in the visual field even when the participants did not attend to the stimuli. This relationship between prediction and attention in visual perception is accounted for in predictive coding models (e.g., Friston, 2009). We argue that an elaborate and explanatory theory of predictive language processing should also consider attention regulation that modulates the predictability effects in language comprehension. 8.2.3 Speech rate We showed that sentence context facilitates comprehension of degraded speech presented at the normal speed. Contrary to common wisdom and previous findings (e.g., Dambacher et al., 2012; Wlotko &amp; Federmeier, 2015), slowing the speech rate did not improve contextual facilitation (cf. Winn &amp; Teece, 2021). Our results from younger adults showed no benefit of slow speech either. Participants in our study were neither able to better understand the degraded speech at a slow speed nor did the slow speech help to better process the context and facilitate comprehension compared to the speech presented at a normal rate. In contrast, fast speech selectively impaired the processing of low predictability sentences. The activation of target words in low constraining sentences was difficult. These findings have some practical implications. The speech that people with sensory neural hearing loss and cochlear implants perceive is spectrally degraded (Parida &amp; Heinz, 2022; R. V. Shannon et al., 1998; R. V. Shannon et al., 2004). The findings from our study can inform the clinical and rehabilitative setup: We do not find evidence that speaking slow benefits degraded speech comprehension. At the same time, we provide evidence that speaking fast is detrimental to processing sentences that are not easily predictable from the context. Therefore, in a rehabilitative setup, auditory-verbal training for the cochlear implantees can likely benefit from a normal speech rate than the exaggerated slowing down of the speech. Other studies have also shown that listeners do not prefer slow speech (e.g., Sutton et al., 1995; cf. Winn &amp; Teece, 2021). On the scientific aspect, our findings inform the theories of predictive language processing, which take into account the speed of lexical processing. For example, Pickering &amp; Gambi (2018)’s account of predictive language processing posits that comprehenders have access to word-form at the late stage of lexical processing. Our findings indicate that at a fast speech rate when the speech is degraded, processing of the less predictable words does not reach the late stage in contrast to the highly predictable words. Replication and extension of these findings measuring the time-course of lexical processing at different speech rates can further test the predictions of these accounts of language processing. 8.3 Limitations and outlook on future studies 8.3.1 Statistical power and sample size The statistical inference made in the studies presented in this thesis stem from the interpretation of statistical significance (or lack thereof) of the effect size of interest primarily in terms of beta estimates from the mixed effects model. There was no a priori expectation about the estimated effect size of the main effect of predictability, the main effect of noise-vocoding channels, or their interactions which could be used to determine the sample size necessary to detect those effects (see Meteyard &amp; Davies, 2020 for the discussion on the complexity of power analysis in mixed model designs). Therefore, we determined our sample size to be approximately equal to that of other studies that examined similar phenomenon like ours, which was language comprehension in different types of adverse listening conditions (e.g., Erb et al., 2013; Hunter &amp; Pisoni, 2018; Obleser et al., 2007; Obleser &amp; Kotz, 2010; Sheldon et al., 2008a, 2008b; Sommers et al., 2020; Strauß et al., 2013). Although, in hindsight, a priori power analysis and sample size determination seem to be a more robust approach, the current sample size determination method left a possibility of our experiments being underpowered12. Given this possibility, we follow the advice of the statisticians: avoid making strong claims and “accept uncertainty” (McShane et al., 2019; Vasishth &amp; Gelman, 2021). Statistically significant findings from a potentially underpowered experiment are not inherently wrong, as long as the claims are not big (see Vasishth &amp; Gelman, 2021 for discussion). Our findings are suggestive, and provide directions for future research. 8.3.2 Measurement of predictability Our interpretation of the experimental results to support the probabilistic and graded nature of predictability is that listeners form expectations about an upcoming word based on the likelihood of its occurrence given the context. The “likelihood of occurrence” was measured with the cloze probability of the target word in a sentence, which is widely used in sentence comprehension studies (see Staub et al., 2015 for discussion). However, some argue that it is not the best metric to measure the predictability of a word (e.g., Smith &amp; Levy, 2011; Verhagen et al., 2018): The cloze probability is an aggregated estimate of whether a group of participants will consider a particular word as a continuation of a sentence given a context. For example, if 40 out of 50 participants in a cloze norming study respond that balloon is the most likely ending of the truncated sentence The child went out to fly a red ___, then “balloon” is considered to be the highly predictable word in this context with the cloze probability value of 0.80. However, it is also likely that another group of 50 participants would respond with “kite” as the most likely ending of the same sentence. This example illustrates that a cloze-based measure is prone to be an inconsistent estimate of predictability, nonetheless, it is still the primary measure of predictability. In recent years, alternatives to cloze-based measures have been proposed. For example, Lopukhina et al. (2021) demonstrated that corpus-based measures of word probability are better predictors of linguistic predictability than the cloze-based measures calculated from a small group of participants in norming studies (see also Michaelov et al., 2022). Similarly, Hofmann et al. (2021) demonstrated that surprisal-based measures estimated from Large Language Models (e.g., GPT-2, GPT-3) explain N400 effects and reading times better than cloze-based measures (see Heilbron et al., 2022 for an implementation of surprisal calculated from GPT-2). However, these alternatives are still being explored and developed. As growing number of studies show that values other than cloze probability are good estimates of predictability, these measures can also be used alongside the established measure like cloze probability. Studies in the domain of probabilistic language processing that we reported in this thesis can benefit from multiple predictability estimates from language models and corpora in addition to the cloze based measures. 8.3.3 Sentence structure and context information One of the goals of the thesis was to replicate the predictability effects in degraded speech comprehension, as shown in studies like Obleser et al. (2007) and Obleser &amp; Kotz (2010). Therefore, we created short Subject–Verb–Object sentences similar to Obleser &amp; Kotz (2010)‘s stimuli. In these sentences, the verb was predictive of the noun. However, in daily conversations, speakers’ utterances are not structured in this short format in which only one preceding word provides a context to predict the next word. Instead, different sources of information, like the knowledge about the speaker, topic, or discourse, build the context information and are jointly predictive of the upcoming linguistic units. Similarly, the speaker indicates important words or concepts via pitch contours, stress, or intonation patterns, which then direct the listener’s attention. Hence, one could argue that the generalisation of the findings beyond the stimuli used here is restricted or limited. Therefore, the next step is to extend these findings using longer utterances in which a discourse provides the context information about an upcoming word in a sentence in the discourse (e.g., Brothers et al., 2015) and the information about the speaker (e.g., Bhandari et al., 2020). 8.3.4 The nature of the predictability effect Strauß et al. (2013) formulated their “narrowed expectations” framework based on an EEG study measuring the latency and amplitude of the N400 component. We did not find support for their claims; instead, we demonstrated in our behavioural experiments that the effect of predictability in degraded speech comprehension is graded in nature. A replication and extension of our findings in an EEG experiment would corroborate our claims. It can be expected that the N400 amplitude of the target word in the low predictability sentences will be larger than in the medium predictability sentences, which in turn, will be larger than in the high predictability sentences. Furthermore, these differences will be significantly larger in the moderately degraded speech (4-channel noise-vocoding) compared to the least degraded speech (8-channel noise-vocoding). We note that the EEG experiment proposed here was initially planned as a part of this thesis. However, due to the closure of the electrophysiology lab during the covid-19 lockdown, the experiment was not conducted. 8.3.5 Individual differences It is evident that language processing is not the same across all participants. For example, working memory capacity, processing speed, literacy, and language experience vary in a group of participants, which results in a difference in language processing and the use of lexical-semantic cues among participants (Federmeier et al., 2010; Mishra et al., 2012; Rommers et al., 2015; Scholman et al., 2020). However, the general conclusions about the top-down–bottom-up interactions presented in this thesis are based on the mean estimates of predictability (in terms of response accuracy) across all participants. Although we controlled for variability among participants in the mixed model analysis, the individual differences measures could inform how the top-down–bottom-up interactions vary among the participants. For example, it can be speculated that participants with faster processing speed benefit more from the semantic context when the speech rate is fast as it was in the experiment in Chapter 7 (e.g., Huettig &amp; Janse, 2016). Therefore, it is recommended that the extension of the studies presented in this thesis include individual differences measures that can moderate linguistic predictions in adverse listening conditions. 8.3.6 Online study with auditory stimuli The experiments presented in this thesis were conducted on the web. This poses a few methodological challenges which in turn can limit the generalisations of the findings. Firstly, the participants listened to the auditory stimuli at their comfort, in their computer using their own headset. This setup is different from a controlled laboratory setup in which all participants use the same device and listen through the same headset that has a standard spectral resolution with a known loudness level. In our experiments, the variability of spectral responses and intensity of the sound presented through the headsets of the participants was unknown13. Similarly, the possiblity that participants used their computer’s loudspeakers instead of headphones could not be entirely ruled out although it has been shown that participants in Prolific are attentive and honest to the instructions (Peer et al., 2022). In future experiments, one could follow Woods et al. (2017)’s proposed headphone test to confirm that stereo headphones are used, not loudspeakers. Similarly, hearing acuity correlates with the effort required to perceive words in adverse listening conditions (Cahana-Amitay et al., 2016; McCoy et al., 2005). Therefore, we recruited only those participants who reported to not have any hearing-related problem by using Prolific’s filter. However, an objective measure of the participants’ hearing acuity could not be obtained (e.g., by audiometric assessment) over the web. Since the participants in our experiments were young adults, the contribution of hearing acuity on listening effort and word recognition is not significant (Benichov et al., 2012; cf. Hunter &amp; Pisoni, 2018). Nonetheless, future studies should consider replicating our findings in a controlled laboratory setup assessing participants’ audiometric hearing threshold and correlating it with their performance in the word recognition task. It is important to note that after decades of the first psycholinguistic experiment with auditory material (discussed in Chapter 3, Section 3.2), the doubt about the validity of online experiments still persists. Cooke &amp; Garcia Lecumberri (2021) proposed that speech recognition experiments can be conducted over the web in the same way as in the lab; they replicated the findings of the speech perception experiments from their lab in online experiments controlling for participant-related differences. They used masked speech, distorted speech, and enhanced speech. Validating online studies using different speech materials, like noise-vocoded speech, background noise, reverberated speech, etc. in different experimental paradigms could help strengthen the conclusions of our findings, and restrict the limitations of online studies. 8.3.7 Generalisation of the findings It follows from the discussion above that the extent to which our findings generalise across different population and experimental setup is limited. The experiments presented in this thesis were conducted among young adults aged 18-31 years. With age, linguistic and world knowledge increases while hearing acuity decreases. Thus, older and younger adults use lexical-semantic and acoustic-phonetic cues differently (e.g., Sheldon et al., 2008a, 2008b). We can speculate that we would also observe an age difference in the facilitatory effect of predictability when the speech is degraded; older adults would likely rely on semantic context more than younger adults (cf. van Os et al., 2021a). However, the restricted age range of the participants does not permit us to generalise the findings of our studies across a wide age range, including older adults. As a next step, it is recommended that the current study be conducted on older adults, considering the effects of cognitive ageing and auditory threshold on the interaction of top-down and bottom-up processes in speech comprehension in adverse listening conditions. 8.4 Concluding remark At the outset of this thesis, we outlined some goals: to replicate the predictability effects in degraded speech comprehension, to investigate whether the nature of the predictability effect is graded, to examine if attention and speech rate limit or moderate contextual facilitation, and if lexical-semantic properties of the speech influence listeners’ adaptation to degraded speech. We have largely achieved these goals. In the experiments presented in Chapters 5, 6, and 7, we have shown that the interaction between top-down and bottom-up processes in the comprehension of degraded speech is dynamic: Probabilistic language prediction is graded in nature at the moderate level of speech degradation while the listeners attend to the context information. It was also revealed that an increase in speech rate is detrimental to processing low predictability sentences. We argued for using a metric of language comprehension that considers listeners’ identification of context information. Despite these findings, this thesis does not answer all the questions about predictive language processing, especially about prediction in adverse listening conditions. The research domain of predictive language processing grapples with the problems like disentangling prediction from integration (e.g., Mantegna et al., 2019) and parallel from sequential prediction (see Gibson &amp; Pearlmutter, 2000 for discussion). These questions are beyond the scope of this thesis to be addressed. It would be an overstatement to claim that our findings are definitive answers to the question of graded vs deterministic prediction in degraded speech comprehension. Nevertheless, the evidence from our experiments lines up with the existing findings supporting the account of graded prediction. We hold a similar position regarding our evidence on the limitations of contextual facilitation (in Chapter 5): Our findings align with the growing body of literature that question the automaticity of predictions (see Huettig &amp; Mani, 2016 for discussion). Based on these, we speculate that although prediction undoubtedly facilitates language comprehension, it is not a ubiquitous process in language comprehension. Listeners can strategically deploy other top-down processes that limit or moderate linguistic predictions. We discussed the limitations of our experiments, widely used methods and metrics (e.g., cloze probability, key word recognition accuracy), and provided avenues for future research (individual differences, replication with EEG experiments, context information in a discourse). This thesis has contributed to a better understanding of spoken language comprehension and lexical-semantic predictions: the dynamic interaction between top-down and bottom-up processes in adverse listening conditions. Advancing the findings of this thesis will undoubtedly inform an elaborate and comprehensive theory of predictive language processing. Bibliography Astheimer, L. B., &amp; Sanders, L. D. (2011). Predictability affects early perceptual processing of word onsets in continuous speech. Neuropsychologia, 49(12), 3512–3516. https://doi.org/10.1016/j.neuropsychologia.2011.08.014 Benichov, J., Cox, L. C., Tun, P. A., &amp; Wingfield, A. (2012). Word recognition within a linguistic context: Effects of age, hearing acuity, verbal ability, and cognitive function. Ear and Hearing, 33(2), 250–256. https://doi.org/10.1097/AUD.0b013e31822f680f Bhandari, P., Prasad, S., &amp; Mishra, R. K. (2020). High proficient bilinguals bring in higher executive control when encountering diverse interlocutors. Journal of Cultural Cognitive Science, 4(2), 201–215. https://doi.org/10.1007/s41809-020-00060-7 Brothers, T., Swaab, T. Y., &amp; Traxler, M. J. (2015). Effects of prediction and contextual support on lexical processing: Prediction takes precedence. Cognition, 136, 135–149. https://doi.org/10.1016/j.cognition.2014.10.017 Cahana-Amitay, D., Spiro, A., Sayers, J. T., Oveis, A. C., Higby, E., Ojo, E. A., Duncan, S., Goral, M., Hyun, J., Albert, M. L., &amp; Obler, L. K. (2016). How older adults use cognition in sentence-final word recognition. Aging, Neuropsychology, and Cognition, 23(4), 418–444. https://doi.org/10.1080/13825585.2015.1111291 Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences, 36(3), 181–204. https://doi.org/10.1017/s0140525x12000477 Cooke, M., &amp; Garcia Lecumberri, M. (2021). Estimating the performance gap between lab and remote speech perception experiment. Acoustical Society of America Journal, 149(4), A111–A111. https://doi.org/10.1121/10.0004674 Dambacher, M., Dimigen, O., Braun, M., Wille, K., Jacobs, A. M., &amp; Kliegl, R. (2012). Stimulus onset asynchrony and the timeline of word recognition: Event-related potentials during sentence reading. Neuropsychologia, 50(8), 1852–1870. https://doi.org/10.1016/j.neuropsychologia.2012.04.011 DeLong, K. A., Urbach, T. P., &amp; Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8), 1117–1121. https://doi.org/10.1038/nn1504 Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Federmeier, K. D., Kutas, M., &amp; Schul, R. (2010). Age-related and individual differences in the use of prediction during language comprehension. Brain and Language, 115(3), 149–161. https://doi.org/10.1016/j.bandl.2010.07.006 Ferreira, F., &amp; Lowder, M. W. (2016). Prediction, information structure, and good-enough language processing (Vol. 65, pp. 217–247). Elsevier Ltd. https://doi.org/10.1016/bs.plm.2016.04.002 Friston, K. J. (2009). The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, 13(7), 293–301. https://doi.org/10.1016/j.tics.2009.04.005 Gibson, E., &amp; Pearlmutter, N. J. (2000). Distinguishing serial and parallel parsing. Journal of Psycholinguistic Research, 29(2), 231–240. https://doi.org/10.1023/A:1005153330168 Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P., &amp; De Lange, F. P. (2022). A hierarchy of linguistic predictions during natural language comprehension. Proceedings of the National Academy of Sciences, 119(32), e2201968119. https://doi.org/10.1073/pnas.2201968119 Hofmann, M. J., Remus, S., Biemann, C., Radach, R., &amp; Kuchinke, L. (2021). Language models explain word reading times better than empirical predictability. Frontiers in Artificial Intelligence, 4. https://doi.org/10.3389/frai.2021.730570 Huettig, F., &amp; Janse, E. (2016). Individual differences in working memory and processing speed predict anticipatory spoken language processing in the visual world. Language, Cognition and Neuroscience, 31(1), 80–93. https://doi.org/10.1080/23273798.2015.1047459 Huettig, F., &amp; Mani, N. (2016). Is prediction necessary to understand language? Probably not. Language, Cognition and Neuroscience, 31(1), 19–31. https://doi.org/10.1080/23273798.2015.1072223 Hunter, C. R., &amp; Pisoni, D. B. (2018). Extrinsic cognitive load impairs spoken word recognition in high-and low-predictability sentences. Ear and Hearing, 39(2), 378–389. https://doi.org/10.1097/AUD.0000000000000493 Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., &amp; De Lange, F. P. (2012). Attention reverses the effect of prediction in silencing sensory signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Li, X., Zhang, Y., Li, L., Zhao, H., &amp; Du, X. (2017). Attention is shaped by semantic level of event-structure during speech comprehension: An electroencephalogram study. Cognitive Neurodynamics, 11(5), 467–481. https://doi.org/10.1007/s11571-017-9442-4 Lopukhina, A., Lopukhin, K., &amp; Laurinavichyute, A. (2021). Morphosyntactic but not lexical corpus-based probabilities can substitute for cloze probabilities in reading experiments. PloS One, 16(1), e0246133. https://doi.org/10.1371/journal.pone.0246133 Mantegna, F., Hintz, F., Ostarek, M., Alday, P. M., &amp; Huettig, F. (2019). Distinguishing integration and prediction accounts of ERP N400 modulations in language processing through experimental design. Neuropsychologia, 134, 107199. https://doi.org/10.1016/j.neuropsychologia.2019.107199 McCoy, S. L., Tun, P. A., Cox, L. C., Colangelo, M., Stewart, R. A., &amp; Wingfield, A. (2005). Hearing loss and perceptual effort: Downstream effects on older adults’ memory for speech. Quarterly Journal of Experimental Psychology Section A: Human Experimental Psychology, 58(1), 22–33. https://doi.org/10.1080/02724980443000151 McShane, B. B., Gal, D., Gelman, A., Robert, C., &amp; Tackett, J. L. (2019). Abandon statistical significance. The American Statistician, 73(sup1), 235–245. https://doi.org/0.1080/00031305.2018.1527253 Meteyard, L., &amp; Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092 Michaelov, J. A., Coulson, S., &amp; Bergen, B. K. (2022). So cloze yet so far: N400 amplitude is better predicted by distributional information than human predictability judgements. IEEE Transactions on Cognitive and Developmental Systems. https://doi.org/10.1109/TCDS.2022.3176783 Mishra, R. K., Singh, N., Pandey, A., &amp; Huettig, F. (2012). Spoken language-mediated anticipatory eye- movements are modulated by reading ability - Evidence from Indian low and high literates. Journal of Eye Movement Research, 5(1), 1–10. https://doi.org/10.16910/jemr.5.1.3 Nieuwland, M. S., Politzer-Ahles, S., Heyselaar, E., Segaert, K., Darley, E., Kazanina, N., Von Grebmer Zu Wolfsthurn, S., Bartolozzi, F., Kogan, V., Ito, A., Mézière, D., Barr, D. J., Rousselet, G. A., Ferguson, H. J., Busch-Moreno, S., Fu, X., Tuomainen, J., Kulakova, E., Husband, E. M., … Huettig, F. (2018). Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. eLife, 7, 1–24. https://doi.org/10.7554/elife.33468 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Parida, S., &amp; Heinz, M. G. (2022). Underlying neural mechanisms of degraded speech intelligibility following noise-induced hearing loss: The importance of distorted tonotopy: Neural mechanisms of degraded speech intelligibility. Hearing Research, 108586. https://doi.org/10.1016/j.heares.2022.108586 Peer, E., Rothschild, D., Gordon, A., Evernden, Z., &amp; Damer, E. (2022). Data quality of platforms and panels for online behavioral research. Behavior Research Methods, 54(4), 1643–1662. https://doi.org/10.3758/s13428-021-01694-3 Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Rommers, J., Meyer, A. S., &amp; Huettig, F. (2015). Verbal and nonverbal predictors of language-mediated anticipatory eye movements. Attention, Perception, &amp; Psychophysics, 77(3), 720–730. https://doi.org/0.3758/s13414-015-0873-x Ryskin, R., Futrell, R., Kiran, S., &amp; Gibson, E. (2018). Comprehenders model the nature of noise in the environment. Cognition, 181(July 2017), 141–150. https://doi.org/10.1016/j.cognition.2018.08.018 Scholman, M. C., Demberg, V., &amp; Sanders, T. J. (2020). Individual differences in expecting coherence relations: Exploring the variability in sensitivity to contextual signals in discourse. Discourse Processes, 57(10), 844–861. https://doi.org/10.1080/0163853X.2020.1813492 Shannon, R. V., Fu, Q.-J., &amp; Galvin Iii, J. (2004). The number of spectral channels required for speech recognition depends on the difficulty of the listening situation. Acta Oto-Laryngologica, 124(0), 50–54. https://doi.org/10.1080/03655230410017562 Shannon, R. V., Zeng, V., &amp; Wygonski, J. (1998). Speech recognition with altered spectral distribution of envelope cues. The Journal of the Acoustical Society of America, 104(4), 2467–2476. https://doi.org/10.1121/1.423774 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008a). Priming and sentence context support listening to noise-vocoded speech by younger and older adults. The Journal of the Acoustical Society of America, 123(1), 489–499. https://doi.org/10.1121/1.2783762 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008b). Effect of age, presentation method, and learning on identification of noise-vocoded words. The Journal of the Acoustical Society of America, 123(1), 476–488. https://doi.org/10.1121/1.2805676 Smith, N. J., &amp; Levy, R. (2011). Cloze but no cigar: The complex relationship between cloze, corpus, and subjective probabilities in language processing. Proceedings of the 33rd Annual Conference of the Cognitive Science Society, 33. Sommers, M. S., Spehar, B., Tye-Murray, N., Myerson, J., &amp; Hale, S. (2020). Age differences in the effects of speaking rate on auditory, visual, and auditory-visual speech perception. Ear and Hearing, 41(3), 549–560. https://doi.org/10.1097/AUD.0000000000000776 Staub, A., Grant, M., Astheimer, L., &amp; Cohen, A. (2015). The influence of cloze probability and item constraint on cloze task response time. Journal of Memory and Language, 82, 1–17. https://doi.org/10.1016/j.jml.2015.02.004 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 Sutton, B., King, J., Hux, K., &amp; Beukelman, D. (1995). Younger and older adults’ rate performance when listening to synthetic speech. Augmentative and Alternative Communication, 11(3), 147–153. https://doi.org/10.1080/07434619512331277269 van Os, M., Kray, J., &amp; Demberg, V. (2021a). Mishearing as a side effect of rational language comprehension in noise. Frontiers in Psychology, 12. https://doi.org/10.3389/fpsyg.2021.679278 Vasishth, S., &amp; Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051 Verhagen, V., Mos, M., Backus, A., &amp; Schilperoord, J. (2018). Predictive language processing revealing usage-based variation. Language and Cognition, 10(2), 329–373. https://doi.org/10.1017/langcog.2018.4 Winn, M. B., &amp; Teece, K. H. (2021). Slower speaking rate reduces listening effort among listeners with cochlear implants. Ear and Hearing, 42(3), 584. https://doi.org/10.1097/aud.0000000000000958 Wlotko, E. W., &amp; Federmeier, K. D. (2015). Time for prediction? The effect of presentation rate on predictive sentence comprehension during word-by-word reading. Cortex, 68, 20–32. https://doi.org/10.1016/j.cortex.2015.03.014 Woods, K. J. P., Siegel, M. H., Traer, J., &amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, and Psychophysics, 79(7), 2064–2072. https://doi.org/10.3758/s13414-017-1361-2 However, note that in the experiments in Chapter 7, we collected data from a large group of participants (n=101 in each experiment) compared to relatively smaller sample size in the preceding Chapters 5 (n=50 in Experiment 1 and n=48 in Experiment 2) and 6 (n=50 in the predictable channel context and n=48 in the unpredictable channel context).↩︎ The type of device that they used may differ among the participants in each experimental group in Chapters 5, 6, and 7. Nonetheless, it is unlikely that there was any systematic difference between the participants in two groups that were compared in each experiment. The participants in both the groups were recruited following the same procedure in the same recruitment platform from the same geographical region. The exclusion criteria was also identical, and the age range of the participants in both groups was also similar.↩︎ "],["appendix-A.html", "A Experimental items", " A Experimental items High, Medium, and Low predictability sentences were used in Chapters 5 and 6. In Chapter 7, only High and Low predictability sentences were used. Sentences with cloze probability of the target word (noun) and the source of the sentences as mentioned in Chapter 5 sn predictability sentence cloze probability of the noun sources 1 High Er loest die Aufgabe 0.6 S 2 High Sie verweigert die Aussage 0.57 O 3 High Er verschrottet das Auto 0.84 O 4 High Er betankt das Auto 0.9 S 5 High Er blitzt den Autofahrer 0.6 S 6 High Er fliest das Bad 0.88 O 7 High Sie jongliert die Baelle 0.85 S 8 High Er dribbelt den Ball 0.95 S 9 High Er ueberfaellt die Bank 0.85 S 10 High Er faellt den Baum 0.69 O 11 High Sie bepflanzt das Beet 0.6 O 12 High Er schient das Bein 1 O 13 High Er erklimmt den Berg 0.6 S 14 High Sie bezieht das Bett 0.64 O 15 High Er zapft das Bier 0.95 S 16 High Sie rahmt das Bild 0.9 S 17 High Sie faltet das Blatt 0.65 S 18 High Sie spitzt den Bleistift 0.9 S 19 High Sie giesst die Blumen 0.7 S 20 High Sie wischt den Boden 0.8 S 21 High Er lutscht das Bonbon 0.68 O 22 High Sie toastet das Brot 0.88 O 23 High Sie liest das Buch 0.68 O 24 High Sie kloeppelt das Deckchen 0.6 O 25 High Sie schleckt das Eis 0.92 O 26 High Sie errechnet das Ergebnis 0.56 O 27 High Er wuerzt das Essen 0.6 O 28 High Er tappt in die Falle 0.8 S 29 High Er berichtigt den Fehler 0.7 S 30 High Er pfluegt das Feld 0.8 S 31 High Sie putzt das Fenster 0.64 O 32 High Er veranstaltet das Fest 0.56 O 33 High Er entfacht das Feuer 0.95 S 34 High Er hisst die Flagge 0.85 S 35 High Es lodern die Flammen 0.9 S 36 High Er fliegt das Flugzeug 0.8 O 37 High Sie posiert fŸr das Foto 0.75 S 38 High Sie dichtet das Gedicht 0.6 O 39 High Er verraet das Geheimnis 0.8 S 40 High Sie zaehlt das Geld 0.8 S 41 High Sie duenstet das Gemuese 0.75 S 42 High Sie erzaehlt die Geschichte 0.81 O 43 High Sie spuelt das Geschirr 0.92 O 44 High Sie stemmt das Gewicht 0.7 S 45 High Er zerbricht das Glas 0.72 O 46 High Er maeht das Gras 0.88 O 47 High Sie pachtet das Grundstueck 0.65 O 48 High Sie buerstet die Haare 0.75 S 49 High Es kraeht der Hahn 0.8 S 50 High Sie saniert das Haus 0.66 O 51 High Er baut das Haus 0.88 O 52 High Sie hobelt das Holz 0.76 O 53 High Sie Ÿbernachtet im Hotel 0.7 S 54 High Er rupft das Huhn 0.8 S 55 High Er fuettert den Hund 0.65 S 56 High Sie pellt die Kartoffeln 0.6 S 57 High Sie erzieht das Kind 0.8 S 58 High Er ermahnt das Kind 0.85 S 59 High Sie gebŠrt das Kind 1 S 60 High Sie naeht das Kleid 0.8 O 61 High Sie beugt die Knie 0.6 S 62 High Sie backt den Kuchen 0.95 S 63 High Sie melkt die Kuh 0.95 S 64 High Sie beraet den Kunden 0.56 O 65 High Sie bereist das Land 0.76 O 66 High Er erkundet die Landschaft 0.75 S 67 High Er klagt das Leid 0.56 O 68 High Sie komponiert das Lied 0.95 S 69 High Er bohrt das Loch 0.8 S 70 High Er baendigt den Loewen 0.6 S 71 High Sie gefaellt dem Mann 0.6 O 72 High Er verschreibt das Medikament 0.65 S 73 High Sie siebt das Mehl 0.76 O 74 High Er summt die Melodie 0.8 S 75 High Er schaerft das Messer 0.84 O 76 High Er entsorgt den Muell 0.95 S 77 High Sie schnaeuzt die Nase 0.9 S 78 High Er heizt den Ofen 0.8 S 79 High Er dirigiert das Orchester 0.6 O 80 High Sie verschickt das Paket 0.7 S 81 High Er reitet das Pferd 1 O 82 High Er besteht die Pruefung 0.67 O 83 High Sie strickt den Pullover 0.61 O 84 High Er bezahlt die Rechnung 0.65 S 85 High Sie massiert den Ruecken 0.7 S 86 High Sie verstreut das Salz 0.6 O 87 High Er kapert das Schiff 0.76 O 88 High Er schmilzt den Schnee 0.75 S 89 High Es rieselt der Schnee 0.85 S 90 High Sie paniert das Schnitzel 0.9 S 91 High Er besohlt den Schuh 0.9 S 92 High Er schlachtet das Schwein 0.76 O 93 High Sie gewinnt das Spiel 0.6 O 94 High Sie mistet den Stall aus 0.75 S 95 High Sie bewirbt sich auf die Stelle 0.75 S 96 High Sie Ÿberquert die Stra§e 0.6 S 97 High Sie schlichtet den Streit 0.93 O 98 High Er rueckt den Stuhl 0.61 O 99 High Sie loeffelt die Suppe 0.59 O 100 High Er knetet den Teig 0.9 S 101 High Er ueberschreitet das Tempo 0.6 S 102 High Sie verschiebt den Termin 0.65 O 103 High Sie schreinert den Tisch 0.7 S 104 High Sie deckt den Tisch 0.9 S 105 High Er schliesst die Tuer 0.61 O 106 High Er verriegelt die Tuer 0.95 S 107 High Sie jaetet das Unkraut 0.68 O 108 High Er vereitelt das Verbrechen 0.6 S 109 High Er unterzeichnet den Vertrag 0.9 S 110 High Es zwitschern die Voegel 1 S 111 High Sie filtert das Wasser 0.64 O 112 High Er geht den Weg 0.86 O 113 High Er geniesst das Wetter 0.6 S 114 High Er erlegt das Wild 0.6 O 115 High Sie buchstabiert das Wort 0.76 O 116 High Er verarztet die Wunde 0.6 S 117 High Er erreicht das Ziel 0.69 O 118 High Sie raucht die Zigarette 0.9 S 119 High Er tapeziert das Zimmer 0.64 O 120 High Er verpasst den Zug 0.56 O 121 Medium Er erledigt die Aufgabe 0.35 S 122 Medium Sie vergisst die Aussage 0.23 O 123 Medium Er verkauft das Auto 0.3 S 124 Medium Er faehrt das Auto 0.54 O 125 Medium Er ueberholt den Autofahrer 0.15 S 126 Medium Er aendert das Bad 0.12 O 127 Medium Sie rollt die Baelle 0.15 S 128 Medium Er faengt den Ball 0.35 S 129 Medium Er beraubt die Bank 0.35 S 130 Medium Er stutzt den Baum 0.35 S 131 Medium Sie bearbeitet das Beet 0.12 O 132 Medium Er untersucht das Bein 0.24 O 133 Medium Er besteigt den Berg 0.55 S 134 Medium Sie beschreibt das Bett 0.16 O 135 Medium Er sieht das Bier 0.16 O 136 Medium Sie starrt auf das Bild 0.21 S 137 Medium Sie knickt das Blatt 0.5 S 138 Medium Sie zerkaut den Bleistift 0.15 S 139 Medium Sie pflanzt die Blumen 0.5 S 140 Medium Sie saugt den Boden 0.5 S 141 Medium Er kauft das Bonbon 0.24 O 142 Medium Sie verteilt das Brot 0.16 O 143 Medium Sie leiht das Buch 0.4 S 144 Medium Sie bemerkt das Deckchen 0.12 O 145 Medium Sie verwendet das Eis 0.16 O 146 Medium Sie korrigiert das Ergebnis 0.5 S 147 Medium Er kostet das Essen 0.52 O 148 Medium Er sitzt in der Falle 0.15 S 149 Medium Er vertuscht den Fehler 0.3 S 150 Medium Er besaet das Feld 0.52 O 151 Medium Sie verglast das Fenster 0.5 S 152 Medium Er organisiert das Fest 0.2 O 153 Medium Er erwaehnt das Feuer 0.16 O 154 Medium Er schwenkt die Flagge 0.25 S 155 Medium Es leuchten die Flammen 0.25 S 156 Medium Er hoert das Flugzeug 0.16 O 157 Medium Sie entwickelt das Foto 0.3 S 158 Medium Sie verfasst das Gedicht 0.15 S 159 Medium Er hŸtet das Geheimnis 0.45 S 160 Medium Sie versteckt das Geld 0.15 S 161 Medium Sie kocht das Gemuese 0.15 S 162 Medium Sie schreibt die Geschichte 0.1 S 163 Medium Sie waescht das Geschirr ab 0.4 S 164 Medium Sie schaetzt das Gewicht 0.4 S 165 Medium Er oeffnet das Glas 0.24 O 166 Medium Er kaut das Gras 0.2 O 167 Medium Sie bebaut das Grundstueck 0.3 S 168 Medium Sie schneidet die Haare 0.25 S 169 Medium Es stolziert der Hahn 0.2 S 170 Medium Sie erwirbt das Haus 0.16 O 171 Medium Er beantragt das Haus 0.2 O 172 Medium Sie hackt das Holz 0.2 S 173 Medium Sie bucht das Hotel 0.3 S 174 Medium Er schlachtet das Huhn 0.15 S 175 Medium Er krault den Hund 0.45 S 176 Medium Sie erntet die Kartoffeln 0.15 S 177 Medium Sie schleppt das Kind 0.15 S 178 Medium Er beaufsichtigt das Kind 0.4 S 179 Medium Sie ruft das Kind 0.5 S 180 Medium Sie waescht das Kleid 0.24 O 181 Medium Sie verletzt sich das Knie 0.55 S 182 Medium Sie verziert den Kuchen 0.4 S 183 Medium Sie besamt die Kuh 0.1 S 184 Medium Sie beliefert den Kunden 0.25 S 185 Medium Sie befaehrt das Land 0.24 O 186 Medium Er malt die Landschaft 0.25 S 187 Medium Er ertraegt das Leid 0.2 S 188 Medium Sie produziert das Lied 0.16 O 189 Medium Er stopft das Loch 0.45 S 190 Medium Er zaehmt den Loewen 0.2 S 191 Medium Sie operiert den Mann 0.2 S 192 Medium Er schluckt das Medikament 0.3 S 193 Medium Sie wiegt das Mehl 0.16 O 194 Medium Er pfeift die Melodie 0.3 S 195 Medium Er beachtet das Messer 0.24 O 196 Medium Er sortiert den Muell 0.35 S 197 Medium Sie pudert die Nase 0.55 O 198 Medium Er kachelt den Ofen 0.3 S 199 Medium Er leitet das Orchester 0.12 O 200 Medium Sie liefert das Paket 0.25 S 201 Medium Er pflegt das Pferd 0.16 O 202 Medium Er versaeumt die Pruefung 0.15 S 203 Medium Sie buegelt den Pullover 0.3 S 204 Medium Er begleicht die Rechnung 0.5 S 205 Medium Sie kruemmt den Ruecken 0.25 S 206 Medium Sie schmeckt das Salz 0.15 S 207 Medium Er erwartet das Schiff 0.12 O 208 Medium Er schippt den Schnee 0.35 S 209 Medium Es faellt der Schnee 0.25 S 210 Medium Sie braet das Schnitzel 0.5 S 211 Medium Er bindet den Schuh 0.5 S 212 Medium Er bewacht das Schwein 0.16 O 213 Medium Sie kennt das Spiel 0.12 O 214 Medium Sie behuetet den Stall 0.1 S 215 Medium Sie kuendigt die Stelle 0.25 S 216 Medium Sie sperrt die Stra§e 0.25 S 217 Medium Sie vermeidet den Streit 0.2 S 218 Medium Er zimmert den Stuhl 0.15 S 219 Medium Sie bringt die Suppe 0.14 O 220 Medium Er ruehrt den Teig 0.5 S 221 Medium Er verlangsamt das Tempo 0.35 S 222 Medium Sie vergisst den Termin 0.25 S 223 Medium Sie poliert den Tisch 0.15 S 224 Medium Sie verrueckt den Tisch 0.25 S 225 Medium Er repariert die Tuer 0.1 S 226 Medium Er reinigt die Tuer 0.21 O 227 Medium Sie rupft das Unkraut 0.16 O 228 Medium Er leugnet das Verbrechen 0.35 S 229 Medium Er praesentiert den Vertrag 0.17 O 230 Medium Es flattern die Voegel 0.5 S 231 Medium Sie reicht das Wasser 0.16 O 232 Medium Er beschreitet den Weg 0.35 S 233 Medium Er trotzt dem Wetter 0.45 S 234 Medium Er riecht das Wild 0.14 O 235 Medium Sie uebersetzt das Wort 0.4 S 236 Medium Er desinfiziert die Wunde 0.2 S 237 Medium Er verfehlt das Ziel 0.5 S 238 Medium Sie dreht die Zigarette 0.1 S 239 Medium Er streicht das Zimmer 0.3 S 240 Medium Er rangiert den Zug 0.4 S 241 Low Er vernachlaessigt die Aufgabe 0 S 242 Low Sie kontrolliert die Aussage 0 S 243 Low Er kriegt das Auto 0.08 O 244 Low Er entrostet das Auto 0.09 O 245 Low Er fragt den Autofahrer 0 S 246 Low Er durchsucht das Bad 0 S 247 Low Sie findet die Baelle 0 S 248 Low Er kauft den Ball 0 S 249 Low Er besucht die Bank 0 S 250 Low Er verletzt den Baum 0.04 O 251 Low Sie lockert das Beet 0 S 252 Low Er testet das Bein 0 S 253 Low Er zeichnet den Berg 0 S 254 Low Sie beschmutzt das Bett 0 S 255 Low Er bringt das Bier 0.05 S 256 Low Sie sieht das Bild 0 S 257 Low Sie zerreisst das Blatt 0.07 O 258 Low Sie verschenkt den Bleistift 0 S 259 Low Sie bestaunt die Blumen 0.05 S 260 Low Sie begutachtet den Boden 0.05 S 261 Low Er šffnet das Bonbon 0 S 262 Low Sie fertigt das Brot 0.05 S 263 Low Sie verleiht das Buch 0.06 O 264 Low Sie verliert das Deckchen 0 S 265 Low Sie haelt das Eis 0 S 266 Low Sie verbessert das Ergebnis 0.03 O 267 Low Er macht das Essen 0.04 O 268 Low Er konstruiert die Falle 0 S 269 Low Er beginnt den Fehler 0.05 S 270 Low Er misst das Feld 0.03 O 271 Low Sie beruecksichtigt das Fenster 0.08 O 272 Low Er bejubelt das Fest 0.05 S 273 Low Er zerstoert das Feuer 0 S 274 Low Er rollt die Flagge ein 0 S 275 Low Es stehen die Flammen 0 S 276 Low Er bewegt das Flugzeug 0 S 277 Low Sie druckt das Foto 0 S 278 Low Sie kann das Gedicht 0.04 O 279 Low Er verdraengt das Geheimnis 0.05 S 280 Low Sie beobachtet das Geld 0 S 281 Low Sie behandelt das Gemuese 0 S 282 Low Sie glaubt die Geschichte 0.04 O 283 Low Sie benoetigt das Geschirr 0 S 284 Low Sie vergleicht das Gewicht 0 S 285 Low Er greift das Glas 0.06 O 286 Low Er klebt das Gras 0 S 287 Low Sie erschliesst das Grundstueck 0.03 O 288 Low Sie sammelt die Haare 0 S 289 Low Es springt der Hahn 0 S 290 Low Sie erkennt das Haus 0 S 291 Low Er bewirft das Haus 0.05 S 292 Low Sie bricht das Holz 0.03 O 293 Low Sie bewertet das Hotel 0 S 294 Low Er teilt das Huhn 0 S 295 Low Er untersucht den Hund 0.05 S 296 Low Sie braucht die Kartoffeln 0 S 297 Low Sie benachrichtigt das Kind 0 S 298 Low Er befragt das Kind 0.04 O 299 Low Sie versetzt das Kind 0.05 S 300 Low Sie bewundert das Kleid 0.05 S 301 Low Sie streichelt das Knie 0 S 302 Low Sie bestellt den Kuchen 0.05 S 303 Low Sie betrauert die Kuh 0 S 304 Low Sie informiert den Kunden 0.03 O 305 Low Sie nennt das Land 0 S 306 Low Er veraendert die Landschaft 0 S 307 Low Er spuert das Leid 0.04 O 308 Low Sie kopiert das Lied 0 S 309 Low Er sucht das Loch 0 S 310 Low Er verlaesst den Loewen 0 S 311 Low Sie verpflichtet den Mann 0.09 O 312 Low Er bezahlt das Medikament 0 S 313 Low Sie pustet das Mehl 0 S 314 Low Er spielt die Melodie 0 S 315 Low Er verraeumt das Messer 0 S 316 Low Er holt den Muell 0 S 317 Low Sie bewegt die Nase 0.03 O 318 Low Er saeubert den Ofen 0 S 319 Low Er beordert das Orchester 0 S 320 Low Sie schliesst das Paket 0 S 321 Low Er sticht das Pferd 0.05 S 322 Low Er versucht die Pruefung 0.03 O 323 Low Sie traegt den Pullover 0.03 O 324 Low Er vergisst die Rechnung 0.05 S 325 Low Sie roentgt den Ruecken 0.05 S 326 Low Sie benutzt das Salz 0.08 O 327 Low Er betritt das Schiff 0 S 328 Low Er nimmt den Schnee 0 S 329 Low Es kommt der Schnee 0 S 330 Low Sie klopft das Schnitzel 0.05 S 331 Low Er stiehlt den Schuh 0 S 332 Low Er beruehrt das Schwein 0 S 333 Low Sie verstaut das Spiel 0 S 334 Low Sie baut den Stall aus 0 S 335 Low Sie entdeckt die Stelle 0 S 336 Low Sie kratzt die Stra§e 0 S 337 Low Sie verantwortet den Streit 0.04 O 338 Low Er verwandelt den Stuhl 0.03 O 339 Low Sie erhitzt die Suppe 0.05 S 340 Low Er zerteilt den Teig 0 S 341 Low Er ignoriert das Tempo 0 S 342 Low Sie bestaetigt den Termin 0.06 O 343 Low Sie hebt den Tisch 0.05 S 344 Low Sie zertruemmert den Tisch 0.05 S 345 Low Er erkennt die Tuer 0.05 S 346 Low Er dreht die Tuer 0 S 347 Low Sie hasst das Unkraut 0 S 348 Low Er filmt das Verbrechen 0 S 349 Low Er liest den Vertrag 0.05 S 350 Low Es fehlen die Voegel 0 S 351 Low Sie liebt das Wasser 0 S 352 Low Er bestimmt den Weg 0.04 O 353 Low Er fuerchtet das Wetter 0.05 S 354 Low Er erschrickt das Wild 0 S 355 Low Sie erlernt das Wort 0.08 O 356 Low Er reinigt die Wunde 0 S 357 Low Er ermittelt das Ziel 0.06 O 358 Low Sie verlangt die Zigarette 0 S 359 Low Er bevorzugt das Zimmer 0.08 O 360 Low Er erwischt den Zug 0.07 O Sources: O=Obleser and Kotz(2010); S=SFB 1102, Project A4 "],["appendix-B.html", "B Forward prediction vs Backward guessing in Chapter 6", " B Forward prediction vs Backward guessing in Chapter 6 Complementary analyses were performed on verb-accuracy in the noun-correct trials to examine the backward guessing effect. The complementary analyses showed that the main effects of target word predictability, channel condition and their interactions were significant. However, subsequent subgroup analyses showed that when listeners identified nouns correctly at 4 channels condition, their response accuracy (of verb recognition) in high predictability sentences was not different than in medium predictability sentences (\\(\\beta\\) = 0.19, SE = 0.29, z(1878) = 0.66, p = .51). There was only a significant difference in accuracy between low predictability and medium predictability sentences (\\(\\beta\\) = 0.56, SE = 0.28, z(1878) = 2.01, p = .04). Compared to the model estimates of accuracy in verb-correct trials (estimates \\(\\beta\\) = 1.14 and \\(\\beta\\) = 1.01), the accuracy for noun-correct trials (\\(\\beta\\) = 0.19 and \\(\\beta\\) = 0.56) were smaller (see Results and Conclusions of Chapter 6). "],["appendix-C.html", "C Forward prediction vs Backward guessing in Chapter 7", " C Forward prediction vs Backward guessing in Chapter 7 Results of the complementary analysis discussed in the Conclusion section of Chapter 7: We performed complementry analyses on noun-correct trials to compare the estimate of backward guessing effect and the forward prediction. The results showed that in both Experiments 1 (fast speech vs normal speech rate) and 2 (slow speech vs normal speech rate), the main effects of target word predictability and speech rate were significant while the interaction of the latter two was not significant (see Tables ?? and ??). These model estimates, i.e., the effect of noun-correct trials (\\(\\beta_{{noun}_{Exp1}}\\) = .73 and \\(\\beta_{{noun}_{Exp2}}\\) = .78) were smaller than the model estimates of verb-correct trials presented in Chapter 7 (\\(\\beta_{{verb}_{Exp1}}\\) = 2.42 and \\(\\beta_{{verb}_{Exp2}}\\) = 2.58). Thus, these findings supported the interpretation of forward-prediction much more strongly than backward-guessing effect. "],["appendix-D.html", "D Data and code", " D Data and code All the code, data, and experimental lists used in the studies presented in this thesis, are available in the following publicly accessible repository: https://osf.io/rtsz3/. "],["appendix-E.html", "E Ethics and funding", " E Ethics and funding Ethics: The studies presented in this thesis involved human subjects. All subjects were recruited following the recommendations of the American Psychological Association. All subjects provided an informed consent in accordance with the Declaration of Helsinki. The ethics committee of the Deutsche Gesellschaft für Sprache (DGfS; EN: German Society for Language Science) provided ethical approval for the experiments conducted. Funding: The research presented in this thesis was funded by the Deutsche Forschungsgemeinschaft (DFG; EN: German Research Foundation) under the research grant SFB1102 (Sonderforschungsbereiche; EN: Collaborative Research Center), Project ID 232722074. "],["bibliography.html", "Bibliography", " Bibliography Aarts, A. A., Anderson, J. E., Anderson, C. J., Attridge, P. R., Attwood, A., Axt, J., Babel, M., Bahník, Š., Baranski, E., Barnett-Cowan, M., Bartmess, E., Beer, J., Bell, R., Bentley, H., Beyan, L., Binion, G., Borsboom, D., Bosch, A., Bosco, F. A., … Zuni, K. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Adank, P., &amp; Janse, E. (2009). Perceptual learning of time-compressed and natural fast speech. The Journal of the Acoustical Society of America, 126(5), 2649–2659. https://doi.org/10.1121/1.3216914 Ahissar, M., Nahum, M., Nelken, I., &amp; Hochstein, S. (2009). Reverse hierarchies and sensory learning. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1515), 285–299. https://doi.org/10.1098/rstb.2008.0253 Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In B. N. Petrov &amp; F. Csaksi (Eds.), Proceedings of the 2nd international symposium on information theory (pp. 267–281). Akademiai Kaido. Altmann, G. T. M., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: restricting the domain of subsequent reference. Cognition, 73(3), 247–264. https://doi.org/10.1016/s0010-0277(99)00059-1 Altmann, G. T. M., &amp; Kamide, Y. (2007). The real-time mediation of visual attention by language and world knowledge: Linking anticipatory (and other) eye movements to linguistic processing. Journal of Memory and Language, 57(4), 502–518. https://doi.org/10.1016/j.jml.2006.12.004 Amichetti, N. M., Atagi, E., Kong, Y.-Y., &amp; Wingfield, A. (2018). Linguistic context versus semantic competition in word recognition by younger and older adults with cochlear implants. Ear &amp; Hearing, 39(1), 101–109. https://doi.org/10.1097/aud.0000000000000469 Ankener, C. S. (2019). The influence of visual information on word predictability and processing effort [Doctoral dissertation]. Saarland University; Saarländische Universitäts-und Landesbibliothek. Ankener, C. S., Sekicki, M., &amp; Staudte, M. (2018). The influence of visual uncertainty on word surprisal and processing effort. Frontiers in Psychology, 9, 2387. https://doi.org/10.3389/fpsyg.2018.02387 Anwyl-Irvine, A. L., Massonnié, J., Flitton, A., Kirkham, N., &amp; Evershed, J. K. (2020). Gorilla in our midst: An online behavioral experiment builder. Behavior Research Methods, 52(1), 388–407. https://doi.org/10.3758/s13428-019-01237-x Anwyl-Irvine, A., Dalmaijer, E. S., Hodges, N., &amp; Evershed, J. K. (2021). Realistic precision and accuracy of online experiment platforms, web browsers, and devices. Behavior Research Methods, 53(4), 1407–1425. https://doi.org/10.3758/s13428-020-01501-5 Astheimer, L. B., &amp; Sanders, L. D. (2011). Predictability affects early perceptual processing of word onsets in continuous speech. Neuropsychologia, 49(12), 3512–3516. https://doi.org/10.1016/j.neuropsychologia.2011.08.014 Aydelott, J., &amp; Bates, E. (2004). Effects of acoustic distortion and semantic context on lexical access. Language and Cognitive Processes, 19(1), 29–56. https://doi.org/10.1080/01690960344000099 Baayen, R. H., Davidson, D. J., &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Bates, D., Kliegl, R., Vasishth, S., &amp; Baayen, H. (2015). Parsimonious Mixed Models. arXiv. https://arxiv.org/abs/1506.04967 Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). https://doi.org/10.18637/jss.v067.i01 Benichov, J., Cox, L. C., Tun, P. A., &amp; Wingfield, A. (2012). Word recognition within a linguistic context: Effects of age, hearing acuity, verbal ability, and cognitive function. Ear and Hearing, 33(2), 250–256. https://doi.org/10.1097/AUD.0b013e31822f680f Berners-Lee, T., Cailliau, R., Groff, J. F., &amp; Pollermann, B. (1992). World-wide web: The information universe. Internet Research, 2(1), 52–58. https://doi.org/10.1108/eb047254 Bhandari, P., Prasad, S., &amp; Mishra, R. K. (2020). High proficient bilinguals bring in higher executive control when encountering diverse interlocutors. Journal of Cultural Cognitive Science, 4(2), 201–215. https://doi.org/10.1007/s41809-020-00060-7 Boersma, P. (2001). Praat, a system for doing phonetics by computer. Glot. Int., 5(9), 341–345. Bolker, B. M., Brooks, M. E., Clark, C. J., Geange, S. W., Poulsen, J. R., Stevens, M. H. H., &amp; White, J. S. S. (2009). Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology and Evolution, 24(3), 127–135. https://doi.org/10.1016/j.tree.2008.10.008 Bondell, H. D., Krishna, A., &amp; Ghosh, S. K. (2010). Joint variable selection for fixed and random effects in linear mixed-effects models. Biometrics, 66(4), 1069–1077. https://doi.org/10.1111/j.1541-0420.2010.01391.x Bowers, J. S., &amp; Davis, C. J. (2012). Bayesian just-so stories in psychology and neuroscience. Psychological Bulletin, 138(3), 389–414. https://doi.org/10.1037/a0026450 Brothers, T., &amp; Kuperberg, G. R. (2021). Word predictability effects are linear, not logarithmic: Implications for probabilistic models of sentence comprehension. Journal of Memory and Language, 116, 104174. https://doi.org/10.1016/j.jml.2020.104174 Brothers, T., Swaab, T. Y., &amp; Traxler, M. J. (2015). Effects of prediction and contextual support on lexical processing: Prediction takes precedence. Cognition, 136, 135–149. https://doi.org/10.1016/j.cognition.2014.10.017 Bruineberg, J., Dolega, K., Dewhurst, J., &amp; Baltieri, M. (2021). The emperor’s new markov blankets. Behavioral and Brain Sciences, 1–63. https://doi.org/10.1017/S0140525X21002351 Brunellière, A., Auran, C., &amp; Delrue, L. (2019). Does the prosodic emphasis of sentential context cause deeper lexical-semantic processing? Language, Cognition and Neuroscience, 34(1), 29–42. https://doi.org/10.1080/23273798.2018.1499945 Cahana-Amitay, D., Spiro, A., Sayers, J. T., Oveis, A. C., Higby, E., Ojo, E. A., Duncan, S., Goral, M., Hyun, J., Albert, M. L., &amp; Obler, L. K. (2016). How older adults use cognition in sentence-final word recognition. Aging, Neuropsychology, and Cognition, 23(4), 418–444. https://doi.org/10.1080/13825585.2015.1111291 Charpentier, F. J., &amp; Stella, M. G. (1986). Diphone synthesis using an overlap-add technique for speech waveforms concatenation. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 2015–2018. https://doi.org/10.1109/icassp.1986.1168657 Chatterjee, S., &amp; Hadi, A. S. (2012). Multiple linear regression. In Regression analysis by example (pp. 57–91). John Wiley &amp; Sons. Chen, F., &amp; Loizou, P. C. (2011). Predicting the intelligibility of vocoded speech. Ear and Hearing, 32(3), 331. https://doi.org/10.1097/AUD.0b013e3181ff3515 Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. Journal of the Acoustical Society of America, 25(5), 975–979. https://doi.org/10.1121/1.1907229 Chingacham, A., Demberg, V., &amp; Klakow, D. (2021). Exploring the potential of lexical paraphrases for mitigating noise-induced comprehension errors. Proc. Interspeech 2021, 1713–1717. https://doi.org/10.21437/Interspeech.2021-306 Christiansen, M. H., &amp; Chater, N. (2015). The Now-or-Never bottleneck: A fundamental constraint on language. Behavioral and Brain Sciences, 39, 1–72. https://doi.org/10.1017/S0140525X1500031X Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences, 36(3), 181–204. https://doi.org/10.1017/s0140525x12000477 Clark, H. H. (1973). The language-as-a-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12(4), 335–359. https://doi.org/10.1016/S0022-5371(73)80014-3 Clendeninn, P. (1940). The vocoder. Nature, 145(3665), 157. https://doi.org/10.1038/145157a0 Cockburn, A., Dragicevic, P., Besançon, L., &amp; Gutwin, C. (2020). Threats of a replication crisis in empirical computer science. Communications of the ACM, 63(8), 70–79. https://doi.org/10.1145/3360311 Cole, A. (2020). The effects of prediction and speech rate on lexical processing (p. 35) [Masters thesis, University of Maryland]. https://doi.org/10.13016/37my-jxp7 Coleman, E. B. (1964). Generalizing to a language population. Psychological Reports, 14(1), 219–226. https://doi.org/10.2466/pr0.1964.14.1.219 Connolly, J. F., Stewart, S., &amp; Phillips, N. (1990). The effects of processing requirements on neurophysiological responses to spoken sentences. Brain and Language, 39(2), 302–318. Cooke, M., &amp; Garcia Lecumberri, M. (2021). Estimating the performance gap between lab and remote speech perception experiment. Acoustical Society of America Journal, 149(4), A111–A111. https://doi.org/10.1121/10.0004674 Cooke, M., Scharenborg, O., &amp; Meyer, B. T. (2022). The time course of adaptation to distorted speech. The Journal of the Acoustical Society of America, 151(4), 2636–2646. https://doi.org/10.1121/10.0010235 Corps, R. E., &amp; Rabagliati, H. (2020). How top-down processing enhances comprehension of noise-vocoded speech: Predictions about meaning are more important than predictions about form. Journal of Memory and Language, 113, 104114. https://doi.org/10.1016/j.jml.2020.104114 Dahan, D., &amp; Magnuson, J. S. (2006). Spoken word recognition. In M. J. Traxler &amp; M. A. Gernsbacher (Eds.), Handbook of psycholinguistics (2nd ed., pp. 249–283). Elsevier. https://doi.org/10.1016/b978-012369374-7/50009-2 Dambacher, M., Dimigen, O., Braun, M., Wille, K., Jacobs, A. M., &amp; Kliegl, R. (2012). Stimulus onset asynchrony and the timeline of word recognition: Event-related potentials during sentence reading. Neuropsychologia, 50(8), 1852–1870. https://doi.org/10.1016/j.neuropsychologia.2012.04.011 Darwiche, A. (2010). Bayesian networks. Communications of the ACM, 53(12), 80–90. https://doi.org/10.1145/1859204.1859227 Darwin, C. (2005). Praat scripts for producing shannon AM speech. http://www.lifesci.sussex.ac.uk/home/Chris_Darwin/Praatscripts/. Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K., &amp; McGettigan, C. (2005). Lexical information drives perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences. Journal of Experimental Psychology: General, 134(2), 222–241. https://doi.org/10.1037/0096-3445.134.2.222 DeLong, K. A., Urbach, T. P., &amp; Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8), 1117–1121. https://doi.org/10.1038/nn1504 Demberg, V., Keller, F., &amp; Koller, A. (2013). Incremental, predictive parsing with psycholinguistically motivated tree-adjoining grammar. Computational Linguistics, 39(4), 1025–1066. Dincer D’Alessandro, H., Boyle, P. J., Ballantyne, D., De Vincentiis, M., &amp; Mancini, P. (2018). The role of speech rate for Italian-speaking cochlear implant users: insights for everyday speech perception. International Journal of Audiology, 57(11), 851–857. https://doi.org/10.1080/14992027.2018.1498139 Dudley, H. (1939). The vocoder. Bell Laboratories Record, 18(4), 122–126. Dupoux, E., &amp; Green, K. (1997). Perceptual adjustment to highly compressed speech: Effects of talker and rate changes. Journal of Experimental Psychology: Human Perception and Performance, 23(3), 914–927. https://doi.org/10.1037/0096-1523.23.3.914 Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, J. A., Conway, J. G., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012 Eckert, M. A., Teubner-Rhodes, S., &amp; Vaden, K. I. (2016). Is Listening in Noise Worth It? The Neurobiology of Speech Recognition in Challenging Listening Conditions. Ear &amp; Hearing, 37(1), 101S–110S. https://doi.org/10.1097/aud.0000000000000300 Ehrlich, S. F., &amp; Rayner, K. (1981). Contextual effects on word perception and eye movements during reading. Journal of Verbal Learning and Verbal Behavior, 20(6), 641–655. https://doi.org/10.1016/S0022-5371(81)90220-6 Ellermeier, W., Kattner, F., Ueda, K., Doumoto, K., &amp; Nakajima, Y. (2015). Memory disruption by irrelevant noise-vocoded speech: Effects of native language and the number of frequency bands. The Journal of the Acoustical Society of America, 138(3), 1561–1569. https://doi.org/10.1121/1.4928954 Erb, J. (2014). The neural dynamics of perceptual adaptation to degraded speech (p. 211) [Doctoral dissertation]. Universität Leipzig. Erb, J., Henry, M. J., Eisner, F., &amp; Obleser, J. (2013). The brain dynamics of rapid perceptual adaptation to adverse listening conditions. Journal of Neuroscience, 33(26), 10688–10697. https://doi.org/10.1523/jneurosci.4596-12.2013 Fairbanks, G., &amp; Kodman Jr., F. (1957). Word intelligibility as a function of time compression. The Journal of the Acoustical Society of America, 29(5), 636–641. Federmeier, K. D. (2007). Thinking ahead: The role and roots of prediction in language comprehension. Psychophysiology, 44(4), 491–505. https://doi.org/10.1111/j.1469-8986.2007.00531.x Federmeier, K. D., Kutas, M., &amp; Schul, R. (2010). Age-related and individual differences in the use of prediction during language comprehension. Brain and Language, 115(3), 149–161. https://doi.org/10.1016/j.bandl.2010.07.006 Federmeier, K. D., Mclennan, D., Ochoa, E. de, &amp; Kutas, M. (2002). The impact of semantic memory organization and sentence context information on spoken language processing by younger and older adults: An ERP study. Psychophysiology, 39(02), 133–146. https://doi.org/10.1111/1469-8986.3920133 Federmeier, K. D., Wlotko, E. W., De Ochoa-Dewald, E., &amp; Kutas, M. (2007a). Multiple effects of sentential constraint on word processing. Brain Research, 1146, 75–84. https://doi.org/10.1016/j.brainres.2006.06.101 Federmeier, K. D., Wlotko, E. W., De Ochoa-Dewald, E., &amp; Kutas, M. (2007b). Multiple effects of sentential constraint on word processing. Brain Research, 1146(1), 75–84. https://doi.org/10.1016/j.brainres.2006.06.101 Fernandez, L. B., Engelhardt, P. E., Patarroyo, A. G., &amp; Allen, S. E. M. (2020). Effects of speech rate on anticipatory eye movements in the visual world paradigm: Evidence from aging, native, and non-native language processing. Quarterly Journal of Experimental Psychology, 73(12), 2348–2361. https://doi.org/10.1177/1747021820948019 Ferreira, F., &amp; Chantavarin, S. (2018). Integration and prediction in language processing: A synthesis of old and new. Current Directions in Psychological Science, 27(6), 443–448. https://doi.org/10.1177/0963721418794491 Ferreira, F., &amp; Clifton Jr, C. (1986). The independence of syntactic processing. Journal of Memory and Language, 25(3), 348–368. https://doi.org/10.1016/0749-596X(86)90006-9 Ferreira, F., &amp; Lowder, M. W. (2016). Prediction, information structure, and good-enough language processing (Vol. 65, pp. 217–247). Elsevier Ltd. https://doi.org/10.1016/bs.plm.2016.04.002 Fontan, L., Tardieu, J., Gaillard, P., Woisard, V., &amp; Ruiz, R. (2015). Relationship between speech intelligibility and speech comprehension in babble noise. Journal of Speech, Language, and Hearing Research, 58(3), 977–986. https://doi.org/10.1044/2015_jslhr-h-13-0335 Forster, K. I. (1981). Priming and the effects of sentence and lexical contexts on naming time: Evidence for autonomous lexical processing. The Quarterly Journal of Experimental Psychology, 33(4), 465–495. https://doi.org/10.1080/14640748108400804 Frank, S. L., Otten, L. J., Galli, G., &amp; Vigliocco, G. (2015). The ERP response to the amount of information conveyed by words in sentences. Brain and Language, 140, 1–11. https://doi.org/10.1016/j.bandl.2014.10.006 Frisson, S., Rayner, K., &amp; Pickering, M. J. (2005). Effects of contextual predictability and transitional probability on eye movements during reading. Journal of Experimental Psychology: Learning Memory and Cognition, 31(5), 862–877. https://doi.org/10.1037/0278-7393.31.5.862 Friston, K. J. (2009). The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, 13(7), 293–301. https://doi.org/10.1016/j.tics.2009.04.005 Friston, K. J., Parr, T., Yufik, Y., Sajid, N., Price, C. J., Holmes, E., &amp; Square, Q. (2020). Generative models, linguistic communication and active inference. Neuroscience &amp; Biobehavioral Reviews, 118, 42–64. https://doi.org/10.1016/j.neubiorev.2020.07.005 Friston, K. J., Sajid, N., Quiroga-Martinez, D. R., Parr, T., Price, C. J., &amp; Holmes, E. (2020). Active listening. Hearing Research, xxxx, 107998. https://doi.org/10.1016/j.heares.2020.107998 Fritz, J. B., Elhilali, M., David, S. V., &amp; Shamma, S. A. (2007). Auditory attention - focusing the searchlight on sound. Current Opinion in Neurobiology, 17(4), 437–455. https://doi.org/10.1016/j.conb.2007.07.011 Gadiraju, U., Möller, S., Nöllenburg, M., Saupe, D., Egger-Lampl, S., Archambault, D., &amp; Fisher, B. (2017). Crowdsourcing versus the laboratory: Towards human-centered experiments using the crowd. In D. Archambault, H. Purchase, &amp; T. Hoßfeld (Eds.), Evaluation in the crowd. Crowdsourcing and human-centered experiments (pp. 6–26). Springer, Cham. https://doi.org/10.1007/978-3-319-66435-4_2 Gagné, N., &amp; Franzen, L. (2021). How to run behavioural experiments online: Best practice suggestions for cognitive psychology and neuroscience. PsyArXiv. https://doi.org/10.31234/osf.io/nt67j Ganong, W. F. (1980). Phonetic categorization in auditory word perception. Journal of Experimental Psychology: Human Perception and Performance, 6(1), 110. https://doi.org/10.1037/0096-1523.6.1.110 Garrido, M. I., Dolan, R. J., &amp; Sahani, M. (2011). Surprise Leads to Noisier Perceptual Decisions. I-Perception, 2(2), 112–120. https://doi.org/10.1068/i0411 Garvey, W. D. (1953). The intelligibility of speeded speech. Journal of Experimental Psychology, 45(2), 102–108. https://doi.org/10.1037/h0054381 Ghitza, O., &amp; Greenberg, S. (2009). On the possible role of brain rhythms in speech perception: Intelligibility of time-compressed speech with periodic and aperiodic insertions of silence. Phonetica, 66(1-2), 113–126. https://doi.org/10.1159/000208934 Gibson, E., Bergen, L., &amp; Piantadosi, S. T. (2013). Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. Proceedings of the National Academy of Sciences, 110(20), 8051–8056. https://doi.org/10.1073/pnas.1216438110 Gibson, E., Futrell, R., Piantadosi, S. P., Dautriche, I., Mahowald, K., Bergen, L., &amp; Levy, R. (2019). How efficiency shapes human language. Trends in Cognitive Sciences, 23(5), 389–407. https://doi.org/10.1016/j.tics.2019.02.003 Gibson, E., &amp; Pearlmutter, N. J. (2000). Distinguishing serial and parallel parsing. Journal of Psycholinguistic Research, 29(2), 231–240. https://doi.org/10.1023/A:1005153330168 Gold, J. I., &amp; Watanabe, T. (2010). Perceptual learning. Current Biology, 20(2), R46–R48. https://doi.org/10.1016/j.cub.2009.10.066 Goldstone, R. L. (1998). Perceptual learning. Annual Review of Psychology, 49(1), 585–612. https://doi.org/10.1146/annurev.psych.49.1.585 Gordon-Salant, S., &amp; Fitzgibbons, P. J. (1995). Recognition of multiply degraded speech by young and elderly listeners. Journal of Speech and Hearing Research, 38(5), 1150–1156. https://doi.org/10.1044/jshr.3805.1150 Gordon-Salant, S., &amp; Fitzgibbons, P. J. (2004). Effects of stimulus and noise rate variability on speech perception by younger and older adults. The Journal of the Acoustical Society of America, 115(4), 1808–1817. https://doi.org/10.1121/1.1645249 Goy, H., Pelletier, M., Coletta, M., &amp; Pichora-Fuller, M. K. (2013). The effects of semantic context and the type and amount of acoustic distortion on lexical decision by younger and older adults. Journal of Speech, Language, and Hearing Research, 56(6), 1715–1732. https://doi.org/10.1044/1092-4388(2013/12-0053) Greenberg, S. (1996). Auditory processing of speech. In N. J. Lass (Ed.), Principles of experimental phonetics (pp. 362–407). Mosby, St. Louis. Greenwood, D. D. (1990). A cochlear frequency-position function for several species—29 years later. Journal of the Acoustical Society of America, 87(6), 2592–2605. https://doi.org/10.1121/1.399052 Grueber, C. E., Nakagawa, S., Laws, R. J., &amp; Jamieson, I. G. (2011). Multimodel inference in ecology and evolution: challenges and solutions. Journal of Evolutionary Biology, 24(4), 699–711. https://doi.org/10.1111/j.1420-9101.2010.02210.x Guediche, S., Blumstein, S. E., Fiez, J. A., &amp; Holt, L. L. (2014). Speech perception under adverse conditions: Insights from behavioral, computational, and neuroscience research. Frontiers in Systems Neuroscience, 7(Jan), 1–16. https://doi.org/10.3389/fnsys.2013.00126 Guest, O., &amp; Martin, A. E. (2021). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science, 16(4), 789–802. https://doi.org/10.1177/1745691620970585 Hafter, E. R., Sarampalis, A., &amp; Loui, P. (2007). Auditory attention and filters. In Auditory perception of sound sources (Vol. 29, pp. 115–142). https://doi.org/10.1007/978-0-387-71305-2_5 Haider, H., &amp; Frensch, P. A. (1996). The role of information reduction in skill acquisition. Cognitive Psychology, 30(3), 304–337. https://doi.org/10.1006/cogp.1996.0009 Hakonen, M., May, P. J. C., Jääskeläinen, I. P., Jokinen, E., Sams, M., &amp; Tiitinen, H. (2017). Predictive processing increases intelligibility of acoustically distorted speech: Behavioral and neural correlates. Brain and Behavior, 7(9), e00789. https://doi.org/10.1002/brb3.789 Hale, J. (2001). A probabilistic earley parser as a psycholinguistic model. Second Meeting of the North American Chapter of the Association for Computational Linguistics, 1–8. https://doi.org/10.3115/1073336.1073357 Hartwigsen, G., Golombek, T., &amp; Obleser, J. (2015). Repetitive transcranial magnetic stimulation over left angular gyrus modulates the predictability gain in degraded speech comprehension. Cortex, 68, 100–110. https://doi.org/10.1016/j.cortex.2014.08.027 Hauser, M. D., Chomsky, N., &amp; Fitch, W. T. (2002). The faculty of language: What is It, who has it, and how did it evolve? Science, 298(5598), 1569–1579. https://doi.org/10.1126/science.298.5598.1569 Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P., &amp; De Lange, F. P. (2022). A hierarchy of linguistic predictions during natural language comprehension. Proceedings of the National Academy of Sciences, 119(32), e2201968119. https://doi.org/10.1073/pnas.2201968119 Hervais-Adelman, A., Kumar, U., Mishra, R. K., Tripathi, V. N., Guleria, A., Singh, J. P., Eisner, F., &amp; Huettig, F. (2019). Learning to read recycles visual cortical networks without destruction. Science Advances, 5(9), eaax0262. https://doi.org/10.1126/sciadv.aax0262 Heyselaar, E., Peeters, D., &amp; Hagoort, P. (2021). Do we predict upcoming speech content in naturalistic environments? Language, Cognition and Neuroscience, 36(4), 440–461. https://doi.org/10.1080/23273798.2020.1859568 Hofmann, M. J., Remus, S., Biemann, C., Radach, R., &amp; Kuchinke, L. (2021). Language models explain word reading times better than empirical predictability. Frontiers in Artificial Intelligence, 4. https://doi.org/10.3389/frai.2021.730570 Huettig, F., &amp; Guerra, E. (2019). Effects of speech rate, preview time of visual context, and participant instructions reveal strong limits on prediction in language processing. Brain Research, 1706(June 2017), 196–208. https://doi.org/10.1016/j.brainres.2018.11.013 Huettig, F., &amp; Janse, E. (2016). Individual differences in working memory and processing speed predict anticipatory spoken language processing in the visual world. Language, Cognition and Neuroscience, 31(1), 80–93. https://doi.org/10.1080/23273798.2015.1047459 Huettig, F., &amp; Mani, N. (2016). Is prediction necessary to understand language? Probably not. Language, Cognition and Neuroscience, 31(1), 19–31. https://doi.org/10.1080/23273798.2015.1072223 Hunter, C. R., &amp; Pisoni, D. B. (2018). Extrinsic cognitive load impairs spoken word recognition in high-and low-predictability sentences. Ear and Hearing, 39(2), 378–389. https://doi.org/10.1097/AUD.0000000000000493 Husband, E. M., &amp; Bovolenta, G. (2020). Prediction failure blocks the use of local semantic context. Language, Cognition and Neuroscience, 35(3), 273–291. https://doi.org/10.1080/23273798.2019.1651881 Ito, A., Corley, M., Pickering, M. J., Martin, A. E., &amp; Nieuwland, M. S. (2016). Predicting form and meaning: Evidence from brain potentials. Journal of Memory and Language, 86, 157–171. https://doi.org/10.1016/j.jml.2015.10.007 Iwasaki, S., Ocho, S., Nagura, M., &amp; Hoshino, T. (2002). Contribution of speech rate to speech perception in multichannel cochlear implant users. Annals of Otology, Rhinology and Laryngology, 111(8), 718–721. https://doi.org/10.1177/000348940211100811 Jackendoff, R. (2002). Précis of foundations of language: Brain, meaning, grammar, evolution. Behavioral and Brain Sciences, 26(6), 651–665. Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007 Janse, E. (2009). Processing of fast speech by elderly listeners. The Journal of the Acoustical Society of America, 125(4), 2361–2373. https://doi.org/10.1121/1.3082117 Johnson, B. P., Dayan, E., Censor, N., &amp; Cohen, L. G. (2021). Crowdsourcing in cognitive and systems neuroscience. The Neuroscientist, 10738584211017018. https://doi.org/10.1177/10738584211017018 Jones, M., &amp; Love, B. C. (2011). Bayesian fundamentalism or enlightenment? on the explanatory status and theoretical contributions of bayesian models of cognition. Behavioral and Brain Sciences, 34(4), 169–188. https://doi.org/10.1017/S0140525X10003134 Kaiser, E., &amp; Trueswell, J. (2004). The role of discourse context in the processing of a flexible word-order language. Cognition, 94(2), 113–147. https://doi.org/10.1016/j.cognition.2004.01.002 Kamide, Y., Altmann, G. T. M., &amp; Haywood, S. L. (2003). The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye movements. Journal of Memory and Language, 49(1), 133–156. https://doi.org/10.1016/s0749-596x(03)00023-8 Kaufeld, G. (2021). Investigating spoken language comprehension as perceptual inference (p. 183) [Doctoral dissertation, Max Planck Research School (IMPRS) for Language Sciences]. https://repository.ubn.ru.nl/handle/2066/228260 Kemper, S., &amp; Harden, T. (1999). Experimentally disentangling what’s beneficial about elderspeak from what’s not. Psychology and Aging, 14(4), 656–670. https://doi.org/10.1037/0882-7974.14.4.656 Knoeferle, P., Crocker, M. W., Scheepers, C., &amp; Pickering, M. J. (2005). The influence of the immediate visual context on incremental thematic role-assignment: evidence from eye-movements in depicted events. Cognition, 95(1), 95–127. https://doi.org/10.1016/j.cognition.2004.03.002 Koch, X., &amp; Janse, E. (2016). Speech rate effects on the processing of conversational speech across the adult life span. The Journal of the Acoustical Society of America, 139(4), 1618–1636. https://doi.org/10.1121/1.4944032 Kochari, A. R., &amp; Flecken, M. (2019). Lexical prediction in language comprehension: a replication study of grammatical gender effects in Dutch. Language, Cognition and Neuroscience, 34(2), 239–253. https://doi.org/10.1080/23273798.2018.1524500 Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., &amp; De Lange, F. P. (2012). Attention reverses the effect of prediction in silencing sensory signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310 Kuperberg, G. R. (2021). Tea with milk? A hierarchical generative framework of sequential event comprehension. Topics in Cognitive Science, 13(1), 256–298. https://doi.org/10.1111/tops.12518 Kuperberg, G. R., &amp; Jaeger, T. F. (2016). What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31(1), 32–59. https://doi.org/10.1080/23273798.2015.1102299 Kutas, M., &amp; Federmeier, K. D. (2011). Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP). Annual Review of Psychology, 62(1), 621–647. https://doi.org/10.1146/annurev.psych.093008.131123 Kutas, M., &amp; Hillyard, S. A. (1984). Brain potentials during reading reflect word expectancy and semantic association. Nature, 307(5947), 161–163. https://doi.org/10.1038/307161a0 Kuznetsova, A., Brockhoff, P. B., &amp; Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13). https://doi.org/10.18637/jss.v082.i13 Lange, K. (2013). The ups and downs of temporal orienting: A review of auditory temporal orienting studies and a model associating the heterogeneous findings on the auditory N1 with opposite effects of attention and prediction. Frontiers in Human Neuroscience, 7, 1–14. https://doi.org/10.3389/fnhum.2013.00263 Lange, K., &amp; Röder, B. (2010). Temporal orienting in audition, touch, and across modalities. In Attention and time (pp. 393–405). Oxford University Press, Oxford, United Kingdom. Lecompte, D. C. (1995). An irrelevant speech effect with repeated and continuous background speech. Psychonomic Bulletin &amp; Review, 2(3), 391–397. https://doi.org/10.3758/BF03210978 Leensen, M. C. J., &amp; Dreschler, W. A. (2013). Speech-in-noise screening tests by internet, Part 3: Test sensitivity for uncontrolled parameters in domestic usage. International Journal of Audiology, 52(10), 658–669. https://doi.org/10.3109/14992027.2013.803610 Lerner, Y., Honey, C. J., Katkov, M., &amp; Hasson, U. (2014). Temporal scaling of neural responses to compressed and dilated natural speech. Journal of Neurophysiology, 111(12), 2433–2444. https://doi.org/10.1152/jn.00497.2013 Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3), 1126–1177. https://doi.org/10.1016/j.cognition.2007.05.006 Li, X., Lu, Y., &amp; Zhao, H. (2014). How and when predictability interacts with accentuation in temporally selective attention during speech comprehension. Neuropsychologia, 64, 71–84. https://doi.org/10.1016/j.neuropsychologia.2014.09.020 Li, X., Zhang, Y., Li, L., Zhao, H., &amp; Du, X. (2017). Attention is shaped by semantic level of event-structure during speech comprehension: An electroencephalogram study. Cognitive Neurodynamics, 11(5), 467–481. https://doi.org/10.1007/s11571-017-9442-4 Lieberman, P. (2013). The unpredictable species. In The unpredictable species. Princeton University Press. Liu, S., &amp; Zeng, F.-G. (2006). Temporal properties in clear speech perception. The Journal of the Acoustical Society of America, 120(1), 424–432. https://doi.org/10.1121/1.2208427 Loizou, P. C., Dorman, M., &amp; Tu, Z. (1999). On the number of channels needed to understand speech. The Journal of the Acoustical Society of America, 106(4), 2097–2103. https://doi.org/10.1121/1.427954 Longster, J. A. (2003). Concatenative speech synthesis : A framework for reducing perceived distortion when using the TD-PSOLA algorithm [Doctoral dissertation]. Bournemouth University. Lopukhina, A., Lopukhin, K., &amp; Laurinavichyute, A. (2021). Morphosyntactic but not lexical corpus-based probabilities can substitute for cloze probabilities in reading experiments. PloS One, 16(1), e0246133. https://doi.org/10.1371/journal.pone.0246133 Lorch, R. F., &amp; Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16(1), 149. https://doi.org/10.1037/0278-7393.16.1.149 Love, T., Walenski, M., &amp; Swinney, D. (2009). Slowed speech input has a differential impact on on-line and off-line processing in children’s comprehension of pronouns. Journal of Psycholinguistic Research, 38(3), 285–304. https://doi.org/10.1007/s10936-009-9103-9 Lowder, M. W., &amp; Ferreira, F. (2016). Prediction in the processing of repair disfluencies. Language, Cognition and Neuroscience, 31(1), 73–79. https://doi.org/10.1080/23273798.2015.1036089 Luce, P. A., &amp; Pisoni, D. B. (1998). Recognizing spoken words: The neighborhood activation model. Ear and Hearing, 19(1), 1. https://doi.org/10.1097/00003446-199802000-00001 Luke, S. G., &amp; Christianson, K. (2016). Limits on lexical prediction during reading. Cognitive Psychology, 88, 22–60. https://doi.org/10.1016/j.cogpsych.2016.06.002 Lupyan, G., &amp; Clark, A. (2015). Words and the World. Current Directions in Psychological Science, 24(4), 279–284. https://doi.org/10.1177/0963721415570732 Malik, W. A., Marco-Llorca, C., Berendzen, K., &amp; Piepho, H. P. (2020). Choice of link and variance function for generalized linear mixed models: a case study with binomial response in proteomics. Communications in Statistics - Theory and Methods, 49(17), 4313–4332. https://doi.org/10.1080/03610926.2019.1599021 Mantegna, F., Hintz, F., Ostarek, M., Alday, P. M., &amp; Huettig, F. (2019). Distinguishing integration and prediction accounts of ERP N400 modulations in language processing through experimental design. Neuropsychologia, 134, 107199. https://doi.org/10.1016/j.neuropsychologia.2019.107199 Markman, A. B., &amp; Otto, A. R. (2011). Cognitive systems optimize energy rather than information. Behav. Brain Sci, 34(207), 10–1017. https://doi.org/10.1017/S0140525X11000355 Marques, T., Nguyen, J., Fioreze, G., &amp; Petreanu, L. (2018). The functional organization of cortical feedback inputs to primary visual cortex. Nature Neuroscience, 21(5), 757–764. https://doi.org/10.1038/s41593-018-0135-z Marrufo-Pérez, M. I., Eustaquio-Martı́n, A., &amp; Lopez-Poveda, E. A. (2019). Speech predictability can hinder communication in difficult listening conditions. Cognition, 192, 103992. https://doi.org/10.1016/j.cognition.2019.06.004 Martin, A. E. (2016). Language processing as cue integration: Grounding the psychology of language in perception and neurophysiology. Frontiers in Psychology, 7, 120. https://doi.org/10.3389/fpsyg.2016.00120 Mattys, S. L., Davis, M. H., Bradlow, A. R., &amp; Scott, S. K. (2012). Speech recognition in adverse conditions: A review. Language and Cognitive Processes, 27(7-8), 953–978. https://doi.org/10.1080/01690965.2012.705006 Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp; Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001 McClelland, J. L., &amp; Elman, J. L. (1986). The TRACE model of speech perception. Cognitive Psychology, 18(1), 1–86. https://doi.org/10.1016/0010-0285(86)90015-0 McCoy, S. L., Tun, P. A., Cox, L. C., Colangelo, M., Stewart, R. A., &amp; Wingfield, A. (2005). Hearing loss and perceptual effort: Downstream effects on older adults’ memory for speech. Quarterly Journal of Experimental Psychology Section A: Human Experimental Psychology, 58(1), 22–33. https://doi.org/10.1080/02724980443000151 McCullough, C. M. (1958). Context aids in reading. The Reading Teacher, 11(4), 225–229. https://www.jstor.org/stable/20197091 McGurk, H., &amp; MacDonald, J. (1976). Hearing lips and seeing voices. Nature, 264(5588), 746–748. https://doi.org/10.1038/264746a0 McShane, B. B., Gal, D., Gelman, A., Robert, C., &amp; Tackett, J. L. (2019). Abandon statistical significance. The American Statistician, 73(sup1), 235–245. https://doi.org/0.1080/00031305.2018.1527253 Meng, Q., Wang, X., Cai, Y., Kong, F., Buck, A. N., Yu, G., Zheng, N., &amp; Schnupp, J. W. H. (2019). Time-compression thresholds for Mandarin sentences in normal-hearing and cochlear implant listeners. Hearing Research, 374, 58–68. https://doi.org/10.1016/j.heares.2019.01.011 Meteyard, L., &amp; Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092 Metusalem, R., Kutas, M., Urbach, T. P., Hare, M., McRae, K., &amp; Elman, J. L. (2012). Generalized event knowledge activation during online sentence comprehension. Journal of Memory and Language, 66(4), 545–567. https://doi.org/10.1016/j.jml.2012.01.001 Michaelov, J. A., Coulson, S., &amp; Bergen, B. K. (2022). So cloze yet so far: N400 amplitude is better predicted by distributional information than human predictability judgements. IEEE Transactions on Cognitive and Developmental Systems. https://doi.org/10.1109/TCDS.2022.3176783 Miller, G. A., Heise, G. A., &amp; Lichten, W. (1951). The intelligibility of speech as a function of the context of the test materials. Journal of Experimental Psychology, 41(5), 329–335. https://doi.org/10.1037/h0062491 Minocher, R., Atmaca, S., Bavero, C., McElreath, R., &amp; Beheim, B. (2021). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450 Mishra, R. K., Singh, N., Pandey, A., &amp; Huettig, F. (2012). Spoken language-mediated anticipatory eye- movements are modulated by reading ability - Evidence from Indian low and high literates. Journal of Eye Movement Research, 5(1), 1–10. https://doi.org/10.16910/jemr.5.1.3 Moon, I. J., &amp; Hong, S. H. (2014). What is temporal fine structure and why is it important? Korean Journal of Audiology, 18(1), 1–7. https://doi.org/10.7874/kja.2014.18.1.1 Moon, I. J., Won, J. H., Park, M. H., Ives, D. T., Nie, K., Heinz, M. G., Lorenzi, C., &amp; Rubinstein, J. T. (2014). Optimal combination of neural temporal envelope and fine structure cues to explain speech identification in background noise. Journal of Neuroscience, 34(36), 12145–12154. https://doi.org/10.1523/JNEUROSCI.1025-14.2014 Morton, J. (1964). The effects of context on the visual duration threshold for words. British Journal of Psychology, 55(2), 165–180. https://doi.org/10.1111/j.2044-8295.1964.tb02716.x Moulines, E., &amp; Charpentier, F. (1990). Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Communication, 9(1990), 453–467. https://doi.org/10.1016/0167-6393(90)90021-Z Müller, J. A., Wendt, D., Kollmeier, B., Debener, S., &amp; Brand, T. (2019). Effect of speech rate on neural tracking of speech. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.00449 Musch, J., &amp; Reips, U.-D. (2000). A brief history of web experimenting. In Psychological experiments on the internet (pp. 61–87). https://doi.org/10.1016/b978-012099980-4/50004-6 Näätänen, R., &amp; Picton, T. (1987). The N1 wave of the human electric and magnetic response to sound: A review and an analysis of the component structure. Psychophysiology, 24(4), 375–425. https://doi.org/10.1111/j.1469-8986.1987.tb00311.x Nahum, M., Nelken, I., &amp; Ahissar, M. (2008). Low-level information and high-level perception: The case of speech in noise. PLoS Biology, 6(5), 0978–0991. https://doi.org/10.1371/journal.pbio.0060126 Nejime, Y., &amp; Moore, B. C. J. (1998). Evaluation of the effect of speech-rate slowing on speech intelligibility in noise using a simulation of cochlear hearing loss. The Journal of the Acoustical Society of America, 103(1), 572–576. https://doi.org/10.1121/1.421123 Nicenboim, B., Vasishth, S., &amp; Rösler, F. (2020). Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a Bayesian random-effects meta-analysis using publicly available data. Neuropsychologia, 142, 107427. https://doi.org/10.1016/j.neuropsychologia.2020.107427 Nieuwland, M. S. (2019). Do ‘early’ brain responses reveal word form prediction during language comprehension? A critical review. Neuroscience and Biobehavioral Reviews, 96, 367–400. https://doi.org/10.1016/j.neubiorev.2018.11.019 Nieuwland, M. S., Barr, D. J., Bartolozzi, F., Busch-Moreno, S., Darley, E., Donaldson, D. I., Ferguson, H. J., Fu, X., Heyselaar, E., Huettig, F., Husband, E. M., Ito, A., Kazanina, N., Kogan, V., Kohút, Z., Kulakova, E., Mézière, D., Politzer-Ahles, S., Rousselet, G., … Von Grebmer Zu Wolfsthurn, S. (2020). Dissociable effects of prediction and integration during language comprehension: Evidence from a largescale study using brain potentials. Philosophical Transactions of the Royal Society B: Biological Sciences. https://doi.org/10.1098/rstb.2018.0522 Nieuwland, M. S., Politzer-Ahles, S., Heyselaar, E., Segaert, K., Darley, E., Kazanina, N., Von Grebmer Zu Wolfsthurn, S., Bartolozzi, F., Kogan, V., Ito, A., Mézière, D., Barr, D. J., Rousselet, G. A., Ferguson, H. J., Busch-Moreno, S., Fu, X., Tuomainen, J., Kulakova, E., Husband, E. M., … Huettig, F. (2018). Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. eLife, 7, 1–24. https://doi.org/10.7554/elife.33468 Norris, D., McQueen, J. M., &amp; Cutler, A. (2016). Prediction, Bayesian inference and feedback in speech recognition. Language, Cognition and Neuroscience, 31(1), 4–18. https://doi.org/10.1080/23273798.2015.1081703 Nosofsky, R. M. (1986). Attention, similarity, and the identification–categorization relationship. Journal of Experimental Psychology: General, 115(1), 39. https://doi.org/10.1037/0096-3445.115.1.39 Obleser, J. (2014). Putting the Listening Brain in Context. Language and Linguistics Compass, 8(12), 646–658. https://doi.org/10.1111/lnc3.12098 Obleser, J., &amp; Kotz, S. A. (2010). Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network. Cerebral Cortex, 20(3), 633–640. https://doi.org/10.1093/cercor/bhp128 Obleser, J., &amp; Kotz, S. A. (2011). Multiple brain signatures of integration in the comprehension of degraded speech. NeuroImage, 55(2), 713–723. https://doi.org/10.1016/j.neuroimage.2010.12.020 Obleser, J., Wise, R. J. S., Dresner, M. A., &amp; Scott, S. K. (2007). Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions. Journal of Neuroscience, 27(9), 2283–2289. https://doi.org/10.1523/jneurosci.4663-06.2007 Oostdijk, N. (2000, May). The spoken Dutch corpus. Overview and first evaluation. Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00). http://www.lrec-conf.org/proceedings/lrec2000/pdf/110.pdf Orena, A. J., &amp; Colby, S. (2021). Recognizing voices through a cochlear implant: A systematic review. PsyArXiv. https://doi.org/10.31234/osf.io/e865q Parida, S., &amp; Heinz, M. G. (2022). Underlying neural mechanisms of degraded speech intelligibility following noise-induced hearing loss: The importance of distorted tonotopy: Neural mechanisms of degraded speech intelligibility. Hearing Research, 108586. https://doi.org/10.1016/j.heares.2022.108586 Patro, C., &amp; Mendel, L. L. (2020). Semantic influences on the perception of degraded speech by individuals with cochlear implants. The Journal of the Acoustical Society of America, 147(3), 1778–1789. https://doi.org/10.1121/10.0000934 Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential reasoning. Proceedings of the 7th Annual Conference of the Cognitive Science Society, 329–334. Peelle, J. E. (2013). Cortical responses to degraded speech are modulated by linguistic predictions. Proceedings of Meetings on Acoustics ICA2013, 19, 060108. Peelle, J. E. (2018). Listening effort: How the cognitive consequences of acoustic challenge are reflected in brain and behavior. Ear and Hearing, 39(2), 204–214. https://doi.org/10.1097/AUD.0000000000000494 Peelle, J. E., &amp; Wingfield, A. (2005). Dissociations in perceptual learning revealed by adult age differences in adaptation to time-compressed speech. Journal of Experimental Psychology: Human Perception and Performance, 31(6), 1315–1330. https://doi.org/10.1037/0096-1523.31.6.1315 Peer, E., Rothschild, D., Gordon, A., Evernden, Z., &amp; Damer, E. (2022). Data quality of platforms and panels for online behavioral research. Behavior Research Methods, 54(4), 1643–1662. https://doi.org/10.3758/s13428-021-01694-3 Peirce, J., Gray, J. R., Simpson, S., MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., &amp; Lindeløv, J. K. (2019). PsychoPy2: Experiments in behavior made easy. Behavior Research Methods, 51(1), 195–203. https://doi.org/10.3758/s13428-018-01193-y Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. Psychological Bulletin, 144(10), 1002–1044. https://doi.org/10.1037/bul0000158 Pierce, A. G. J., &amp; Ollason, J. G. (1987). Eight reasons why optimal foraging theory is a complete waste of time. Oikos, 49(1), 111–117. https://doi.org/10.2307/3565560 Pinker, S., &amp; Jackendoff, R. (2005). The faculty of language: what’s special about it? Cognition, 95(2), 201–236. https://doi.org/10.1016/j.cognition.2004.08.004 Poldrack, R., Protopapas, A., Nagarajan, S., Tallal, P., Merzenich, M., Temple, E., &amp; Gabrieli, J. (1998). Auditory processing of temporally compressed speech: An fMRI study. Journal of Cognitive Neuroscience, 10, 126–126. Prolific. (2014). Prolific academic. https://www.prolific.co. Pusse, F., Sayeed, A., &amp; Demberg, V. (2016). LingoTurk: Managing crowdsourced tasks for psycholinguistics. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, 57–61. https://doi.org/10.18653/v1/n16-3012 R Core Team. (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Rayner, K., Reichle, E. D., Stroud, M. J., Williams, C. C., &amp; Pollatsek, A. (2006). The effect of word frequency, word predictability, and font difficulty on the eye movements of young and older readers. Psychology and Aging, 21(3), 448. https://doi.org/10.1037/0882-7974.21.3.448 Reips, U.-D. (2021). Web-based research in psychology. Zeitschrift für Psychologie. https://doi.org/10.1027/2151-2604/a000475 Richards, S. A., Whittingham, M. J., &amp; Stephens, P. A. (2011). Model selection and model averaging in behavioural ecology: the utility of the IT-AIC framework. Behavioral Ecology and Sociobiology, 65(1), 77–89. https://doi.org/10.1007/s00265-010-1035-8 Roberts, B., Summers, R. J., &amp; Bailey, P. J. (2011). The intelligibility of noise-vocoded speech: Spectral information available from acrosschannel comparison of amplitude envelopes. Proceedings of the Royal Society B: Biological Sciences, 278(1711), 1595–1600. https://doi.org/10.1098/rspb.2010.1554 Rodero, E. (2016). Influence of speech rate and information density on recognition: The moderate dynamic mechanism. Media Psychology, 19(2), 224–242. https://doi.org/10.1080/15213269.2014.1002942 Rommers, J., Meyer, A. S., &amp; Huettig, F. (2015). Verbal and nonverbal predictors of language-mediated anticipatory eye movements. Attention, Perception, &amp; Psychophysics, 77(3), 720–730. https://doi.org/0.3758/s13414-015-0873-x Rönnberg, J., Lunner, T., Zekveld, A., Sörqvist, P., Danielsson, H., Lyxell, B., Dahlström, Ö., Signoret, C., Stenfelt, S., Pichora-Fuller, M. K., &amp; Rudner, M. (2013). The Ease of Language Understanding (ELU) model: Theoretical, empirical, and clinical advances. Frontiers in Systems Neuroscience, 7(31), 1–17. https://doi.org/10.3389/fnsys.2013.00031 Rosen, S., Faulkner, A., &amp; Wilkinson, L. (1999). Adaptation by normal listeners to upward spectral shifts of speech: Implications for cochlear implants. The Journal of the Acoustical Society of America, 106(6), 3629–3636. https://doi.org/10.1121/1.428215 Ryskin, R., &amp; Fang, X. (2021). The many timescales of context in language processing. In Psychology of learning and motivation (Vol. 75, pp. 201–243). Elsevier. https://doi.org/10.1016/bs.plm.2021.08.001 Ryskin, R., Futrell, R., Kiran, S., &amp; Gibson, E. (2018). Comprehenders model the nature of noise in the environment. Cognition, 181(July 2017), 141–150. https://doi.org/10.1016/j.cognition.2018.08.018 Samuel, A. G. (1996). Does lexical information influence the perceptual restoration of phonemes? Journal of Experimental Psychology: General, 125(1), 28. Samuel, A. G., &amp; Kraljic, T. (2009). Perceptual learning for speech. Attention, Perception, &amp; Psychophysics, 71(6), 1207–1218. https://doi.org/10.3758/app.71.6.1207 Sanders, L. D., &amp; Astheimer, L. B. (2008). Temporally selective attention modulates early perceptual processing: Event-related potential evidence. Perception and Psychophysics, 70(4), 732–742. https://doi.org/10.3758/PP.70.4.732 Sanderson, S. K., &amp; Roberts, W. W. (2008). The evolutionary forms of the religious life: A cross-cultural, quantitative analysis. American Anthropologist, 110(4), 454–466. https://doi.org/10.1111/j.1548-1433.2008.00078.x Sanford, A. J., Sanford, A. J., Molle, J., &amp; Emmott, C. (2006). Shallow processing and attention capture in written and spoken discourse. Discourse Processes, 42(2), 109–130. https://doi.org/10.1207/s15326950dp4202_2 Schlueter, A., Lemke, U., Kollmeier, B., &amp; Holube, I. (2014). Intelligibility of time-compressed speech: The effect of uniform versus non-uniform time-compression algorithms. The Journal of the Acoustical Society of America, 135(3), 1541–1555. https://doi.org/10.1121/1.4863654 Schneider, B. A., &amp; Pichora-Fuller, M. K. (2001). Age-related changes in temporal processing: Implications for listening comprehension. Seminars in Hearing, 22(3), 227–239. https://doi.org/10.1055/s-2001-15628 Scholman, M. C., Demberg, V., &amp; Sanders, T. J. (2020). Individual differences in expecting coherence relations: Exploring the variability in sensitivity to contextual signals in discourse. Discourse Processes, 57(10), 844–861. https://doi.org/10.1080/0163853X.2020.1813492 Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 461–464. https://doi.org/10.1214/aos/1176344136 Seow, T. X. F., &amp; Hauser, T. U. (2022). Reliability of web-based affective auditory stimulus presentation. Behavior Research Methods, 54(1), 378–392. https://doi.org/10.3758/s13428-021-01643-0 Seth, A. K. (2013). Interoceptive inference, emotion, and the embodied self. Trends in Cognitive Sciences, 17(11), 565–573. https://doi.org/10.1016/j.tics.2013.09.007 Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(4), 623–656. https://doi.org/10.1002/j.1538-7305.1948.tb00917.x Shannon, R. V., Fu, Q.-J., &amp; Galvin Iii, J. (2004). The number of spectral channels required for speech recognition depends on the difficulty of the listening situation. Acta Oto-Laryngologica, 124(0), 50–54. https://doi.org/10.1080/03655230410017562 Shannon, R. V., Zeng, F.-G., Kamath, V., Wygonski, J., &amp; Ekelid, M. (1995). Speech Recognition with Primarily Temporal Cues. Science, 270(5234), 303–304. https://doi.org/10.1126/science.270.5234.303 Shannon, R. V., Zeng, V., &amp; Wygonski, J. (1998). Speech recognition with altered spectral distribution of envelope cues. The Journal of the Acoustical Society of America, 104(4), 2467–2476. https://doi.org/10.1121/1.423774 Sharit, J., Czaja, S. J., Nair, S., &amp; Lee, C. C. (2003). Effects of age, speech rate, and environmental support in using telephone voice menu systems. Human Factors, 45(2), 234–251. https://doi.org/10.1518/hfes.45.2.234.27245 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008a). Priming and sentence context support listening to noise-vocoded speech by younger and older adults. The Journal of the Acoustical Society of America, 123(1), 489–499. https://doi.org/10.1121/1.2783762 Sheldon, S., Pichora-Fuller, M. K., &amp; Schneider, B. A. (2008b). Effect of age, presentation method, and learning on identification of noise-vocoded words. The Journal of the Acoustical Society of America, 123(1), 476–488. https://doi.org/10.1121/1.2805676 Simantiraki, O., &amp; Cooke, M. (2020). Exploring listeners’ speech rate preferences. Proc. Interspeech 2020, 1346–1350. https://doi.org/10.21437/Interspeech.2020-1832 Slattery, T. J., Sturt, P., Christianson, K., Yoshida, M., &amp; Ferreira, F. (2013). Lingering misinterpretations of garden path sentences arise from competing syntactic representations. Journal of Memory and Language, 69(2), 104–120. https://doi.org/10.1016/j.jml.2013.04.001 Smith, N. J., &amp; Levy, R. (2008). Optimal processing times in reading: A formal model and empirical investigation. Proceedings of the 30th Annual Conference of the Cognitive Science Society, 30. Smith, N. J., &amp; Levy, R. (2011). Cloze but no cigar: The complex relationship between cloze, corpus, and subjective probabilities in language processing. Proceedings of the 33rd Annual Conference of the Cognitive Science Society, 33. Sohoglu, E., Peelle, J. E., Carlyon, R. P., &amp; Davis, M. H. (2012). Predictive top-down integration of prior knowledge during speech perception. Journal of Neuroscience, 32(25), 8443–8453. https://doi.org/10.1523/JNEUROSCI.5069-11.2012 Sommers, M. S., Nygaard, L. C., &amp; Pisoni, D. B. (1994). Stimulus variability and spoken word recognition. I. Effects of variability in speaking rate and overall amplitude. The Journal of the Acoustical Society of America, 96(3), 1314–1324. https://doi.org/10.1121/1.411453 Sommers, M. S., Spehar, B., Tye-Murray, N., Myerson, J., &amp; Hale, S. (2020). Age differences in the effects of speaking rate on auditory, visual, and auditory-visual speech perception. Ear and Hearing, 41(3), 549–560. https://doi.org/10.1097/AUD.0000000000000776 Stadler, W., Ott, D. V. M., Springer, A., Schubotz, R. I., Schütz-Bosbach, S., &amp; Prinz, W. (2012). Repetitive TMS suggests a role of the human dorsal premotor cortex in action prediction. Frontiers in Human Neuroscience, 6. https://doi.org/10.3389/fnhum.2012.00020 Staub, A. (2015). The effect of lexical predictability on eye movements in reading: Critical review and theoretical interpretation. Language and Linguistics Compass, 9(8), 311–327. https://doi.org/10.1111/lnc3.12151 Staub, A. (2011). The effect of lexical predictability on distributions of eye fixation durations. Psychonomic Bulletin and Review, 18(2), 371–376. https://doi.org/10.3758/s13423-010-0046-9 Staub, A., Grant, M., Astheimer, L., &amp; Cohen, A. (2015). The influence of cloze probability and item constraint on cloze task response time. Journal of Memory and Language, 82, 1–17. https://doi.org/10.1016/j.jml.2015.02.004 Stilp, C. (2020). Acoustic context effects in speech perception. Wiley Interdisciplinary Reviews: Cognitive Science, 11(1), 1–18. https://doi.org/10.1002/wcs.1517 Strauß, A., Kotz, S. A., &amp; Obleser, J. (2013). Narrowed expectancies under degraded speech: Revisiting the N400. Journal of Cognitive Neuroscience, 25(8), 1383–1395. https://doi.org/10.1162/jocn_a_00389 Sturt, P., Sanford, A. J., Stewart, A., &amp; Dawydiak, E. (2004). Linguistic focus and good-enough representations: An application of the change-detection paradigm. Psychonomic Bulletin &amp; Review, 11(5), 882–888. Sutton, B., King, J., Hux, K., &amp; Beukelman, D. (1995). Younger and older adults’ rate performance when listening to synthetic speech. Augmentative and Alternative Communication, 11(3), 147–153. https://doi.org/10.1080/07434619512331277269 Taleb, N. (2020). Assessing the intelligibility and acoustic changes of time-processed speech [Masters thesis, Case Western Reserve University]. http://rave.ohiolink.edu/etdc/view?acc_num=case1586637814204979 Taylor, W. L. (1953). “Cloze procedure”: A new tool for measuring readability. Journalism Quarterly, 30(4), 415–433. https://doi.org/10.1177/107769905303000401 Thornton, A. R. D., Harmer, M., &amp; Lavoie, B. A. (2007). Selective attention increases the temporal precision of the auditory N100 event-related potential. Hearing Research, 230(1-2), 73–79. https://doi.org/10.1016/j.heares.2007.04.004 Tóth, B., Honbolygó, F., Szalárdy, O., Orosz, G., Farkas, D., &amp; Winkler, I. (2020). The effects of speech processing units on auditory stream segregation and selective attention in a multi-talker (cocktail party) situation. Cortex, 130, 387–400. https://doi.org/10.1016/j.cortex.2020.06.007 Tuthill, J. C., &amp; Azim, E. (2018). Proprioception. Current Biology, 28(5), R194–R203. https://doi.org/10.1016/j.cub.2018.01.064 Vaden, K. I., Kuchinsky, S. E., Cute, S. L., Ahlstrom, J. B., Dubno, J. R., &amp; Eckert, M. A. (2013). The cingulo-opercular network provides word-recognition benefit. Journal of Neuroscience, 33(48), 18979–18986. https://doi.org/10.1523/JNEUROSCI.1417-13.2013 Vagharchakian, L., Dehaene-Lambertz, G., Pallier, C., &amp; Dehaene, S. (2012). A temporal bottleneck in the language comprehension network. Journal of Neuroscience, 32(26), 9089–9102. https://doi.org/10.1523/JNEUROSCI.5685-11.2012 van Os, M., Kray, J., &amp; Demberg, V. (2021a). Mishearing as a side effect of rational language comprehension in noise. Frontiers in Psychology, 12. https://doi.org/10.3389/fpsyg.2021.679278 van Os, M., Kray, J., &amp; Demberg, V. (2021b). Recognition of minipairs in (un)predictive sentence contexts in two types of noise. Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 43(43), 2943–2949. Van Petten, C., &amp; Luka, B. J. (2012). Prediction during language comprehension: Benefits, costs, and ERP components. International Journal of Psychophysiology, 83(2), 176–190. https://doi.org/10.1016/j.ijpsycho.2011.09.015 Vasishth, S., &amp; Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051 Vasishth, S., Schad, D., Bürki, A., &amp; Kliegl, R. (2022). Hypothetical repeated sampling and the t-test. In Linear mixed models in linguistics and psychology: A comprehensive introduction (DRAFT). Verhagen, V., Mos, M., Backus, A., &amp; Schilperoord, J. (2018). Predictive language processing revealing usage-based variation. Language and Cognition, 10(2), 329–373. https://doi.org/10.1017/langcog.2018.4 Verhelst, W., &amp; Roelands, M. (1993). Overlap-add technique based on waveform similarity (WSOLA) for high quality time-scale modification of speech. IEEE International Conference on Acoustics, Speech and Signal Processing, 2, 554–557. https://doi.org/10.1109/icassp.1993.319366 Warren, R. M. (1970). Perceptual restoration of missing speech sounds. Science, 167(3917), 392–393. https://doi.org/10.1126/science.167.3917.39 Welch, N., &amp; Krantz, J. H. (1996). The World-Wide Web as a medium for psychoacoustical demonstrations and experiments: Experience and results. Behavior Research Methods, Instruments, and Computers, 28(2), 192–196. https://doi.org/10.3758/bf03204764 Wild, C. J., Yusuf, A., Wilson, D. E., Peelle, J. E., Davis, M. H., &amp; Johnsrude, I. S. (2012). Effortful listening: The processing of degraded speech depends critically on attention. Journal of Neuroscience, 32(40), 14010–14021. https://doi.org/10.1523/JNEUROSCI.1528-12.2012 Wingfield, A., McCoy, S. L., Peelle, J. E., Tun, P. A., &amp; Cox, C. L. (2006). Effects of Adult Aging and Hearing Loss on Comprehension of Rapid Speech Varying in Syntactic Complexity. Journal of the American Academy of Audiology, 17(07), 487–497. https://doi.org/10.3766/jaaa.17.7.4 Wingfield, A., Tun, P. A., Koh, C. K., &amp; Rosen, M. J. (1999). Regaining lost time: Adult aging and the effect of time restoration on recall of time-compressed speech. Psychology and Aging, 14(3), 380–389. https://doi.org/10.1037/0882-7974.14.3.380 Winn, M. B. (2016). Rapid release from listening effort resulting from semantic context, and effects of spectral degradation and cochlear implants. Trends in Hearing, 20, 1–17. https://doi.org/10.1177/2331216516669723 Winn, M. B., &amp; Teece, K. H. (2021). Slower speaking rate reduces listening effort among listeners with cochlear implants. Ear and Hearing, 42(3), 584. https://doi.org/10.1097/aud.0000000000000958 Wlotko, E. W., &amp; Federmeier, K. D. (2012). Age-related changes in the impact of contextual strength on multiple aspects of sentence comprehension. Psychophysiology, 49(6), 770–785. https://doi.org/10.1111/j.1469-8986.2012.01366.x Wlotko, E. W., &amp; Federmeier, K. D. (2015). Time for prediction? The effect of presentation rate on predictive sentence comprehension during word-by-word reading. Cortex, 68, 20–32. https://doi.org/10.1016/j.cortex.2015.03.014 Woods, K. J. P., Siegel, M. H., Traer, J., &amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, and Psychophysics, 79(7), 2064–2072. https://doi.org/10.3758/s13414-017-1361-2 Wöstmann, M., &amp; Obleser, J. (2016). Acoustic detail but not predictability of task-irrelevant speech disrupts working memory. Frontiers in Human Neuroscience, 10, 538. https://doi.org/10.3389/fnhum.2016.00538 Xiang, M., &amp; Kuperberg, G. R. (2015). Reversing expectations during discourse comprehension. Language, Cognition and Neuroscience, 30(6), 648–672. https://doi.org/10.1080/23273798.2014.995679 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
